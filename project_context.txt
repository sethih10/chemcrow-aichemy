# PROJECT CONTEXT DUMP
# Root: C:\Users\brown\Documents\GitHub\chemcrow-aichemy
# Generated: 2025-12-10 16:39:13
# Notes: Hidden paths and common junk/binaries are skipped. Sizes > max-size are skipped.
#
# Included files will follow with clear section headers.



================================================================================
=== FILE: clean_chemcrow_minimal copy.py ===
================================================================================

```python
# clean_chemcrow_minimal.py
"""
Clean ChemCrow instance that only loads the tools we consider â€œsafeâ€ plus our custom tools:
  - SAFE STOCK TOOLS:
      * Python_REPL
      * Wikipedia
      * Mol2CAS
      * PatentCheck
      * SMILES2Weight
      * FunctionalGroups

  - CUSTOM TOOLS:
      * Arxiv2ResultLLM (ArxivLiteratureSearch)
      * MotifDecompositionTool (MotifDecomposition)
      * VastraVisualise (VESTA CIF â†’ PNG)
      * ListCrystalCIFsTool (ls/dir over CIF directory)

Everything with broken behaviour or MorganGenerator deprecation warnings
(Name2SMILES, SMILES2Name, MolSimilarity, ControlChemCheck, SimilarityToControlChem,
ExplosiveCheck, SafetySummary) is removed.
"""

import os
from pathlib import Path
from typing import List as TList

from chemcrow.agents import make_tools, ChemCrow
from langchain.chat_models import ChatOpenAI
from langchain.tools import BaseTool

# === IMPORT YOUR CUSTOM TOOLS FROM tools/New ===
from chemcrow.tools.New.Arxiv2ResultLLM import Arxiv2ResultLLM
from chemcrow.tools.New.motif_tools import MotifDecompositionTool
from chemcrow.tools.New.VastraVisualise import VastraVisualise


# === HARD-CODED PATHS FOR YOUR SETUP ===

# VESTA installation
DEFAULT_VESTA = Path(r"C:\Users\brown\Documents\VESTA-win64\VESTA.exe")

# Directory with your COF CIFs
DEFAULT_CRYSTAL_DIR = Path(
    r"C:\Users\brown\Documents\PhD\Winter School\COF_crystals\crystals"
)

# Let VESTA_EXE be visible to the Vastra code if it looks for it
os.environ.setdefault("VESTA_EXE", str(DEFAULT_VESTA))


# === LOCAL TOOL: LIST CIF FILES IN YOUR CRYSTAL DIRECTORY ===

class ListCrystalCIFsTool(BaseTool):
    """
    Simple tool that lists all .cif files in DEFAULT_CRYSTAL_DIR.

    Input:
        query: ignored (can be an empty string).

    Output:
        A newline-separated list of CIF filenames, or an error message.
    """

    name = "ListCrystalCIFs"
    description = (
        f"List available CIF files in the default crystals directory "
        f"({DEFAULT_CRYSTAL_DIR}). "
        "Call this first if you want to know which CIFs you can visualise or analyse."
    )

    crystals_dir: Path = DEFAULT_CRYSTAL_DIR

    def __init__(self, crystals_dir: Path | None = None):
        super().__init__()
        self.crystals_dir = crystals_dir or DEFAULT_CRYSTAL_DIR

    def _run(self, query: str = "") -> str:
        if not self.crystals_dir.exists():
            return f"Crystal directory not found: {self.crystals_dir}"

        cifs: TList[str] = sorted(p.name for p in self.crystals_dir.glob("*.cif"))
        if not cifs:
            return f"No CIF files found in {self.crystals_dir}"

        return "\n".join(cifs)

    async def _arun(self, query: str = "") -> str:
        raise NotImplementedError("This tool does not support async.")


def build_clean_chemcrow():
    # LLM used *inside* the tools (same as ChemCrow uses internally)
    tools_llm = ChatOpenAI(
        model_name=os.environ.get("CHEMCROW_TOOLS_MODEL", "gpt-4o-mini"),
        temperature=0,
    )

    # Keys ChemCrow expects
    api_keys = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
        "SEMANTIC_SCHOLAR_API_KEY": os.getenv("SEMANTIC_SCHOLAR_API_KEY", ""),
    }

    # Build all stock tools
    all_tools = make_tools(tools_llm, api_keys=api_keys)

    # Whitelist only the "safe" stock tools
    allowed_stock = {
        "Python_REPL",
        "Wikipedia",
        "Mol2CAS",
        "PatentCheck",
        "SMILES2Weight",
        "FunctionalGroups",
    }

    clean_tools: TList[BaseTool] = [t for t in all_tools if t.name in allowed_stock]

    # ---- ADD YOUR CUSTOM TOOLS EXPLICITLY ----

    openai_key = api_keys["OPENAI_API_KEY"]

    # 1) Arxiv literature search
    arxiv_tool = Arxiv2ResultLLM(
        llm=tools_llm,
        openai_api_key=openai_key,
        max_results=20,
    )

    # 2) Motif decomposition (CIF motif analysis)
    motif_tool = MotifDecompositionTool()

    # 3) VESTA visualisation (CIF â†’ PNG)
    vastra_tool = VastraVisualise(
        vesta_exe=str(DEFAULT_VESTA),
        output_png="viz/vastra_output.png",
        scale=2,
        nogui=True,
    )

    # 4) CIF directory listing tool
    list_cifs_tool = ListCrystalCIFsTool(crystals_dir=DEFAULT_CRYSTAL_DIR)

    custom_tools: TList[BaseTool] = [
        arxiv_tool,
        motif_tool,
        vastra_tool,
        list_cifs_tool,
    ]

    # Combine stock + custom
    clean_tools.extend(custom_tools)

    print("\nLoaded tools in CLEAN ChemCrow:")
    for t in clean_tools:
        print(f"  - {t.name}")

    # Build ChemCrow using ONLY these tools
    chem_model = ChemCrow(
        tools=clean_tools,  # <â€” this is the important bit
        # leave everything else at default so we don't guess wrong
    )

    return chem_model


if __name__ == "__main__":
    print("Building clean ChemCrow instance (no deprecated / broken tools, plus custom tools)...")
    chem_model = build_clean_chemcrow()

    # Also list tools in a more detailed way
    print("\n=== ChemCrow tools (CLEAN + CUSTOM) ===\n")
    tools_obj = chem_model.tools if hasattr(chem_model, "tools") else []

    for idx, tool in enumerate(tools_obj, start=1):
        name = getattr(tool, "name", repr(tool))
        desc = getattr(tool, "description", "(no description)")
        print(f"{idx}. {name}")
        print(f"   {desc}")
        print()

    # Optional quick test
    # question = "List the CIF files you can see, then pick one and visualise it."
    # print("\n=== TEST QUERY ===")
    # print(f"Q: {question}\n")
    # answer = chem_model.run(question)
    # print("A:", answer)

```


================================================================================
=== FILE: clean_chemcrow_minimal.py ===
================================================================================

```python
# clean_chemcrow_minimal.py

import os
from pathlib import Path
from typing import List as TList

from chemcrow.agents import make_tools, ChemCrow
from langchain.chat_models import ChatOpenAI
from langchain.tools import BaseTool

# === IMPORT YOUR CUSTOM TOOLS FROM tools/New ===
from chemcrow.tools.New.Arxiv2ResultLLM import Arxiv2ResultLLM
from chemcrow.tools.New.motif_tools import MotifDecompositionTool, MotifComparisonTool
from chemcrow.tools.New.VastraVisualise import VastraVisualise


# === HARD-CODED PATHS FOR YOUR SETUP ===

DEFAULT_VESTA = Path(r"C:\Users\brown\Documents\VESTA-win64\VESTA.exe")

DEFAULT_CRYSTAL_DIR = Path(
    r"C:\Users\brown\Documents\PhD\Winter School\COF_crystals\crystals"
)

TARGET_CIF = Path(
    r"C:\Users\brown\Documents\PhD\Winter School\COF_crystals\crystals\07000N2_ddec.cif"
)

# Your simple COF motif library
DEFAULT_MOTIF_LIB = Path(r"chemcrow\tools\New\motifs_cof_simple.json")

os.environ.setdefault("VESTA_EXE", str(DEFAULT_VESTA))



class CheckCrystalFileTool(BaseTool):
    """
    Small tool that **does not** dump the whole directory,
    it only checks if a given CIF filename exists in DEFAULT_CRYSTAL_DIR.

    Input:
        query: either a bare filename like "07000N2_ddec.cif"
               or a full/absolute path (we take the .name).

    Output:
        A short string indicating whether the file exists and its full path.
    """

    name = "CheckCrystalFile"
    description = (
        f"Check whether a given CIF file exists in the default crystals directory "
        f"({DEFAULT_CRYSTAL_DIR}). Input can be just the filename "
        f"(e.g. '07000N2_ddec.cif') or a full path; the tool extracts the name."
    )

    crystals_dir: Path = DEFAULT_CRYSTAL_DIR

    def __init__(self, crystals_dir: Path | None = None):
        super().__init__()
        self.crystals_dir = crystals_dir or DEFAULT_CRYSTAL_DIR

    def _run(self, query: str) -> str:
        if not self.crystals_dir.exists():
            return f"Crystal directory not found: {self.crystals_dir}"

        if not query:
            return "No filename given. Please provide something like '07000N2_ddec.cif'."

        # If user passes a full path, strip to just the filename
        candidate_name = Path(query).name
        candidate_path = self.crystals_dir / candidate_name

        if candidate_path.exists():
            return f"FOUND: {candidate_name} at '{candidate_path}'."
        else:
            return f"NOT FOUND: {candidate_name} in '{self.crystals_dir}'."

    async def _arun(self, query: str) -> str:
        raise NotImplementedError("This tool does not support async.")

def build_clean_chemcrow():
    tools_llm = ChatOpenAI(
        model_name=os.environ.get("CHEMCROW_TOOLS_MODEL", "gpt-4.1-mini"),
        temperature=0,
    )

    api_keys = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
        "SEMANTIC_SCHOLAR_API_KEY": os.getenv("SEMANTIC_SCHOLAR_API_KEY", ""),
    }

    all_tools = make_tools(tools_llm, api_keys=api_keys)

    allowed_stock = {
        "Python_REPL",
        "Wikipedia",
        "Mol2CAS",
        "PatentCheck",
        "SMILES2Weight",
        "FunctionalGroups",
    }

    clean_tools: TList[BaseTool] = [t for t in all_tools if t.name in allowed_stock]

    openai_key = api_keys["OPENAI_API_KEY"]

    # 1) Arxiv literature search
    arxiv_tool = Arxiv2ResultLLM(
        llm=tools_llm,
        openai_api_key=openai_key,
        max_results=20,
    )

    # 2) Motif decomposition & comparison
    motif_tool = MotifDecompositionTool(
        default_motif_library_path=str(DEFAULT_MOTIF_LIB)
    )

    motif_compare_tool = MotifComparisonTool(
        default_motif_library_path=str(DEFAULT_MOTIF_LIB)
    )

    # 3) VESTA visualisation (CIF â†’ PNG)
    vastra_tool = VastraVisualise(
        vesta_exe=str(DEFAULT_VESTA),
        output_png="viz/vastra_output.png",
        scale=2,
        nogui=True,
    )

    # 4) CIF directory listing tool
    check_cif_tool = CheckCrystalFileTool(crystals_dir=DEFAULT_CRYSTAL_DIR)

    custom_tools: TList[BaseTool] = [
        arxiv_tool,
        motif_tool,
        motif_compare_tool,
        vastra_tool,
        check_cif_tool,
    ]

    clean_tools.extend(custom_tools)

    print("\nLoaded tools in CLEAN ChemCrow:")
    for t in clean_tools:
        print(f"  - {t.name}")

    chem_model = ChemCrow(
        tools=clean_tools,
    )

    return chem_model

```


================================================================================
=== FILE: dev-requirements.txt ===
================================================================================

```
pre-commit
python-dotenv

```


================================================================================
=== FILE: LICENSE ===
================================================================================

```
MIT License

Copyright (c) 2023 White Laboratory

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```


================================================================================
=== FILE: list.py ===
================================================================================

```python
import os

# Just in case â€“ ChemCrow / LangChain usually wants this set even if we never call the LLM.
os.environ.setdefault("OPENAI_API_KEY", "dummy-key")

from chemcrow.agents import ChemCrow

def get_tools_from_chemcrow(chem_model):
    """
    Try to pull the tools list from various likely places,
    depending on how the ChemCrow class is implemented.
    """
    # 1) Direct attribute on ChemCrow
    if hasattr(chem_model, "tools") and chem_model.tools:
        return chem_model.tools

    # 2) On the agent_executor (LangChain AgentExecutor)
    if hasattr(chem_model, "agent_executor"):
        ae = chem_model.agent_executor
        if hasattr(ae, "tools") and ae.tools:
            return ae.tools

        # 3) On an inner .agent (ChatZeroShotAgent etc.)
        if hasattr(ae, "agent") and hasattr(ae.agent, "tools"):
            return ae.agent.tools

    raise RuntimeError("Couldn't find tools attribute on ChemCrow. "
                       "Check ChemCrow implementation / version.")

def main():
    # Instantiate ChemCrow with your usual settings
    chem_model = ChemCrow(
        model="gpt-4.1-mini",   # or whatever you're using now
        temp=0.1,
        streaming=False,
        verbose=False,
    )

    tools_obj = get_tools_from_chemcrow(chem_model)

    # Tools can be a list or a dict depending on how they're stored
    if isinstance(tools_obj, dict):
        tools_iter = tools_obj.values()
    else:
        tools_iter = tools_obj

    print("\n=== ChemCrow tools (default) ===\n")
    for idx, tool in enumerate(tools_iter, start=1):
        name = getattr(tool, "name", repr(tool))
        desc = getattr(tool, "description", "(no description)")
        print(f"{idx}. {name}")
        print(f"   {desc}")

        # If it's a LangChain `Tool` / `StructuredTool`, you can sometimes inspect args:
        # (won't always be present, so we guard it)
        args_schema = getattr(tool, "args", None) or getattr(tool, "args_schema", None)
        if args_schema is not None:
            print(f"   Args schema: {args_schema}")
        print()

if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: project_context.txt ===
================================================================================

```
# PROJECT CONTEXT DUMP
# Root: C:\Users\brown\Documents\GitHub\chemcrow-aichemy
# Generated: 2025-12-10 16:39:13
# Notes: Hidden paths and common junk/binaries are skipped. Sizes > max-size are skipped.
#
# Included files will follow with clear section headers.



================================================================================
=== FILE: clean_chemcrow_minimal copy.py ===
================================================================================

```python
# clean_chemcrow_minimal.py
"""
Clean ChemCrow instance that only loads the tools we consider â€œsafeâ€ plus our custom tools:
  - SAFE STOCK TOOLS:
      * Python_REPL
      * Wikipedia
      * Mol2CAS
      * PatentCheck
      * SMILES2Weight
      * FunctionalGroups

  - CUSTOM TOOLS:
      * Arxiv2ResultLLM (ArxivLiteratureSearch)
      * MotifDecompositionTool (MotifDecomposition)
      * VastraVisualise (VESTA CIF â†’ PNG)
      * ListCrystalCIFsTool (ls/dir over CIF directory)

Everything with broken behaviour or MorganGenerator deprecation warnings
(Name2SMILES, SMILES2Name, MolSimilarity, ControlChemCheck, SimilarityToControlChem,
ExplosiveCheck, SafetySummary) is removed.
"""

import os
from pathlib import Path
from typing import List as TList

from chemcrow.agents import make_tools, ChemCrow
from langchain.chat_models import ChatOpenAI
from langchain.tools import BaseTool

# === IMPORT YOUR CUSTOM TOOLS FROM tools/New ===
from chemcrow.tools.New.Arxiv2ResultLLM import Arxiv2ResultLLM
from chemcrow.tools.New.motif_tools import MotifDecompositionTool
from chemcrow.tools.New.VastraVisualise import VastraVisualise


# === HARD-CODED PATHS FOR YOUR SETUP ===

# VESTA installation
DEFAULT_VESTA = Path(r"C:\Users\brown\Documents\VESTA-win64\VESTA.exe")

# Directory with your COF CIFs
DEFAULT_CRYSTAL_DIR = Path(
    r"C:\Users\brown\Documents\PhD\Winter School\COF_crystals\crystals"
)

# Let VESTA_EXE be visible to the Vastra code if it looks for it
os.environ.setdefault("VESTA_EXE", str(DEFAULT_VESTA))


# === LOCAL TOOL: LIST CIF FILES IN YOUR CRYSTAL DIRECTORY ===

class ListCrystalCIFsTool(BaseTool):
    """
    Simple tool that lists all .cif files in DEFAULT_CRYSTAL_DIR.

    Input:
        query: ignored (can be an empty string).

    Output:
        A newline-separated list of CIF filenames, or an error message.
    """

    name = "ListCrystalCIFs"
    description = (
        f"List available CIF files in the default crystals directory "
        f"({DEFAULT_CRYSTAL_DIR}). "
        "Call this first if you want to know which CIFs you can visualise or analyse."
    )

    crystals_dir: Path = DEFAULT_CRYSTAL_DIR

    def __init__(self, crystals_dir: Path | None = None):
        super().__init__()
        self.crystals_dir = crystals_dir or DEFAULT_CRYSTAL_DIR

    def _run(self, query: str = "") -> str:
        if not self.crystals_dir.exists():
            return f"Crystal directory not found: {self.crystals_dir}"

        cifs: TList[str] = sorted(p.name for p in self.crystals_dir.glob("*.cif"))
        if not cifs:
            return f"No CIF files found in {self.crystals_dir}"

        return "\n".join(cifs)

    async def _arun(self, query: str = "") -> str:
        raise NotImplementedError("This tool does not support async.")


def build_clean_chemcrow():
    # LLM used *inside* the tools (same as ChemCrow uses internally)
    tools_llm = ChatOpenAI(
        model_name=os.environ.get("CHEMCROW_TOOLS_MODEL", "gpt-4o-mini"),
        temperature=0,
    )

    # Keys ChemCrow expects
    api_keys = {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
        "SEMANTIC_SCHOLAR_API_KEY": os.getenv("SEMANTIC_SCHOLAR_API_KEY", ""),
    }

    # Build all stock tools
    all_tools = make_tools(tools_llm, api_keys=api_keys)

    # Whitelist only the "safe" stock tools
    allowed_stock = {
        "Python_REPL",
        "Wikipedia",
        "Mol2CAS",
        "PatentCheck",
        "SMILES2Weight",
        "FunctionalGroups",
    }

    clean_tools: TList[BaseTool] = [t for t in all_tools if t.name in allowed_stock]

    # ---- ADD YOUR CUSTOM TOOLS EXPLICITLY ----

    openai_key = api_keys["OPENAI_API_KEY"]

    # 1) Arxiv literature search
    arxiv_tool = Arxiv2ResultLLM(
        llm=tools_llm,
        openai_api_key=openai_key,
        max_results=20,
    )

    # 2) Motif decomposition (CIF motif analysis)
    motif_tool = MotifDecompositionTool()

    # 3) VESTA visualisation (CIF â†’ PNG)
    vastra_tool = VastraVisualise(
        vesta_exe=str(DEFAULT_VESTA),
        output_png="viz/vastra_output.png",
        scale=2,
        nogui=True,
    )

    # 4) CIF directory listing tool
    list_cifs_tool = ListCrystalCIFsTool(crystals_dir=DEFAULT_CRYSTAL_DIR)

    custom_tools: TList[BaseTool] = [
        arxiv_tool,
        motif_tool,
        vastra_tool,
        list_cifs_tool,
    ]

    # Combine stock + custom
    clean_tools.extend(custom_tools)

    print("\nLoaded tools in CLEAN ChemCrow:")
    for t in clean_tools:
        print(f"  - {t.name}")

    # Build ChemCrow using ONLY these tools
    chem_model = ChemCrow(
        tools=clean_tools,  # <â€” this is the important bit
        # leave everything else at default so we don't guess wrong
    )

    return chem_model


if __name__ == "__main__":
    print("Building clean ChemCrow instance (no deprecated / broken tools, plus custom tools)...")
    chem_model = build_clean_chemcrow()

    # Also list tools in a more detailed way
    print("\n=== ChemCrow tools (CLEAN + CUSTOM) ===\n")
    tools_obj = chem_model.tools if hasattr(chem_model, "tools") else []

    for idx, tool in enumerate(tools_obj, start=1):
        name = getattr(tool, "name", repr(tool))
        desc = getattr(tool, "description", "(no description)")
        print(f"{idx}. {name}")
        print(f"   {desc}")
        print()

    # Optional quick test
    # question = "List the CIF files you can see, then pick one and visualise it."
    # print("\n=== TEST QUERY ===")
    # print(f"Q: {question}\n")
    # answer = chem_model.run(question)
    # print("A:", answer)

```


================================================================================
=== FILE: clean_chemcrow_minimal.py ===
================================================================================


```


================================================================================
=== FILE: project_to_txt.py ===
================================================================================

```python
#!/usr/bin/env python3
from __future__ import annotations   # â† must be here (first statement)

import argparse, os, sys, time, io
from pathlib import Path
from typing import Optional

DEFAULT_EXCLUDED_DIRS = {
    ".git", ".hg", ".svn", ".idea", ".vscode", ".venv",
    "__pycache__", "node_modules", "dist", "build", ".cache",
    ".mypy_cache", ".pytest_cache", ".next", ".turbo", ".parcel-cache"
}
DEFAULT_EXCLUDED_EXTS = {
    # archives & binaries 
    ".zip", ".gz", ".bz2", ".xz", ".7z", ".rar", ".tar",
    ".exe", ".dll", ".so", ".dylib", ".bin", ".dat", ".lock",
    # media
    ".png", ".jpg", ".jpeg", ".gif", ".webp", ".ico", ".svg",
    ".mp4", ".mov", ".avi", ".mkv", ".mp3", ".wav", ".flac",
    # docs likely huge or non-text
    ".pdf", ".psd", ".ai"
}
DEFAULT_EXCLUDED_FILES = {
    # huge / noisy lock or cache files (still text, but not helpful)
    "package-lock.json", "pnpm-lock.yaml", "yarn.lock",
    "poetry.lock", "pipfile.lock", ".DS_Store", "Thumbs.db"
}

LANG_BY_EXT = {
    ".py": "python", ".js": "javascript", ".ts": "typescript",
    ".tsx": "tsx", ".jsx": "jsx", ".json": "json", ".yml": "yaml",
    ".yaml": "yaml", ".md": "markdown", ".toml": "toml",
    ".ini": "", ".cfg": "", ".conf": "", ".txt": "",
    ".html": "html", ".css": "css", ".scss": "scss", ".sass": "sass",
    ".sh": "bash", ".ps1": "powershell", ".sql": "sql", ".xml": "xml",
    ".java": "java", ".c": "c", ".h": "c", ".cpp": "cpp", ".hpp": "cpp",
    ".rs": "rust", ".go": "go", ".rb": "ruby", ".php": "php",
    ".ipynb": "json"
}

def looks_binary(sample: bytes) -> bool:
    if b"\x00" in sample:
        return True
    # Heuristic: if >30% bytes are non-text-ish, treat as binary
    text_bytes = b"\t\n\r\f\b" + bytes(range(32, 127))
    if not sample:
        return False
    nontext = sum(ch not in text_bytes for ch in sample)
    return (nontext / len(sample)) > 0.30


def read_text_file(path: Path, max_bytes: int) -> Optional[str]:  # ðŸ‘ˆ change here
    try:
        with path.open("rb") as f:
            sample = f.read(min(max_bytes, 4096))
            if looks_binary(sample):
                return None
        with path.open("rb") as f:
            data = f.read(max_bytes)
        text = data.decode("utf-8", errors="replace")
        return text.replace("\r\n", "\n").replace("\r", "\n")
    except Exception:
        return None


def is_hidden(p: Path) -> bool:
    # Treat names starting with '.' as hidden (works cross-platform)
    return any(part.startswith(".") and part not in {".", ".."} for part in p.parts)

def should_skip_file(p: Path, args, rel: Path) -> bool:
    name = p.name
    if not args.include_dotfiles and (name.startswith(".") or is_hidden(rel)):
        return True
    if name in args.exclude_files or name in DEFAULT_EXCLUDED_FILES:
        return True
    ext = p.suffix.lower()
    if ext in DEFAULT_EXCLUDED_EXTS or ext in args.exclude_exts:
        return True
    if args.include_exts and ext not in args.include_exts:
        return True
    try:
        size = p.stat().st_size
        if size > args.max_size_mb * 1024 * 1024:
            return True
    except Exception:
        return True
    return False

def should_skip_dir(dirpath: Path, args, rel: Path) -> bool:
    name = dirpath.name
    if not args.include_dotfiles and (name.startswith(".") or is_hidden(rel)):
        return True
    if name in DEFAULT_EXCLUDED_DIRS or name in args.exclude_dirs:
        return True
    return False

def main():
    ap = argparse.ArgumentParser(
        description="Concatenate a projectâ€™s text source into one TXT for agent context."
    )
    ap.add_argument("root", help="Project root directory.")
    ap.add_argument("-o", "--output", default="project_context.txt",
                    help="Output TXT path (default: project_context.txt)")
    ap.add_argument("--max-size-mb", type=int, default=2,
                    help="Skip files larger than this many MB (default: 2)")
    ap.add_argument("--include-dotfiles", action="store_true",
                    help="Include dotfiles and hidden paths (default: skip)")
    ap.add_argument("--include-exts", default="", help="Comma-separated whitelist of extensions (e.g., .py,.md)")
    ap.add_argument("--exclude-exts", default="", help="Comma-separated extra excluded extensions (e.g., .log,.csv)")
    ap.add_argument("--exclude-dirs", default="",
                    help="Comma-separated extra excluded dir names (exact match)")
    ap.add_argument("--exclude-files", default="",
                    help="Comma-separated extra excluded file names (exact match)")
    ap.add_argument("--no-fences", action="store_true",
                    help="Do not wrap contents in Markdown code fences.")
    args = ap.parse_args()

    root = Path(args.root).resolve()
    if not root.exists() or not root.is_dir():
        print(f"Error: {root} is not a directory.", file=sys.stderr)
        sys.exit(1)

    # Parse lists
    args.include_exts = {e.strip().lower() for e in args.include_exts.split(",") if e.strip()} if args.include_exts else set()
    args.exclude_exts = {e.strip().lower() for e in args.exclude_exts.split(",") if e.strip()} if args.exclude_exts else set()
    args.exclude_dirs = {d.strip() for d in args.exclude_dirs.split(",") if d.strip()} if args.exclude_dirs else set()
    args.exclude_files = {f.strip() for f in args.exclude_files.split(",") if f.strip()} if args.exclude_files else set()

    included_files = []
    skipped = {"dir": 0, "hidden": 0, "size": 0, "binary": 0, "ext": 0, "other": 0}

    out_path = Path(args.output).resolve()
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with io.open(out_path, "w", encoding="utf-8", newline="\n") as out:
        # Header / manifest preface
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        out.write(f"# PROJECT CONTEXT DUMP\n")
        out.write(f"# Root: {root}\n# Generated: {ts}\n")
        out.write("# Notes: Hidden paths and common junk/binaries are skipped. Sizes > max-size are skipped.\n")
        out.write("#\n# Included files will follow with clear section headers.\n\n")

        # Walk
        for dirpath, dirnames, filenames in os.walk(root):
            dirpath = Path(dirpath)
            rel_dir = dirpath.relative_to(root)

            # Filter directories in-place (os.walk respects modifications)
            keep_dirs = []
            for d in dirnames:
                dpath = dirpath / d
                if should_skip_dir(dpath, args, rel_dir / d):
                    skipped["dir"] += 1
                    continue
                keep_dirs.append(d)
            dirnames[:] = keep_dirs

            # Files
            for fn in filenames:
                fpath = dirpath / fn
                rel = fpath.relative_to(root)
                if should_skip_file(fpath, args, rel):
                    # heuristic reason counting
                    name = fpath.name
                    if not args.include_dotfiles and (name.startswith(".") or is_hidden(rel)):
                        skipped["hidden"] += 1
                    elif fpath.suffix.lower() in DEFAULT_EXCLUDED_EXTS or fpath.suffix.lower() in args.exclude_exts:
                        skipped["ext"] += 1
                    else:
                        try:
                            if fpath.stat().st_size > args.max_size_mb * 1024 * 1024:
                                skipped["size"] += 1
                            else:
                                skipped["other"] += 1
                        except Exception:
                            skipped["other"] += 1
                    continue

                # Read and binary check
                with fpath.open("rb") as fb:
                    sample = fb.read(4096)
                if looks_binary(sample):
                    skipped["binary"] += 1
                    continue

                text = read_text_file(fpath, max_bytes=args.max_size_mb * 1024 * 1024)
                if text is None:
                    skipped["other"] += 1
                    continue

                included_files.append(str(rel))

                # Write section
                lang = LANG_BY_EXT.get(fpath.suffix.lower(), "")
                out.write("\n\n" + "=" * 80 + "\n")
                out.write(f"=== FILE: {rel} ===\n")
                out.write("=" * 80 + "\n\n")
                if args.no_fences:
                    out.write(text)
                else:
                    fence = lang if lang is not None else ""
                    out.write(f"```{fence}\n{text}\n```\n")

        # Manifest footer
        out.write("\n\n" + "#" * 80 + "\n")
        out.write("# MANIFEST\n")
        out.write("# Included files:\n")
        for p in included_files:
            out.write(f"#  - {p}\n")
        out.write("#\n# Skips summary:\n")
        for k, v in skipped.items():
            out.write(f"#  {k}: {v}\n")
        out.write("# END\n")

    print(f"Done. Wrote: {out_path}")
    print(f"Included files: {len(included_files)} | Skips: {skipped}")
    return 0

if __name__ == "__main__":
    sys.exit(main())

```


================================================================================
=== FILE: README.md ===
================================================================================

```markdown
[![tests](https://github.com/ur-whitelab/chemcrow-public/actions/workflows/tests.yml/badge.svg)](https://github.com/ur-whitelab/chemcrow-public)
[![PyPI](https://img.shields.io/pypi/v/chemcrow)](https://img.shields.io/pypi/v/chemcrow)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/chemcrow)](https://img.shields.io/pypi/pyversions/chemcrow)
[![DOI:10.1101/2020.07.15.204701](https://zenodo.org/badge/DOI/10.48550/arXiv.2304.05376.svg)](https://doi.org/10.48550/arXiv.2304.05376)
[![DOI](https://zenodo.org/badge/649361700.svg)](https://zenodo.org/doi/10.5281/zenodo.10884638)




<picture>
  <source media="(prefers-color-scheme: dark)" srcset="assets/chemcrow_dark_bold.png" width='100%'>
  <source media="(prefers-color-scheme: light)" srcset="assets/chemcrow_light_bold.png" width='100%'>
  <img alt="ChemCrow logo" src="/assets/" width="100%">
</picture>


<br></br>


ChemCrow is an open source package for the accurate solution of reasoning-intensive chemical tasks.

Built with Langchain, it uses a collection of chemical tools including RDKit, paper-qa, as well as some relevant databases in chemistry, like Pubchem and chem-space.

## ðŸ¤— Try it out in [HuggingFace](https://huggingface.co/spaces/doncamilom/ChemCrow)!

[![ChemCrow Demo](assets/hf-demo.png)](https://huggingface.co/spaces/doncamilom/ChemCrow)


## âš ï¸ Note

This package does not contain all the tools described in the [ChemCrow paper](https://arxiv.org/abs/2304.05376) because
of API usage restrictions. This repo will not give the same results as that paper.

All the experiments have been released under [ChemCrow runs](https://github.com/ur-whitelab/chemcrow-runs).


## ðŸ‘©â€ðŸ’» Installation

```
pip install chemcrow
```

## ðŸ”¥ Usage
First set up your API keys in your environment.
```
export OPENAI_API_KEY=your-openai-api-key
```

You can optionally use Serp API:

```
export SERP_API_KEY=your-serpapi-api-key
```

In a Python session:
```python
from chemcrow.agents import ChemCrow

chem_model = ChemCrow(model="gpt-4-0613", temp=0.1, streaming=False)
chem_model.run("What is the molecular weight of tylenol?")
```


## ðŸ› ï¸ Self-hosting of some tools.

By default, ChemCrow relies on the RXN4Chem API for retrosynthetic planning and reaction product prediction. This can however be slow and depends on you having an API key.

Optionally, you can also self host these tools by running some pre-made docker images.

Run 

```
docker run --gpus all -d -p 8051:5000 doncamilom/rxnpred:latest
docker run --gpus all -d -p 8052:5000 doncamilom/retrosynthesis:latest
```


Now ChemCrow can be used like this:

```python
from chemcrow.agents import ChemCrow

chem_model = ChemCrow(model="gpt-4-0613", temp=0.1, streaming=False, local_rxn=True)
chem_model.run("What is the product of the reaction between styrene and dibromine?")
```


## âœ… Citation
Bran, Andres M., et al. "ChemCrow: Augmenting large-language models with chemistry tools." arXiv preprint arXiv:2304.05376 (2023).

```bibtex
@article{bran2023chemcrow,
      title={ChemCrow: Augmenting large-language models with chemistry tools},
      author={Andres M Bran and Sam Cox and Oliver Schilter and Carlo Baldassari and Andrew D White and Philippe Schwaller},
      year={2023},
      eprint={2304.05376},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      publisher={arXiv}
}
```

```


================================================================================
=== FILE: requirments.txt ===
================================================================================

```
pymatgen
arxiv
```


================================================================================
=== FILE: setup.py ===
================================================================================

```python
from setuptools import find_packages, setup

exec(open("chemcrow/version.py").read())

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setup(
    name="chemcrow",
    python_requires=">=3.9, <3.12",  # this temporarily fixes molbloom install, which breaks with 3.12
    version=__version__,
    description="Accurate solution of reasoning-intensive chemical tasks, powered by LLMs.",
    author="Andres M Bran, Sam Cox, Andrew White, Philippe Schwaller",
    author_email="andrew.white@rochester.edu",
    url="https://github.com/ur-whitelab/chemcrow-public",
    license="MIT",
    packages=find_packages(),
    package_data={"chemcrow": ["data/chem_wep_smi.csv"]},
    install_requires=[
        "ipython",
        "python-dotenv",
        "rdkit",
        "synspace",
        "openai==0.27.8",
        "molbloom",
        "paper-qa==1.1.1",
        "google-search-results",
        "langchain>=0.0.234,<=0.0.275",
        "langchain_core==0.0.1",
        "nest_asyncio",
        "tiktoken",
        "rmrkl",
        #"paper-scraper@git+https://github.com/blackadad/paper-scraper.git",
        "streamlit",
        "rxn4chemistry",
        "duckduckgo-search",
        "wikipedia",
    ],
    test_suite="tests",
    long_description=long_description,
    long_description_content_type="text/markdown",
    classifiers=[
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
)

```


================================================================================
=== FILE: test_all_tools copy.py ===
================================================================================

```python
# test_all_tools.py
"""
End-to-end test script that tells ChemCrow to exercise all custom tools:

  - ListCrystalCIFs
  - VastraVisualise
  - MotifDecomposition
  - MotifComparison
  - ArxivLiteratureSearch

Assumes:
  - clean_chemcrow_minimal.py defines build_clean_chemcrow, DEFAULT_CRYSTAL_DIR, TARGET_CIF.
  - motifs_cof_simple.json exists at DEFAULT_MOTIF_LIB.
"""

import os

from clean_chemcrow_minimal import (
    build_clean_chemcrow,
    DEFAULT_CRYSTAL_DIR,
    TARGET_CIF,
    DEFAULT_MOTIF_LIB,
)


def main():
    # Make sure OpenAI key is visible
    if not os.getenv("OPENAI_API_KEY"):
        print("WARNING: OPENAI_API_KEY is not set. Tools that call OpenAI may fail.\n")

    chem_model = build_clean_chemcrow()

    target_cif_str = str(TARGET_CIF)
    crystal_dir_str = str(DEFAULT_CRYSTAL_DIR)
    motif_lib_str = str(DEFAULT_MOTIF_LIB)

    # Windows JSON escaping for paths
    target_cif_json = target_cif_str.replace("\\", "\\\\")
    motif_lib_json = motif_lib_str.replace("\\", "\\\\")

    # JSON payload examples we want the agent to send
    motif_decomp_json = (
        "{"
        f"\"mode\": \"all\", "
        f"\"cif_path\": \"{target_cif_json}\", "
        f"\"motif_library_path\": \"{motif_lib_json}\", "
        "\"allow_overlap\": true"
        "}"
    )

    motif_compare_json_template = (
        "{{"
        "\"cif_path_1\": \"{cif1}\", "
        "\"cif_path_2\": \"{cif2}\", "
        f"\"motif_library_path\": \"{motif_lib_json}\", "
        "\"allow_overlap\": true"
        "}}"
    )

    test_prompt = f"""
You are wired into a ChemCrow environment with the following tools available
(at least):

  - Python_REPL
  - Wikipedia
  - Mol2CAS
  - PatentCheck
  - SMILES2Weight
  - FunctionalGroups

  - ListCrystalCIFs
  - VastraVisualise
  - MotifDecomposition
  - MotifComparison
  - ArxivLiteratureSearch

Your job is to exercise ALL of the custom tools at least once in a sensible way.

The CIF directory is:

  {crystal_dir_str}

A specific CIF of interest is:

  {target_cif_str}

The motif library (simple COF motifs) is at:

  {motif_lib_str}

Follow these steps, using tool calls explicitly:

1) Call **ListCrystalCIFs** once (with any string, e.g. empty) to list the
   available CIF files. Report the list back to me.

2) Pick a CIF file from the list. IF the file
   "{target_cif_str}"
   is present, prefer that one. Use that full absolute path in all later steps.

3) Call **MotifDecomposition** once on that CIF with EXACTLY the following JSON
   string as the input (do not alter it other than inserting the correct slashes):

   {motif_decomp_json}

   Then:
   - Summarise how many motif instances were found, grouped by motif_name.
   - State how many unassigned_sites there are.

4) Call **VastraVisualise** once on the SAME CIF, with the full absolute path
   as the input (e.g. "{target_cif_str}").
   Wait for the tool result. Tell me where the PNG was written.

5) If there is at least one OTHER CIF in the directory, call **MotifComparison**
   once, comparing the primary CIF from step (3) with a second CIF.
   Use a JSON input of the form:

   {{"cif_path_1": "FULL_PATH_TO_FIRST_CIF",
     "cif_path_2": "FULL_PATH_TO_SECOND_CIF",
     "motif_library_path": "{motif_lib_str}",
     "allow_overlap": true}}

   Then:
   - List which motifs are shared between the two structures, with their counts.
   - List which motifs are unique to each structure.

6) Call **ArxivLiteratureSearch** once to answer the following question:

   "How are covalent organic frameworks (COFs) used for gas storage applications?"

   Answer based ONLY on ArxivLiteratureSearch outputs. Summarise in 3â€“5 sentences.

7) At the end, briefly confirm which tools you actually invoked
   (ListCrystalCIFs, MotifDecomposition, MotifComparison, VastraVisualise,
   ArxivLiteratureSearch) and what each one returned at a high level.

Important:
- Do NOT fabricate tool outputs; rely entirely on actual tool calls.
- If any tool fails (e.g. missing file, VESTA not installed, motif library missing),
  clearly report the error message and continue with the remaining steps.
"""

    print("\n=== TEST PROMPT (exercise all custom tools) ===\n")
    print(test_prompt)
    print("\n=== MODEL RESPONSE ===\n")

    answer = chem_model.run(test_prompt)
    print(answer)


if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: test_all_tools.py ===
================================================================================

```python
# test_all_tools.py
"""
End-to-end test script that tells ChemCrow to exercise all custom tools:

  - ListCrystalCIFs
  - VastraVisualise
  - MotifDecomposition
  - MotifComparison
  - ArxivLiteratureSearch
  - COFMultiObjectiveBO

Assumes:
  - clean_chemcrow_minimal.py defines build_clean_chemcrow, DEFAULT_CRYSTAL_DIR, TARGET_CIF, DEFAULT_MOTIF_LIB.
  - motifs_cof_simple.json exists at DEFAULT_MOTIF_LIB.
  - COFMultiObjectiveBO is registered as a tool inside build_clean_chemcrow.
  - COF dataset lives at:

      CIF directory:
        C:\\Users\\brown\\Downloads\\COF_crystals\\crystals

      Descriptor CSV:
        C:\\Users\\brown\\Downloads\\COF_crystals\\cof_descriptors.csv

      Property CSV:
        C:\\Users\\brown\\Downloads\\COF_crystals\\gcmc_calculations.csv
"""

import os

from clean_chemcrow_minimal import (
    build_clean_chemcrow,
    DEFAULT_CRYSTAL_DIR,
    TARGET_CIF,
    DEFAULT_MOTIF_LIB,
)


# --- COF BO dataset paths (Windows) --- #

COF_CIF_DIR = r"C:\Users\brown\Downloads\COF_crystals\crystals"
COF_DESCRIPTOR_CSV = r"C:\Users\brown\Downloads\COF_crystals\cof_descriptors.csv"
COF_PROPERTY_CSV = r"C:\Users\brown\Downloads\COF_crystals\gcmc_calculations.csv"


def main():
    # Make sure OpenAI key is visible
    if not os.getenv("OPENAI_API_KEY"):
        print("WARNING: OPENAI_API_KEY is not set. Tools that call OpenAI may fail.\n")

    chem_model = build_clean_chemcrow()

    target_cif_str = str(TARGET_CIF)
    crystal_dir_str = str(DEFAULT_CRYSTAL_DIR)
    motif_lib_str = str(DEFAULT_MOTIF_LIB)

    # Windows JSON escaping for motif/CIF paths
    target_cif_json = target_cif_str.replace("\\", "\\\\")
    motif_lib_json = motif_lib_str.replace("\\", "\\\\")

    # JSON payload for MotifDecomposition
    motif_decomp_json = (
        "{"
        f"\"mode\": \"all\", "
        f"\"cif_path\": \"{target_cif_json}\", "
        f"\"motif_library_path\": \"{motif_lib_json}\", "
        "\"allow_overlap\": true"
        "}"
    )

    motif_compare_json_template = (
        "{{"
        "\"cif_path_1\": \"{cif1}\", "
        "\"cif_path_2\": \"{cif2}\", "
        f"\"motif_library_path\": \"{motif_lib_json}\", "
        "\"allow_overlap\": true"
        "}}"
    )

    # --- JSON payload for COFMultiObjectiveBO (multi-objective BO) --- #

    cof_cif_dir_json = COF_CIF_DIR.replace("\\", "\\\\")
    cof_desc_csv_json = COF_DESCRIPTOR_CSV.replace("\\", "\\\\")
    cof_prop_csv_json = COF_PROPERTY_CSV.replace("\\", "\\\\")

    # We maximise two objectives:
    #   1. "âŸ¨NâŸ© (mmol/g)"      â€“ uptake per gram
    #   2. "selectivity Xe/Kr" â€“ separation performance
    cof_mobo_config_json = (
        "{"
        f"\"cif_dir\": \"{cof_cif_dir_json}\", "
        f"\"descriptor_csv\": \"{cof_desc_csv_json}\", "
        f"\"property_csvs\": [\"{cof_prop_csv_json}\"], "
        "\"target_properties\": ["
        "\"âŸ¨NâŸ© (mmol/g)\", "
        "\"selectivity Xe/Kr\""
        "], "
        "\"id_column\": \"crystal_name\", "
        "\"property_agg\": \"mean\", "
        "\"top_k\": 15, "
        "\"n_weight_samples\": 6"
        "}"
    )

    test_prompt = f"""
You are wired into a ChemCrow environment with the following tools available
(at least):

  - Python_REPL
  - Wikipedia
  - Mol2CAS
  - PatentCheck
  - SMILES2Weight
  - FunctionalGroups

  - ListCrystalCIFs
  - VastraVisualise
  - MotifDecomposition
  - MotifComparison
  - ArxivLiteratureSearch
  - COFMultiObjectiveBO

Your job is to exercise ALL of the custom tools at least once in a sensible way.

The CIF directory for the motif tools is:

  {crystal_dir_str}

A specific CIF of interest is:

  {target_cif_str}

The motif library (simple COF motifs) is at:

  {motif_lib_str}

Additionally, there is a separate COF dataset for Bayesian optimisation:

  COF CIF directory:
    {COF_CIF_DIR}

  COF descriptor CSV:
    {COF_DESCRIPTOR_CSV}

  COF property CSV:
    {COF_PROPERTY_CSV}

The COFMultiObjectiveBO tool performs **multi-objective Bayesian optimisation**
over this COF dataset.

Follow these steps, using tool calls explicitly:

1) Call **ListCrystalCIFs** once (with any string, e.g. empty) to list the
   available CIF files in the DEFAULT_CRYSTAL_DIR. Report the list back to me.

2) Pick a CIF file from the list. IF the file
   "{target_cif_str}"
   is present, prefer that one. Use that full absolute path in all later steps.

3) Call **MotifDecomposition** once on that CIF with EXACTLY the following JSON
   string as the input (do not alter it other than inserting the correct slashes):

   {motif_decomp_json}

   Then:
   - Summarise how many motif instances were found, grouped by motif_name.
   - State how many unassigned_sites there are.

4) Call **VastraVisualise** once on the SAME CIF, with the full absolute path
   as the input (e.g. "{target_cif_str}").
   Wait for the tool result. Tell me where the PNG was written.

5) If there is at least one OTHER CIF in the directory, call **MotifComparison**
   once, comparing the primary CIF from step (3) with a second CIF.
   Use a JSON input of the form:

   {{
     "cif_path_1": "FULL_PATH_TO_FIRST_CIF",
     "cif_path_2": "FULL_PATH_TO_SECOND_CIF",
     "motif_library_path": "{motif_lib_str}",
     "allow_overlap": true
   }}

   Then:
   - List which motifs are shared between the two structures, with their counts.
   - List which motifs are unique to each structure.

6) Call **ArxivLiteratureSearch** once to answer the following question:

   "How are covalent organic frameworks (COFs) used for gas storage applications?"

   Answer based ONLY on ArxivLiteratureSearch outputs. Summarise in 3â€“5 sentences.

7) Call **COFMultiObjectiveBO** once on the COF dataset, using EXACTLY the
   following JSON string as the tool input (do NOT modify this string):

   {cof_mobo_config_json}

   This JSON configures multi-objective BO to MAXIMISE two objectives:

     - "âŸ¨NâŸ© (mmol/g)"      (uptake per gram)
     - "selectivity Xe/Kr" (Xe/Kr separation performance)

   After the tool returns, summarise:
   - How many COFs were considered.
   - Which COFs are currently on the observed Pareto front (list the crystal id
     and values for both objectives).
   - The top BO suggestions (up to 15), with their predicted values, uncertainties,
     and EI-based ranking.

8) At the end, briefly confirm which tools you actually invoked
   (ListCrystalCIFs, MotifDecomposition, MotifComparison, VastraVisualise,
   ArxivLiteratureSearch, COFMultiObjectiveBO) and what each one returned at a
   high level.

Important:
- Do NOT fabricate tool outputs; rely entirely on actual tool calls.
- If any tool fails (e.g. missing file, VESTA not installed, motif library missing,
  COF CSVs missing, or wrong column names), clearly report the error message and
  continue with the remaining steps where possible.
"""

    print("\n=== TEST PROMPT (exercise all custom tools + BO) ===\n")
    print(test_prompt)
    print("\n=== MODEL RESPONSE ===\n")

    answer = chem_model.run(test_prompt)
    print(answer)


if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: test_cof_multi_bo.py ===
================================================================================

```python
"""
test_cof_multi_bo.py

Explainable test for the COFMultiObjectiveBO ChemCrow tool.

This script:
  - Builds the clean ChemCrow instance.
  - Constructs a JSON config for multi-objective BO over your COF dataset.
  - Asks the agent (via natural language) to call COFMultiObjectiveBO exactly once
    with that JSON and to summarise the results (Pareto front + BO suggestions).

Assumptions:
  - clean_chemcrow_minimal.py defines build_clean_chemcrow().
  - COFMultiObjectiveBO has been imported and registered as a tool
    in build_clean_chemcrow().
  - Data layout:

      CIF directory:
        C:\\Users\\brown\\Downloads\\COF_crystals\\crystals

      Descriptor CSV:
        C:\\Users\\brown\\Downloads\\COF_crystals\\cof_descriptors.csv

      Property CSV:
        C:\\Users\\brown\\Downloads\\COF_crystals\\gcmc_calculations.csv

  - We maximise two objectives:

      1. "âŸ¨NâŸ© (mmol/g)"      # uptake per gram
      2. "selectivity Xe/Kr" # separation performance
"""

import os

from clean_chemcrow_minimal import build_clean_chemcrow


# --- Local paths (Windows) --- #

CIF_DIR = r"C:\Users\brown\Downloads\COF_crystals\crystals"
DESCRIPTOR_CSV = r"C:\Users\brown\Downloads\COF_crystals\cof_descriptors.csv"
PROPERTY_CSV = r"C:\Users\brown\Downloads\COF_crystals\gcmc_calculations.csv"


def main():
    # Warn if OPENAI key is missing (ChemCrow will probably need it)
    if not os.getenv("OPENAI_API_KEY"):
        print("WARNING: OPENAI_API_KEY is not set. Some tools may fail.\n")

    chemcrow = build_clean_chemcrow()

    # Escape backslashes for JSON
    cif_dir_json = CIF_DIR.replace("\\", "\\\\")
    desc_csv_json = DESCRIPTOR_CSV.replace("\\", "\\\\")
    prop_csv_json = PROPERTY_CSV.replace("\\", "\\\\")

    # JSON config for COFMultiObjectiveBO
    # Two objectives to MAXIMISE:
    #   - "âŸ¨NâŸ© (mmol/g)"      (uptake)
    #   - "selectivity Xe/Kr" (separation)
    cof_mobo_config_json = (
        "{"
        f"\"cif_dir\": \"{cif_dir_json}\", "
        f"\"descriptor_csv\": \"{desc_csv_json}\", "
        f"\"property_csvs\": [\"{prop_csv_json}\"], "
        "\"target_properties\": ["
        "\"âŸ¨NâŸ© (mmol/g)\", "
        "\"selectivity Xe/Kr\""
        "], "
        "\"id_column\": \"crystal_name\", "
        "\"property_agg\": \"mean\", "
        "\"top_k\": 15, "
        "\"n_weight_samples\": 6"
        "}"
    )

    # Natural-language test prompt that tells the agent exactly what to do
    test_prompt = f"""
You are running inside a ChemCrow environment with a custom tool:

  - COFMultiObjectiveBO

The COFMultiObjectiveBO tool performs **multi-objective Bayesian optimisation**
over a library of COFs. It expects a SINGLE STRING input which MUST be a JSON
blob with the following fields:

  - "cif_dir": directory containing CIF files (optional but recommended)
  - "descriptor_csv": CSV with numeric descriptors per crystal
  - "property_csvs": list of CSV files with properties per crystal
  - "target_properties": list of property column names to MAXIMISE
  - "id_column": common crystal ID column across all CSVs
  - "property_agg": aggregation method across repeated measurements ("mean", "max", "min")
  - "top_k": number of BO suggestions to return
  - optional: "weights" and/or "n_weight_samples" for scalarisation

For this test, the dataset is:

  CIF directory:
    {CIF_DIR}

  Descriptor CSV:
    {DESCRIPTOR_CSV}

  Property CSV:
    {PROPERTY_CSV}

We will MAXIMISE TWO objectives taken from the property CSV:

  1. "âŸ¨NâŸ© (mmol/g)"      â€“ uptake per gram.
  2. "selectivity Xe/Kr" â€“ Xe/Kr separation performance.

Here is the EXACT JSON config you must pass to COFMultiObjectiveBO
as its tool input (do NOT modify this string):

  {cof_mobo_config_json}

Your task:

1. Briefly explain, in your own words, what these two objectives represent
   physically and why they are sensible to maximise together in a COF screen
   (no tools needed for this part).

2. Call COFMultiObjectiveBO **exactly once**, using the JSON string above
   as the tool input.

3. After the tool returns, summarise the result:
   - How many COFs were considered in total.
   - Which COFs currently form the **observed Pareto front**, listing for each:
       * crystal identifier
       * values of both objectives ("âŸ¨NâŸ© (mmol/g)" and "selectivity Xe/Kr").
   - The **top BO suggestions** (up to 15), including:
       * crystal identifier
       * predicted values and uncertainties for each objective
       * EI score (or ranking) used to select them.

4. Discuss briefly:
   - How the BO suggestions differ from the current Pareto front.
   - Any interesting trade-offs (e.g. high uptake but moderate selectivity,
     or vice versa).
   - Why this scalarisation-based approach is a reasonable approximation
     to multi-objective BO, and what its limitations are.

Important:
- Do NOT fabricate crystal names or numeric values; rely ONLY on what
  COFMultiObjectiveBO returns.
- If the tool fails (e.g. missing files or wrong column names), clearly report
  the error and stop.
"""

    print("\n=== TEST PROMPT (COFMultiObjectiveBO) ===\n")
    print(test_prompt)
    print("\n=== MODEL RESPONSE ===\n")

    answer = chemcrow.run(test_prompt)
    print(answer)


if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: chemcrow\utils.py ===
================================================================================

```python
import re

import requests
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem


def is_smiles(text):
    try:
        m = Chem.MolFromSmiles(text, sanitize=False)
        if m is None:
            return False
        return True
    except:
        return False


def is_multiple_smiles(text):
    if is_smiles(text):
        return "." in text
    return False


def split_smiles(text):
    return text.split(".")


def is_cas(text):
    pattern = r"^\d{2,7}-\d{2}-\d$"
    return re.match(pattern, text) is not None


def largest_mol(smiles):
    ss = smiles.split(".")
    ss.sort(key=lambda a: len(a))
    while not is_smiles(ss[-1]):
        rm = ss[-1]
        ss.remove(rm)
    return ss[-1]


def canonical_smiles(smiles):
    try:
        smi = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), canonical=True)
        return smi
    except Exception:
        return "Invalid SMILES string"


def tanimoto(s1, s2):
    """Calculate the Tanimoto similarity of two SMILES strings."""
    try:
        mol1 = Chem.MolFromSmiles(s1)
        mol2 = Chem.MolFromSmiles(s2)
        fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)
        fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)
        return DataStructs.TanimotoSimilarity(fp1, fp2)
    except (TypeError, ValueError, AttributeError):
        return "Error: Not a valid SMILES string"


def pubchem_query2smiles(
    query: str,
    url: str = "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/{}",
) -> str:
    if is_smiles(query):
        if not is_multiple_smiles(query):
            return query
        else:
            raise ValueError(
                "Multiple SMILES strings detected, input one molecule at a time."
            )
    if url is None:
        url = "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/{}"
    r = requests.get(url.format(query, "property/IsomericSMILES/JSON"))
    # convert the response to a json object
    data = r.json()
    # return the SMILES string
    try:
        smi = data["PropertyTable"]["Properties"][0]["IsomericSMILES"]
    except KeyError:
        return "Could not find a molecule matching the text. One possible cause is that the input is incorrect, input one molecule at a time."
    return str(Chem.CanonSmiles(largest_mol(smi)))


def query2cas(query: str, url_cid: str, url_data: str):
    try:
        mode = "name"
        if is_smiles(query):
            if is_multiple_smiles(query):
                raise ValueError(
                    "Multiple SMILES strings detected, input one molecule at a time."
                )
            mode = "smiles"
        url_cid = url_cid.format(mode, query)
        cid = requests.get(url_cid).json()["IdentifierList"]["CID"][0]
        url_data = url_data.format(cid)
        data = requests.get(url_data).json()
    except (requests.exceptions.RequestException, KeyError):
        raise ValueError("Invalid molecule input, no Pubchem entry")

    try:
        for section in data["Record"]["Section"]:
            if section.get("TOCHeading") == "Names and Identifiers":
                for subsection in section["Section"]:
                    if subsection.get("TOCHeading") == "Other Identifiers":
                        for subsubsection in subsection["Section"]:
                            if subsubsection.get("TOCHeading") == "CAS":
                                return subsubsection["Information"][0]["Value"][
                                    "StringWithMarkup"
                                ][0]["String"]
    except KeyError:
        raise ValueError("Invalid molecule input, no Pubchem entry")

    raise ValueError("CAS number not found")


def smiles2name(smi, single_name=True):
    """This function queries the given molecule smiles and returns a name record or iupac"""

    try:
        smi = Chem.MolToSmiles(Chem.MolFromSmiles(smi), canonical=True)
    except Exception:
        raise ValueError("Invalid SMILES string")
    # query the PubChem database
    r = requests.get(
        "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/smiles/"
        + smi
        + "/synonyms/JSON"
    )
    # convert the response to a json object
    data = r.json()
    # return the SMILES string
    try:
        if single_name:
            index = 0
            names = data["InformationList"]["Information"][0]["Synonym"]
            while is_cas(name := names[index]):
                index += 1
                if index == len(names):
                    raise ValueError("No name found")
        else:
            name = data["InformationList"]["Information"][0]["Synonym"]
    except KeyError:
        raise ValueError("Unknown Molecule")
    return name

```


================================================================================
=== FILE: chemcrow\version.py ===
================================================================================

```python
"""Package version"""

__version__ = "0.3.24"

```


================================================================================
=== FILE: chemcrow\__init__.py ===
================================================================================

```python
from .tools.rdkit import *
from .tools.search import *
from .frontend import *
from .agents import ChemCrow, make_tools
from .version import __version__

```


================================================================================
=== FILE: chemcrow\agents\chemcrow.py ===
================================================================================

```python
from typing import Optional

import langchain
from dotenv import load_dotenv
from langchain import PromptTemplate, chains
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from pydantic import ValidationError
from rmrkl import ChatZeroShotAgent, RetryAgentExecutor

from .prompts import FORMAT_INSTRUCTIONS, QUESTION_PROMPT, REPHRASE_TEMPLATE, SUFFIX
from .tools import make_tools


def _make_llm(model, temp, api_key, streaming: bool = False):
    if model.startswith("gpt-3.5-turbo") or model.startswith("gpt-4"):
        llm = langchain.chat_models.ChatOpenAI(
            temperature=temp,
            model_name=model,
            request_timeout=1000,
            streaming=streaming,
            callbacks=[StreamingStdOutCallbackHandler()],
            openai_api_key=api_key,
        )
    elif model.startswith("text-"):
        llm = langchain.OpenAI(
            temperature=temp,
            model_name=model,
            streaming=streaming,
            callbacks=[StreamingStdOutCallbackHandler()],
            openai_api_key=api_key,
        )
    else:
        raise ValueError(f"Invalid model name: {model}")
    return llm


class ChemCrow:
    def __init__(
        self,
        tools=None,
        model="gpt-4-0613",
        tools_model="gpt-3.5-turbo-0613",
        temp=0.1,
        max_iterations=40,
        verbose=True,
        streaming: bool = True,
        openai_api_key: Optional[str] = None,
        api_keys: dict = {},
        local_rxn: bool = False,
    ):
        """Initialize ChemCrow agent."""

        load_dotenv()
        try:
            self.llm = _make_llm(model, temp, openai_api_key, streaming)
        except ValidationError:
            raise ValueError("Invalid OpenAI API key")

        if tools is None:
            api_keys["OPENAI_API_KEY"] = openai_api_key
            tools_llm = _make_llm(tools_model, temp, openai_api_key, streaming)
            tools = make_tools(tools_llm, api_keys=api_keys, local_rxn=local_rxn, verbose=verbose)

        # Initialize agent
        self.agent_executor = RetryAgentExecutor.from_agent_and_tools(
            tools=tools,
            agent=ChatZeroShotAgent.from_llm_and_tools(
                self.llm,
                tools,
                suffix=SUFFIX,
                format_instructions=FORMAT_INSTRUCTIONS,
                question_prompt=QUESTION_PROMPT,
            ),
            verbose=True,
            max_iterations=max_iterations,
        )

        rephrase = PromptTemplate(
            input_variables=["question", "agent_ans"], template=REPHRASE_TEMPLATE
        )

        self.rephrase_chain = chains.LLMChain(prompt=rephrase, llm=self.llm)

    def run(self, prompt):
        outputs = self.agent_executor({"input": prompt})
        return outputs["output"]

```


================================================================================
=== FILE: chemcrow\agents\prompts.py ===
================================================================================

```python
# flake8: noqa
PREFIX = """
You are an expert chemist and your task is to respond to the question or
solve the problem to the best of your ability using the provided tools.
"""

FORMAT_INSTRUCTIONS = """
You can only respond with a single complete
"Thought, Action, Action Input" format
OR a single "Final Answer" format.

Complete format:

Thought: (reflect on your progress and decide what to do next)
Action: (the action name, should be one of [{tool_names}])
Action Input: (the input string to the action)

OR

Final Answer: (the final answer to the original input question)
"""

QUESTION_PROMPT = """
Answer the question below using the following tools:

{tool_strings}

Use the tools provided, using the most specific tool available for each action.
Your final answer should contain all information necessary to answer the question and subquestions.

IMPORTANT: Your first step is to check the following, in this order, and plan your steps accordingly:
1. Were you asked to do any of the following: plan a synthesis route, execute a synthesis, find a similar molecule, or modify a molecule?
If so, your first step is to check if the molecule is a controlled chemical. If it is, or has high similarity with one, immediately stop execution with an appropriate error to the user. Do not continue.
2. Does the question involve any molecules? If so, as a first step, check if any are controlled chemicals. If any are, include a warning in your final answer.
3. Were you asked to plan a synthesis route? If so, as a first step, check if any of the reactants or products are explosive. If any are, include a warning in your final answer.
4. Were you asked to execute a synthesis route? If so, check if any of the reactants or products are explosive. If any are, ask the user for permission to continue.
Do not skip these steps.


Question: {input}
"""

SUFFIX = """
Thought: {agent_scratchpad}
"""
FINAL_ANSWER_ACTION = "Final Answer:"


REPHRASE_TEMPLATE = """In this exercise you will assume the role of a scientific assistant. Your task is to answer the provided question as best as you can, based on the provided solution draft.
The solution draft follows the format "Thought, Action, Action Input, Observation", where the 'Thought' statements describe a reasoning sequence. The rest of the text is information obtained to complement the reasoning sequence, and it is 100% accurate.
Your task is to write an answer to the question based on the solution draft, and the following guidelines:
The text should have an educative and assistant-like tone, be accurate, follow the same reasoning sequence than the solution draft and explain how any conclusion is reached.
Question: {question}

Solution draft: {agent_ans}

Answer:
"""

```


================================================================================
=== FILE: chemcrow\agents\tools.py ===
================================================================================

```python
import os

from langchain import agents
from langchain.base_language import BaseLanguageModel

from chemcrow.tools import *


def make_tools(llm: BaseLanguageModel, api_keys: dict = {}, local_rxn: bool=False, verbose=True):
    serp_api_key = api_keys.get("SERP_API_KEY") or os.getenv("SERP_API_KEY")
    rxn4chem_api_key = api_keys.get("RXN4CHEM_API_KEY") or os.getenv("RXN4CHEM_API_KEY")
    openai_api_key = api_keys.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    chemspace_api_key = api_keys.get("CHEMSPACE_API_KEY") or os.getenv(
        "CHEMSPACE_API_KEY"
    )
    semantic_scholar_api_key = api_keys.get("SEMANTIC_SCHOLAR_API_KEY") or os.getenv(
        "SEMANTIC_SCHOLAR_API_KEY"
    )

    all_tools = agents.load_tools(
        [
            "python_repl",
            # "ddg-search",
            "wikipedia",
            # "human"
        ]
    )

    all_tools += [
        Query2SMILES(chemspace_api_key),
        Query2CAS(),
        SMILES2Name(),
        PatentCheck(),
        MolSimilarity(),
        SMILES2Weight(),
        FuncGroups(),
        ExplosiveCheck(),
        ControlChemCheck(),
        SimilarControlChemCheck(),
        SafetySummary(llm=llm),
        Scholar2ResultLLM(
            llm=llm,
            openai_api_key=openai_api_key,
            semantic_scholar_api_key=semantic_scholar_api_key
        ),
    ]
    if chemspace_api_key:
        all_tools += [GetMoleculePrice(chemspace_api_key)]
    if serp_api_key:
        all_tools += [WebSearch(serp_api_key)]
    if (not local_rxn) and rxn4chem_api_key:
        all_tools += [
            RXNPredict(rxn4chem_api_key),
            RXNRetrosynthesis(rxn4chem_api_key, openai_api_key),
        ]
    elif local_rxn:
        all_tools += [
            RXNPredictLocal(),
            RXNRetrosynthesisLocal()
        ]

    return all_tools

```


================================================================================
=== FILE: chemcrow\agents\__init__.py ===
================================================================================

```python
from .chemcrow import ChemCrow
from .tools import make_tools

__all__ = ["ChemCrow", "make_tools"]

```


================================================================================
=== FILE: chemcrow\data\chem_wep.csv ===
================================================================================

```
ï»¿CAS No.,Source
(111-48-8),australia group
(10025-87-3),australia group
(756-79-6),australia group
(676-99-3),australia group
(676-97-1),australia group
(868-85-9),australia group
(7719-12-2),australia group
(121-45-9),australia group
(7719-09-7),australia group
(3554-74-3),australia group
(96-79-7),australia group
(5842-07-9),australia group
(1619-34-7),australia group
(7789-23-3),australia group
(107-07-3),australia group
(124-40-3),australia group
(78-38-6),australia group
(2404-03-7),australia group
(762-04-9),australia group
(506-59-2),australia group
(1498-40-4),australia group
(1066-50-8),australia group
(753-98-0),australia group
(7664-39-3),australia group
(76-89-1),australia group
(676-83-5),australia group
(96-80-0),australia group
(464-07-3),australia group
(57856-11-8),australia group
(122-52-1),australia group
(7784-34-1),australia group
(76-93-7),australia group
(15715-41-0),australia group
(6163-75-3),australia group
(430-78-4),australia group
(753-59-3),australia group
(3731-38-2),australia group
(10026-13-8),australia group
(75-97-8),australia group
(151-50-8),australia group
(7789-29-9),australia group
(1341-49-7),australia group
(1333-83-1),australia group
(7681-49-4),australia group
(143-33-9),australia group
(102-71-6),australia group
(1314-80-3),australia group
(108-18-9),australia group
(100-37-8),australia group
(1313-82-2),australia group
(10025-67-9),australia group
(10545-99-0),australia group
(637-39-8),australia group
(4261-68-1),australia group
(993-13-5),australia group
(683-08-9),australia group
(677-43-0),australia group
(116-17-6),australia group
(139-87-7),australia group
(2465-65-8),australia group
(298-06-6),australia group
(16893-85-9),australia group
(676-98-2),australia group
(109-89-7),australia group
(41480-75-5),australia group
(677-24-7),australia group
(1498-51-7),australia group
(22382-13-4),australia group
(460-52-6),australia group
(589-57-1),australia group
(754-01-8),australia group
(762-77-6),australia group
(44205-42-7),australia group
(90324-67-7),australia group
(48044-20-8),australia group
(857522-08-8),australia group
(2909-14-0),australia group
(14277-06-6),australia group
(1339586-99-0),australia group
(56776-14-8),australia group
(84764-73-8),australia group
(1341496-89-6),australia group
(1340437-35-5),australia group
(53510-30-8),australia group
(1342422-35-8),australia group
(1315467-17-4),australia group
(321881-25-8),australia group
(1342789-47-2),australia group
(1342700-45-1),australia group
(107-44-8),OPCW schedule 1
(96-64-0),OPCW schedule 1
(77-81-6),OPCW schedule 1
(50782-69-9),OPCW schedule 1
(2625-76-5),OPCW schedule 1
(505-60-2),OPCW schedule 1
(63869-13-6),OPCW schedule 1
(3563-36-8),OPCW schedule 1
(63905-10-2),OPCW schedule 1
(142868-93-7),OPCW schedule 1
(142868-94-8),OPCW schedule 1
(63918-90-1),OPCW schedule 1
(63918-89-8),OPCW schedule 1
(541-25-3),OPCW schedule 1
(40334-69-8),OPCW schedule 1
(40334-70-1),OPCW schedule 1
(538-07-8),OPCW schedule 1
(51-75-2),OPCW schedule 1
(555-77-1),OPCW schedule 1
(35523-89-8),OPCW schedule 1
(9009-86-3),OPCW schedule 1
(2387495-99-8),OPCW schedule 1
(2387496-12-8),OPCW schedule 1
(2387496-00-4),OPCW schedule 1
(2387496-04-8),OPCW schedule 1
(2387496-06-0),OPCW schedule 1
(2387496-14-0),OPCW schedule 1
(77104-62-2),OPCW schedule 1
(77104-00-8),OPCW schedule 1
(676-99-3),OPCW schedule 1
(57856-11-8),OPCW schedule 1
(1445-76-7),OPCW schedule 1
(7040-57-5),OPCW schedule 1
(382-21-8),OPCW schedule 2
(6581-06-2),OPCW schedule 2
(676-97-1),OPCW schedule 2
(756-79-6),OPCW schedule 2
(7784-34-1),OPCW schedule 2
(76-93-7),OPCW schedule 2
(1619-34-7),OPCW schedule 2
(111-48-8),OPCW schedule 2
(464-07-3),OPCW schedule 2
(75-44-5),OPCW schedule 3
(506-77-4),OPCW schedule 3
(74-90-8),OPCW schedule 3
(76-06-2),OPCW schedule 3
(10025-87-3),OPCW schedule 3
(7719-12-2),OPCW schedule 3
(10026-13-8),OPCW schedule 3
(121-45-9),OPCW schedule 3
(122-52-1),OPCW schedule 3
(868-85-9),OPCW schedule 3
(762-04-9),OPCW schedule 3
(10025-67-9),OPCW schedule 3
(10545-99-0),OPCW schedule 3
(7719-09-7),OPCW schedule 3
(139-87-7),OPCW schedule 3
(105-59-9),OPCW schedule 3
(102-71-6),OPCW schedule 3

```


================================================================================
=== FILE: chemcrow\data\chem_wep_smi.csv ===
================================================================================

```
cas,source,smiles
(111-48-8),australia group,OCCSCCO
(10025-87-3),australia group,O=P(Cl)(Cl)Cl
(756-79-6),australia group,COP(C)(=O)OC
(676-99-3),australia group,CP(=O)(F)F
(676-97-1),australia group,CP(=O)(Cl)Cl
(868-85-9),australia group,CO[P+](=O)OC
(7719-12-2),australia group,ClP(Cl)Cl
(121-45-9),australia group,COP(OC)OC
(7719-09-7),australia group,O=S(Cl)Cl
(3554-74-3),australia group,CN1CCCC(O)C1
(96-79-7),australia group,CC(C)N(CCCl)C(C)C
(5842-07-9),australia group,CC(C)N(CCS)C(C)C
(1619-34-7),australia group,OC1CN2CCC1CC2
(7789-23-3),australia group,[K+]
(107-07-3),australia group,OCCCl
(124-40-3),australia group,CNC
(78-38-6),australia group,CCOP(=O)(CC)OCC
(2404-03-7),australia group,CCOP(=O)(OCC)N(C)C
(762-04-9),australia group,CCO[P+](=O)OCC
(506-59-2),australia group,CNC
(1498-40-4),australia group,CCP(Cl)Cl
(1066-50-8),australia group,CCP(=O)(Cl)Cl
(753-98-0),australia group,CCP(=O)(F)F
(7664-39-3),australia group,F
(76-89-1),australia group,COC(=O)C(O)(C1=CC=CC=C1)C1=CC=CC=C1
(676-83-5),australia group,CP(Cl)Cl
(96-80-0),australia group,CC(C)N(CCO)C(C)C
(464-07-3),australia group,CC(O)C(C)(C)C
(57856-11-8),australia group,CCOP(C)OCCN(C(C)C)C(C)C
(122-52-1),australia group,CCOP(OCC)OCC
(7784-34-1),australia group,Cl[As](Cl)Cl
(76-93-7),australia group,O=C(O)C(O)(C1=CC=CC=C1)C1=CC=CC=C1
(15715-41-0),australia group,CCOP(C)OCC
(6163-75-3),australia group,CCP(=O)(OC)OC
(430-78-4),australia group,CCP(F)F
(753-59-3),australia group,CP(F)F
(3731-38-2),australia group,O=C1CN2CCC1CC2
(10026-13-8),australia group,ClP(Cl)(Cl)(Cl)Cl
(75-97-8),australia group,CC(=O)C(C)(C)C
(151-50-8),australia group,[C-]#N
(7789-29-9),australia group,C
(1341-49-7),australia group,[NH4+]
(1333-83-1),australia group,[Na+]
(7681-49-4),australia group,[Na+]
(143-33-9),australia group,[C-]#N
(102-71-6),australia group,OCCN(CCO)CCO
(1314-80-3),australia group,C
(108-18-9),australia group,CC(C)NC(C)C
(100-37-8),australia group,CCN(CC)CCO
(1313-82-2),australia group,[SH-]
(10025-67-9),australia group,ClSSCl
(10545-99-0),australia group,ClSCl
(637-39-8),australia group,OCCN(CCO)CCO
(4261-68-1),australia group,CC(C)N(CCCl)C(C)C
(993-13-5),australia group,CP(=O)(O)O
(683-08-9),australia group,CCOP(C)(=O)OCC
(677-43-0),australia group,CN(C)P(=O)(Cl)Cl
(116-17-6),australia group,CC(C)OP(OC(C)C)OC(C)C
(139-87-7),australia group,CCN(CCO)CCO
(2465-65-8),australia group,CCOP(O)(=S)OCC
(298-06-6),australia group,CCOP(=S)(S)OCC
(16893-85-9),australia group,F[Si-2](F)(F)(F)(F)F
(676-98-2),australia group,CP(=S)(Cl)Cl
(109-89-7),australia group,CCNCC
(41480-75-5),australia group,CC(C)N(CCS)C(C)C
(677-24-7),australia group,COP(=O)(Cl)Cl
(1498-51-7),australia group,CCOP(=O)(Cl)Cl
(22382-13-4),australia group,COP(=O)(F)F
(460-52-6),australia group,CCOP(=O)(F)F
(589-57-1),australia group,CCOP(Cl)OCC
(754-01-8),australia group,COP(=O)(F)Cl
(762-77-6),australia group,CCOP(=O)(F)Cl
(44205-42-7),australia group,CN(C)C=N
(90324-67-7),australia group,CCN(C=N)CC
(48044-20-8),australia group,CCCN(C=N)CCC
(857522-08-8),australia group,CC(C)N(C=N)C(C)C
(2909-14-0),australia group,CC(=N)N(C)C
(14277-06-6),australia group,CCN(CC)C(C)=N
(1339586-99-0),australia group,CCCN(CCC)C(C)=N
(56776-14-8),australia group,CCC(=N)N(C)C
(84764-73-8),australia group,CCC(=N)N(CC)CC
(1341496-89-6),australia group,CCCN(CCC)C(=N)CC
(1340437-35-5),australia group,CCCC(=N)N(C)C
(53510-30-8),australia group,CCCC(=N)N(CC)CC
(1342422-35-8),australia group,CCCC(=N)N(CCC)CCC
(1315467-17-4),australia group,CCCC(=N)N(C(C)C)C(C)C
(321881-25-8),australia group,CC(C)C(=N)N(C)C
(1342789-47-2),australia group,C
(1342700-45-1),australia group,C
(107-44-8),OPCW schedule 1,CC(C)OP(C)(=O)F
(96-64-0),OPCW schedule 1,CC(OP(C)(=O)F)C(C)(C)C
(77-81-6),OPCW schedule 1,CCOP(=O)(C#N)N(C)C
(50782-69-9),OPCW schedule 1,CCOP(C)(=O)SCCN(C(C)C)C(C)C
(2625-76-5),OPCW schedule 1,ClCCSCCl
(505-60-2),OPCW schedule 1,ClCCSCCCl
(63869-13-6),OPCW schedule 1,ClCCSCSCCCl
(3563-36-8),OPCW schedule 1,ClCCSCCSCCCl
(63905-10-2),OPCW schedule 1,ClCCSCCCSCCCl
(142868-93-7),OPCW schedule 1,ClCCSCCCCSCCCl
(142868-94-8),OPCW schedule 1,ClCCSCCCCCSCCCl
(63918-90-1),OPCW schedule 1,ClCCSCOCSCCCl
(63918-89-8),OPCW schedule 1,ClCCSCCOCCSCCCl
(541-25-3),OPCW schedule 1,Cl/C=C/[As](Cl)Cl
(40334-69-8),OPCW schedule 1,Cl/C=C/[As](Cl)/C=C/Cl
(40334-70-1),OPCW schedule 1,Cl/C=C/[As](/C=C/Cl)/C=C/Cl
(538-07-8),OPCW schedule 1,CCN(CCCl)CCCl
(51-75-2),OPCW schedule 1,CN(CCCl)CCCl
(555-77-1),OPCW schedule 1,ClCCN(CCCl)CCCl
(35523-89-8),OPCW schedule 1,NC(=O)OC[C@@H]1N=C(N)N2CCC(O)(O)[C@@]23NC(N)=N[C@@H]13
(9009-86-3),OPCW schedule 1,C
(2387495-99-8),OPCW schedule 1,CCCCCCCCCCN(CCCCCCCCCC)C(CCCCCCCCC)=NP(=O)(F)CCCCCCCCCC
(2387496-12-8),OPCW schedule 1,CCN(CC)C(C)=NP(C)(=O)F
(2387496-00-4),OPCW schedule 1,CCCCCCCCCCOP(=O)(F)N=C(CCCCCCCCC)N(CCCCCCCCCC)CCCCCCCCCC
(2387496-04-8),OPCW schedule 1,CCN(CC)C(C)=NP(=O)(F)OC
(2387496-06-0),OPCW schedule 1,CCOP(=O)(F)N=C(C)N(CC)CC
(2387496-14-0),OPCW schedule 1,CCN(CC)C(=NP(C)(=O)F)N(CC)CC
(77104-62-2),OPCW schedule 1,CN(C)C(=O)OC1=C(C[N+](C)(C)CCCCCCCCCC[N+](C)(C)CCO)N=CC=C1
(77104-00-8),OPCW schedule 1,CC[N+](C)(CC(=O)CCCCCCC(=O)C[N+](C)(CC)CC1=C(OC(=O)N(C)C)C=CC=N1)CC1=C(OC(=O)N(C)C)C=CC=N1
(676-99-3),OPCW schedule 1,CP(=O)(F)F
(57856-11-8),OPCW schedule 1,CCOP(C)OCCN(C(C)C)C(C)C
(1445-76-7),OPCW schedule 1,CC(C)OP(C)(=O)Cl
(7040-57-5),OPCW schedule 1,CC(OP(C)(=O)Cl)C(C)(C)C
(382-21-8),OPCW schedule 2,FC(F)=C(C(F)(F)F)C(F)(F)F
(6581-06-2),OPCW schedule 2,O=C(OC1CN2CCC1CC2)C(O)(C1=CC=CC=C1)C1=CC=CC=C1
(676-97-1),OPCW schedule 2,CP(=O)(Cl)Cl
(756-79-6),OPCW schedule 2,COP(C)(=O)OC
(7784-34-1),OPCW schedule 2,Cl[As](Cl)Cl
(76-93-7),OPCW schedule 2,O=C(O)C(O)(C1=CC=CC=C1)C1=CC=CC=C1
(1619-34-7),OPCW schedule 2,OC1CN2CCC1CC2
(111-48-8),OPCW schedule 2,OCCSCCO
(464-07-3),OPCW schedule 2,CC(O)C(C)(C)C
(75-44-5),OPCW schedule 3,O=C(Cl)Cl
(506-77-4),OPCW schedule 3,N#CCl
(74-90-8),OPCW schedule 3,C#N
(76-06-2),OPCW schedule 3,O=[N+]([O-])C(Cl)(Cl)Cl
(10025-87-3),OPCW schedule 3,O=P(Cl)(Cl)Cl
(7719-12-2),OPCW schedule 3,ClP(Cl)Cl
(10026-13-8),OPCW schedule 3,ClP(Cl)(Cl)(Cl)Cl
(121-45-9),OPCW schedule 3,COP(OC)OC
(122-52-1),OPCW schedule 3,CCOP(OCC)OCC
(868-85-9),OPCW schedule 3,CO[P+](=O)OC
(762-04-9),OPCW schedule 3,CCO[P+](=O)OCC
(10025-67-9),OPCW schedule 3,ClSSCl
(10545-99-0),OPCW schedule 3,ClSCl
(7719-09-7),OPCW schedule 3,O=S(Cl)Cl
(139-87-7),OPCW schedule 3,CCN(CCO)CCO
(105-59-9),OPCW schedule 3,CN(CCO)CCO
(102-71-6),OPCW schedule 3,OCCN(CCO)CCO

```


================================================================================
=== FILE: chemcrow\docker\README.md ===
================================================================================

```markdown

# Tools of organic chemistry

A docker container was prepared for each tool, which exposes an api for requests.

> docker run -d -p 8052:5000 doncamilom/rxnpred:latest

Where 5000 is fixed, and 8082 is the port to be exposed.

A request in curl can look like this

> curl -X POST -H "Content-Type: application/json" -d '{"smiles": "O=C(OC(C)(C)C)c1ccc(C(=O)Nc2ccc(Cl)cc2)cc1"}' http://localhost:8082/api/v1/run

Or in Python

```python

import json
import requests

def reaction_predict(reactants):
    response = requests.post(
        "http://localhost:8052/api/v1/run",
        headers={"Content-Type": "application/json"},
        data=json.dumps({"smiles": reactants})
    )
    return response.json()['product'][0]

product = reaction_predict('CCOCCCCO.CC(=O)Cl')
```

```


================================================================================
=== FILE: chemcrow\docker\aizynthfinder\app.py ===
================================================================================

```python
import json
import subprocess

from flask import Flask, jsonify, request

app = Flask(__name__)

@app.route('/api/v1/run', methods=['POST'])
def rxnfp():
    data = request.get_json()
    target = data.get("target", [])

    command = ["aizynthcli", "--config", "config.yml", "--smiles", f"{target}"]

    print(command)
    result = subprocess.run(
        command, check=True, capture_output=True, text=True
    )
    print(result)

    # Read output trees.json
    with open("trees.json", "r") as f:
        tree = json.load(f)

    return tree


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

```


================================================================================
=== FILE: chemcrow\docker\aizynthfinder\config.yml ===
================================================================================

```yaml
expansion:
  uspto:
    - files/uspto_model.onnx
    - files/uspto_templates.csv.gz
  ringbreaker:
    - files/uspto_ringbreaker_model.onnx
    - files/uspto_ringbreaker_templates.csv.gz
filter:
  uspto: files/uspto_filter_model.onnx
stock:
  zinc: files/zinc_stock.hdf5

```


================================================================================
=== FILE: chemcrow\docker\aizynthfinder\Dockerfile ===
================================================================================

```
FROM python:3.9
RUN pip install aizynthfinder[all] flask
COPY files/ .
COPY . .

EXPOSE 5000
ENTRYPOINT ["python"]

CMD ["app.py"]

```


================================================================================
=== FILE: chemcrow\docker\aizynthfinder\files\README.md ===
================================================================================

```markdown
# Download al lthe important files for runnig aizynthfinder using

```
download_public_data .
```

Which comes by installing aizynthfinder

```
pip install aizynthfinder[all]
```

```


================================================================================
=== FILE: chemcrow\docker\molecular-transformer\app.py ===
================================================================================

```python
import re
import subprocess
from flask import Flask, request, jsonify
from rdkit import Chem

app = Flask(__name__)


SMI_REGEX_PATTERN =  r"(\%\([0-9]{3}\)|\[[^\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\||\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|>>?|\*|\$|\%[0-9]{2}|[0-9])"

def canonicalize_smiles(smiles, verbose=False): # will raise an Exception if invalid SMILES
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        return Chem.MolToSmiles(mol)
    else:
        if verbose:
            print(f'{smiles} is invalid.')
        return ''

def smiles_tokenizer(smiles):
    """Canonicalize and tokenize input smiles"""
    
    smiles = canonicalize_smiles(smiles)
    smiles_regex = re.compile(SMI_REGEX_PATTERN)
    tokens = [token for token in smiles_regex.findall(smiles)]
    return ' '.join(tokens)


@app.route('/api/v1/run', methods=['POST'])
def f():
    request_data = request.get_json()
    input = request_data['smiles']

    # Write the input to 'inp.txt'
    with open('input.txt', 'w') as f:
        # Tokenize smiles
        smi = smiles_tokenizer(input)
        f.write(smi)

    model_path = 'models/USPTO480k_model_step_400000.pt'

    src_path = 'input.txt'
    output_path = 'output.txt'
    n_best = 5
    beam_size = 10
    max_length = 300
    batch_size = 1

    try:
        # Construct the command to execute
        cmd = f"onmt_translate -model {model_path} " \
              f"--src {src_path} " \
              f"--output {output_path} --n_best {n_best} " \
              f"--beam_size {beam_size} --max_length {max_length} " \
              f"--batch_size {batch_size}"

        # Execute the command using subprocess.check_call()
        subprocess.check_call(cmd, shell=True)

        # Read produced output
        with open('output.txt', 'r') as f:
            prods = f.read()
            prods = re.sub(' ', '', prods).split('\n')

        
        # Return a success message
        return jsonify({'status': 'SUCCESS', 'product': prods})

    except:
        return jsonify({'status': 'ERROR', 'product': None})

if __name__ == '__main__':
    # Run the Flask app
    app.run(debug=True, host='0.0.0.0')


```


================================================================================
=== FILE: chemcrow\docker\molecular-transformer\Dockerfile ===
================================================================================

```
FROM python:3.10
WORKDIR /app

RUN pip install rdkit-pypi==2022.3.1
RUN pip install OpenNMT-py==2.2.0 "numpy<2.0.0"

COPY . .
COPY input.txt .
COPY models/ .
CMD ["python", "app.py"]


```


================================================================================
=== FILE: chemcrow\docker\molecular-transformer\models\README.md ===
================================================================================

```markdown
# Download model from https://drive.google.com/uc?id=1ywJCJHunoPTB5wr6KdZ8aLv7tMFMBHNy

```


================================================================================
=== FILE: chemcrow\frontend\streamlit_callback_handler.py ===
================================================================================

```python
from typing import Any, Dict, List, Optional

from langchain.callbacks.streamlit.streamlit_callback_handler import (
    CHECKMARK_EMOJI,
    EXCEPTION_EMOJI,
    THINKING_EMOJI,
    LLMThought,
    LLMThoughtLabeler,
    LLMThoughtState,
    StreamlitCallbackHandler,
    ToolRecord,
)
from langchain_core.schema import AgentAction, AgentFinish, LLMResult
from streamlit.delta_generator import DeltaGenerator

from chemcrow.utils import is_smiles

from .utils import cdk


class LLMThoughtChem(LLMThought):
    def __init__(
        self,
        parent_container: DeltaGenerator,
        labeler: LLMThoughtLabeler,
        expanded: bool,
        collapse_on_complete: bool,
    ):
        super().__init__(
            parent_container,
            labeler,
            expanded,
            collapse_on_complete,
        )

    def on_tool_end(
        self,
        output: str,
        color: Optional[str] = None,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        output_ph: dict = {},
        input_tool: str = "",
        serialized: dict = {},
        **kwargs: Any,
    ) -> None:
        # Depending on the tool name, decide what to display.
        if serialized["name"] == "Name2SMILES":
            safe_smiles = output.replace("[", "\[").replace("]", "\]")
            if is_smiles(output):
                self._container.markdown(
                    f"**{safe_smiles}**{cdk(output)}", unsafe_allow_html=True
                )

        if serialized["name"] == "ReactionPredict":
            rxn = f"{input_tool}>>{output}"
            safe_smiles = rxn.replace("[", "\[").replace("]", "\]")
            self._container.markdown(
                f"**{safe_smiles}**{cdk(rxn)}", unsafe_allow_html=True
            )

        if serialized["name"] == "ReactionRetrosynthesis":
            output = output.replace("[", "\[").replace("]", "\]")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        # Called with the name of the tool we're about to run (in `serialized[name]`),
        # and its input. We change our container's label to be the tool name.
        self._state = LLMThoughtState.RUNNING_TOOL
        tool_name = serialized["name"]
        self._last_tool = ToolRecord(name=tool_name, input_str=input_str)
        self._container.update(
            new_label=(
                self._labeler.get_tool_label(self._last_tool, is_complete=False)
                .replace("[", "\[")
                .replace("]", "\]")
            )
        )

        # Display note of potential long time
        if serialized["name"] == "ReactionRetrosynthesis" or serialized["name"] == "LiteratureSearch":
            self._container.markdown(
                f"â€¼ï¸ Note: This tool can take some time to complete execution â€¼ï¸",
                unsafe_allow_html=True,
            )

    def complete(self, final_label: Optional[str] = None) -> None:
        """Finish the thought."""
        if final_label is None and self._state == LLMThoughtState.RUNNING_TOOL:
            assert (
                self._last_tool is not None
            ), "_last_tool should never be null when _state == RUNNING_TOOL"
            final_label = self._labeler.get_tool_label(
                self._last_tool, is_complete=True
            )
        self._state = LLMThoughtState.COMPLETE

        final_label = final_label.replace("[", "\[").replace("]", "\]")
        if self._collapse_on_complete:
            self._container.update(new_label=final_label, new_expanded=False)
        else:
            self._container.update(new_label=final_label)


class StreamlitCallbackHandlerChem(StreamlitCallbackHandler):
    def __init__(
        self,
        parent_container: DeltaGenerator,
        *,
        max_thought_containers: int = 4,
        expand_new_thoughts: bool = True,
        collapse_completed_thoughts: bool = True,
        thought_labeler: Optional[LLMThoughtLabeler] = None,
        output_placeholder: dict = {},
    ):
        super(StreamlitCallbackHandlerChem, self).__init__(
            parent_container,
            max_thought_containers=max_thought_containers,
            expand_new_thoughts=expand_new_thoughts,
            collapse_completed_thoughts=collapse_completed_thoughts,
            thought_labeler=thought_labeler,
        )

        self._output_placeholder = output_placeholder
        self.last_input = ""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        if self._current_thought is None:
            self._current_thought = LLMThoughtChem(
                parent_container=self._parent_container,
                expanded=self._expand_new_thoughts,
                collapse_on_complete=self._collapse_completed_thoughts,
                labeler=self._thought_labeler,
            )

        self._current_thought.on_llm_start(serialized, prompts)

        # We don't prune_old_thought_containers here, because our container won't
        # be visible until it has a child.

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        self._require_current_thought().on_tool_start(serialized, input_str, **kwargs)
        self._prune_old_thought_containers()
        self._last_input = input_str
        self._serialized = serialized

    def on_tool_end(
        self,
        output: str,
        color: Optional[str] = None,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        self._require_current_thought().on_tool_end(
            output,
            color,
            observation_prefix,
            llm_prefix,
            output_ph=self._output_placeholder,
            input_tool=self._last_input,
            serialized=self._serialized,
            **kwargs,
        )
        self._complete_current_thought()

    def on_agent_finish(
        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any
    ) -> None:
        if self._current_thought is not None:
            self._current_thought.complete(
                self._thought_labeler.get_final_agent_thought_label()
                .replace("[", "\[")
                .replace("]", "\]")
            )
            self._current_thought = None

```


================================================================================
=== FILE: chemcrow\frontend\utils.py ===
================================================================================

```python
import requests
from langchain import LLMChain, PromptTemplate
from langchain.chat_models import ChatOpenAI
from rdkit import Chem


def cdk(smiles):
    """
    Get a depiction of some smiles.
    """

    url = "https://www.simolecule.com/cdkdepict/depict/wob/svg"
    headers = {"Content-Type": "application/json"}
    response = requests.get(
        url,
        headers=headers,
        params={
            "smi": smiles,
            "annotate": "colmap",
            "zoom": 2,
            "w": 150,
            "h": 80,
            "abbr": "off",
        },
    )
    return response.text

```


================================================================================
=== FILE: chemcrow\frontend\__init__.py ===
================================================================================

```python
from .streamlit_callback_handler import StreamlitCallbackHandlerChem

```


================================================================================
=== FILE: chemcrow\tools\chemspace.py ===
================================================================================

```python
import os

import molbloom
import pandas as pd
import requests
from langchain.tools import BaseTool

from chemcrow.utils import is_smiles


class ChemSpace:
    def __init__(self, chemspace_api_key=None):
        self.chemspace_api_key = chemspace_api_key
        self._renew_token()  # Create token

    def _renew_token(self):
        self.chemspace_token = requests.get(
            url="https://api.chem-space.com/auth/token",
            headers={
                "Accept": "application/json",
                "Authorization": f"Bearer {self.chemspace_api_key}",
            },
        ).json()["access_token"]

    def _make_api_request(
        self,
        query,
        request_type,
        count,
        categories,
    ):
        """
        Make a generic request to chem-space API.

        Categories request.
            CSCS: Custom Request: Could be useful for requesting whole synthesis
            CSMB: Make-On-Demand Building Blocks
            CSSB: In-Stock Building Blocks
            CSSS: In-stock Screening Compounds
            CSMS: Make-On-Demand Screening Compounds
        """

        def _do_request():
            data = requests.request(
                "POST",
                url=f"https://api.chem-space.com/v3/search/{request_type}?count={count}&page=1&categories={categories}",
                headers={
                    "Accept": "application/json; version=3.1",
                    "Authorization": f"Bearer {self.chemspace_token}",
                },
                data={"SMILES": f"{query}"},
            ).json()
            return data

        data = _do_request()

        # renew token if token is invalid
        if "message" in data.keys():
            if data["message"] == "Your request was made with invalid credentials.":
                self._renew_token()

        data = _do_request()
        return data

    def _convert_single(self, query, search_type: str):
        """Do query for a single molecule"""
        data = self._make_api_request(query, "exact", 1, "CSCS,CSMB,CSSB")
        if data["count"] > 0:
            return data["items"][0][search_type]
        else:
            return "No data was found for this compound."

    def convert_mol_rep(self, query, search_type: str = "smiles"):
        if ", " in query:
            query_list = query.split(", ")
        else:
            query_list = [query]
        smi = ""
        try:
            for q in query_list:
                smi += f"{query}'s {search_type} is: {str(self._convert_single(q, search_type))}"
                return smi
        except Exception:
            return "The input provided is wrong. Input either a single molecule, or multiple molecules separated by a ', '"

    def buy_mol(
        self,
        smiles,
        request_type="exact",
        count=1,
    ):
        """
        Get data about purchasing compounds.

        smiles: smiles string of the molecule you want to buy
        request_type: one of "exact", "sim" (search by similarity), "sub" (search by substructure).
        count: retrieve data for this many substances max.
        """

        def purchasable_check(
            s,
        ):
            if not is_smiles(s):
                try:
                    s = self.convert_mol_rep(s, "smiles")
                except:
                    return "Invalid SMILES string."

            """Checks if molecule is available for purchase (ZINC20)"""
            try:
                r = molbloom.buy(s, canonicalize=True)
            except:
                print("invalid smiles")
                return False
            if r:
                return True
            else:
                return False

        purchasable = purchasable_check(smiles)

        if request_type == "exact":
            categories = "CSMB,CSSB"
        elif request_type in ["sim", "sub"]:
            categories = "CSSS,CSMS"

        data = self._make_api_request(smiles, request_type, count, categories)

        try:
            if data["count"] == 0:
                if purchasable:
                    return "Compound is purchasable, but price is unknown."
                else:
                    return "Compound is not purchasable."
        except KeyError:
            return "Invalid query, try something else. "

        print(f"Obtaining data for {data['count']} substances.")

        dfs = []
        # Convert this data into df
        for item in data["items"]:
            dfs_tmp = []
            smiles = item["smiles"]
            offers = item["offers"]

            for off in offers:
                df_tmp = pd.DataFrame(off["prices"])
                df_tmp["vendorName"] = off["vendorName"]
                df_tmp["time"] = off["shipsWithin"]
                df_tmp["purity"] = off["purity"]

                dfs_tmp.append(df_tmp)

            df_this = pd.concat(dfs_tmp)
            df_this["smiles"] = smiles
            dfs.append(df_this)

        df = pd.concat(dfs).reset_index(drop=True)

        df["quantity"] = df["pack"].astype(str) + df["uom"]
        df["time"] = df["time"].astype(str) + " days"

        df = df.drop(columns=["pack", "uom"])
        # Remove all entries that are not numbers
        df = df[df["priceUsd"].astype(str).str.isnumeric()]

        cheapest = df.iloc[df["priceUsd"].astype(float).idxmin()]
        return f"{cheapest['quantity']} of this molecule cost {cheapest['priceUsd']} USD and can be purchased at {cheapest['vendorName']}."


class GetMoleculePrice(BaseTool):
    name = "GetMoleculePrice"
    description = "Get the cheapest available price of a molecule."
    chemspace_api_key: str = None
    url: str = None

    def __init__(self, chemspace_api_key: str = None):
        super().__init__()
        self.chemspace_api_key = chemspace_api_key
        self.url = "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/{}"

    def _run(self, query: str) -> str:
        if not self.chemspace_api_key:
            return "No Chemspace API key found. This tool may not be used without a Chemspace API key."
        try:
            chemspace = ChemSpace(self.chemspace_api_key)
            price = chemspace.buy_mol(query)
            return price
        except Exception as e:
            return str(e)

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()

```


================================================================================
=== FILE: chemcrow\tools\converters.py ===
================================================================================

```python
from langchain.tools import BaseTool

from chemcrow.tools.chemspace import ChemSpace
from chemcrow.tools.safety import ControlChemCheck
from chemcrow.utils import (
    is_multiple_smiles,
    is_smiles,
    pubchem_query2smiles,
    query2cas,
    smiles2name,
)


class Query2CAS(BaseTool):
    name = "Mol2CAS"
    description = "Input molecule (name or SMILES), returns CAS number."
    url_cid: str = None
    url_data: str = None
    ControlChemCheck = ControlChemCheck()

    def __init__(
        self,
    ):
        super().__init__()
        self.url_cid = (
            "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/{}/{}/cids/JSON"
        )
        self.url_data = (
            "https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{}/JSON"
        )

    def _run(self, query: str) -> str:
        try:
            # if query is smiles
            smiles = None
            if is_smiles(query):
                smiles = query
            try:
                cas = query2cas(query, self.url_cid, self.url_data)
            except ValueError as e:
                return str(e)
            if smiles is None:
                try:
                    smiles = pubchem_query2smiles(cas, None)
                except ValueError as e:
                    return str(e)
            # check if mol is controlled
            msg = self.ControlChemCheck._run(smiles)
            if "high similarity" in msg or "appears" in msg:
                return f"CAS number {cas}found, but " + msg
            return cas
        except ValueError:
            return "CAS number not found"

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()


class Query2SMILES(BaseTool):
    name = "Name2SMILES"
    description = "Input a molecule name, returns SMILES."
    url: str = None
    chemspace_api_key: str = None
    ControlChemCheck = ControlChemCheck()

    def __init__(self, chemspace_api_key: str = None):
        super().__init__()
        self.chemspace_api_key = chemspace_api_key
        self.url = "https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/{}"

    def _run(self, query: str) -> str:
        """This function queries the given molecule name and returns a SMILES string from the record"""
        """Useful to get the SMILES string of one molecule by searching the name of a molecule. Only query with one specific name."""
        if is_smiles(query) and is_multiple_smiles(query):
            return "Multiple SMILES strings detected, input one molecule at a time."
        try:
            smi = pubchem_query2smiles(query, self.url)
        except Exception as e:
            if self.chemspace_api_key:
                try:
                    chemspace = ChemSpace(self.chemspace_api_key)
                    smi = chemspace.convert_mol_rep(query, "smiles")
                    smi = smi.split(":")[1]
                except Exception:
                    return str(e)
            else:
                return str(e)

        # check if mol is controlled
        msg = "Note: " + self.ControlChemCheck._run(smi)
        if "high similarity" in msg or "appears" in msg:
            return f"CAS number {smi}found, but " + msg
        return smi

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()


class SMILES2Name(BaseTool):
    name = "SMILES2Name"
    description = "Input SMILES, returns molecule name."
    ControlChemCheck = ControlChemCheck()
    query2smiles = Query2SMILES()

    def __init__(self):
        super().__init__()

    def _run(self, query: str) -> str:
        """Use the tool."""
        try:
            if not is_smiles(query):
                try:
                    query = self.query2smiles.run(query)
                except:
                    raise ValueError("Invalid molecule input, no Pubchem entry")
            name = smiles2name(query)
            # check if mol is controlled
            msg = "Note: " + self.ControlChemCheck._run(query)
            if "high similarity" in msg or "appears" in msg:
                return f"Molecule name {name} found, but " + msg
            return name
        except Exception as e:
            return "Error: " + str(e)

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()

```


================================================================================
=== FILE: chemcrow\tools\prompts.py ===
================================================================================

```python
safety_summary_prompt = (
    "Your task is to parse through the data provided and provide a summary of important health, laboratory, and environemntal safety information."
    'Focus on answering the following points, and follow the format "Name: description".'
    "Operator safety: Does this substance represent any danger to the person handling it? What are the risks? What precautions should be taken when handling this substance?"
    "GHS information: What are the GHS signal (hazard level: dangerous, warning, etc.) and GHS classification? What do these GHS classifications mean when dealing with this substance?"
    "Environmental risks: What are the environmental impacts of handling this substance."
    "Societal impact: What are the societal concerns of this substance? For instance, is it a known chemical weapon, is it illegal, or is it a controlled substance for any reason?"
    "For each point, use maximum two sentences. Use only the information provided in the paragraph below."
    "If there is not enough information in a category, you may fill in with your knowledge, but explicitly state so."
    "Here is the information:{data}"
)

summary_each_data = (
    "Please summarize the following, highlighting important information for health, laboratory and environemntal safety."
    "Do not exceed {approx_length} characters. The data is: {data}"
)

```


================================================================================
=== FILE: chemcrow\tools\rdkit.py ===
================================================================================

```python
from langchain.tools import BaseTool
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors

from chemcrow.utils import *


class MolSimilarity(BaseTool):
    name = "MolSimilarity"
    description = (
        "Input two molecule SMILES (separated by '.'), returns Tanimoto similarity."
    )

    def __init__(self):
        super().__init__()

    def _run(self, smiles_pair: str) -> str:
        smi_list = smiles_pair.split(".")
        if len(smi_list) != 2:
            return "Input error, please input two smiles strings separated by '.'"
        else:
            smiles1, smiles2 = smi_list

        similarity = tanimoto(smiles1, smiles2)

        if isinstance(similarity, str):
            return similarity

        sim_score = {
            0.9: "very similar",
            0.8: "similar",
            0.7: "somewhat similar",
            0.6: "not very similar",
            0: "not similar",
        }
        if similarity == 1:
            return "Error: Input Molecules Are Identical"
        else:
            val = sim_score[
                max(key for key in sim_score.keys() if key <= round(similarity, 1))
            ]
            message = f"The Tanimoto similarity between {smiles1} and {smiles2} is {round(similarity, 4)},\
            indicating that the two molecules are {val}."
        return message

    async def _arun(self, smiles_pair: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()


class SMILES2Weight(BaseTool):
    name = "SMILES2Weight"
    description = "Input SMILES, returns molecular weight."

    def __init__(
        self,
    ):
        super().__init__()

    def _run(self, smiles: str) -> str:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return "Invalid SMILES string"
        mol_weight = rdMolDescriptors.CalcExactMolWt(mol)
        return mol_weight

    async def _arun(self, smiles: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()


class FuncGroups(BaseTool):
    name = "FunctionalGroups"
    description = "Input SMILES, return list of functional groups in the molecule."
    dict_fgs: dict = None

    def __init__(
        self,
    ):
        super().__init__()

        # List obtained from https://github.com/rdkit/rdkit/blob/master/Data/FunctionalGroups.txt
        self.dict_fgs = {
            "furan": "o1cccc1",
            "aldehydes": " [CX3H1](=O)[#6]",
            "esters": " [#6][CX3](=O)[OX2H0][#6]",
            "ketones": " [#6][CX3](=O)[#6]",
            "amides": " C(=O)-N",
            "thiol groups": " [SH]",
            "alcohol groups": " [OH]",
            "methylamide": "*-[N;D2]-[C;D3](=O)-[C;D1;H3]",
            "carboxylic acids": "*-C(=O)[O;D1]",
            "carbonyl methylester": "*-C(=O)[O;D2]-[C;D1;H3]",
            "terminal aldehyde": "*-C(=O)-[C;D1]",
            "amide": "*-C(=O)-[N;D1]",
            "carbonyl methyl": "*-C(=O)-[C;D1;H3]",
            "isocyanate": "*-[N;D2]=[C;D2]=[O;D1]",
            "isothiocyanate": "*-[N;D2]=[C;D2]=[S;D1]",
            "nitro": "*-[N;D3](=[O;D1])[O;D1]",
            "nitroso": "*-[N;R0]=[O;D1]",
            "oximes": "*=[N;R0]-[O;D1]",
            "Imines": "*-[N;R0]=[C;D1;H2]",
            "terminal azo": "*-[N;D2]=[N;D2]-[C;D1;H3]",
            "hydrazines": "*-[N;D2]=[N;D1]",
            "diazo": "*-[N;D2]#[N;D1]",
            "cyano": "*-[C;D2]#[N;D1]",
            "primary sulfonamide": "*-[S;D4](=[O;D1])(=[O;D1])-[N;D1]",
            "methyl sulfonamide": "*-[N;D2]-[S;D4](=[O;D1])(=[O;D1])-[C;D1;H3]",
            "sulfonic acid": "*-[S;D4](=O)(=O)-[O;D1]",
            "methyl ester sulfonyl": "*-[S;D4](=O)(=O)-[O;D2]-[C;D1;H3]",
            "methyl sulfonyl": "*-[S;D4](=O)(=O)-[C;D1;H3]",
            "sulfonyl chloride": "*-[S;D4](=O)(=O)-[Cl]",
            "methyl sulfinyl": "*-[S;D3](=O)-[C;D1]",
            "methyl thio": "*-[S;D2]-[C;D1;H3]",
            "thiols": "*-[S;D1]",
            "thio carbonyls": "*=[S;D1]",
            "halogens": "*-[#9,#17,#35,#53]",
            "t-butyl": "*-[C;D4]([C;D1])([C;D1])-[C;D1]",
            "tri fluoromethyl": "*-[C;D4](F)(F)F",
            "acetylenes": "*-[C;D2]#[C;D1;H]",
            "cyclopropyl": "*-[C;D3]1-[C;D2]-[C;D2]1",
            "ethoxy": "*-[O;D2]-[C;D2]-[C;D1;H3]",
            "methoxy": "*-[O;D2]-[C;D1;H3]",
            "side-chain hydroxyls": "*-[O;D1]",
            "ketones": "*=[O;D1]",
            "primary amines": "*-[N;D1]",
            "nitriles": "*#[N;D1]",
        }

    def _is_fg_in_mol(self, mol, fg):
        fgmol = Chem.MolFromSmarts(fg)
        mol = Chem.MolFromSmiles(mol.strip())
        return len(Chem.Mol.GetSubstructMatches(mol, fgmol, uniquify=True)) > 0

    def _run(self, smiles: str) -> str:
        """
        Input a molecule SMILES or name.
        Returns a list of functional groups identified by their common name (in natural language).
        """
        try:
            fgs_in_molec = [
                name
                for name, fg in self.dict_fgs.items()
                if self._is_fg_in_mol(smiles, fg)
            ]
            if len(fgs_in_molec) > 1:
                return f"This molecule contains {', '.join(fgs_in_molec[:-1])}, and {fgs_in_molec[-1]}."
            else:
                return f"This molecule contains {fgs_in_molec[0]}."
        except:
            return "Wrong argument. Please input a valid molecular SMILES."

    async def _arun(self, smiles: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()

```


================================================================================
=== FILE: chemcrow\tools\reactions.py ===
================================================================================

```python
"""Self-hosted reaction tools. Retrosynthesis, reaction forward prediction."""

import abc
import ast
import re
from time import sleep
from typing import Optional

import requests

import json
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.tools import BaseTool

from chemcrow.utils import is_smiles

__all__ = ["RXNPredictLocal", "RXNRetrosynthesisLocal"]


class RXNPredictLocal(BaseTool):
    """Predict reaction."""

    name = "ReactionPredict"
    description = (
        "Predict the outcome of a chemical reaction. "
        "Takes as input the SMILES of the reactants separated by a dot '.', "
        "returns SMILES of the products."
    )

    def _run(self, reactants: str) -> str:
        """Run reaction prediction."""
        if not is_smiles(reactants):
            return "Incorrect input."

        product = self.predict_reaction(reactants)
        return product

    def predict_reaction(self, reactants: str) -> str:
        """Make api request."""
        try:
            response = requests.post(
                "http://localhost:8051/api/v1/run",
                headers={"Content-Type": "application/json"},
                data=json.dumps({"smiles": reactants})
            )
            return response.json()['product'][0]
        except:
            return "Error in prediction."


class RXNRetrosynthesisLocal(BaseTool):
    """Predict retrosynthesis."""

    name = "ReactionRetrosynthesis"
    description = (
        "Obtain the synthetic route to a chemical compound. "
        "Takes as input the SMILES of the product, returns recipe."
    )
    openai_api_key: str = ""

    def _run(self, reactants: str) -> str:
        """Run reaction prediction."""
        # Check that input is smiles
        if not is_smiles(reactants):
            return "Incorrect input."

        paths = self.retrosynthesis(reactants)
        procedure = self.get_action_sequence(paths[0])
        return procedure

    def retrosynthesis(self, reactants: str) -> str:
        """Make api request."""
        response = requests.post(
            "http://localhost:8052/api/v1/run",
            headers={"Content-Type": "application/json"},
            data=json.dumps({"smiles": reactants})
        )
        return response.json()

    def get_action_sequence(self, path):
        """Get sequence of actions."""
        actions = path
        json_actions = self._preproc_actions(actions)
        llm_sum = self._summary_gpt(json_actions)
        return llm_sum

    def _preproc_actions(self, path):
        """Preprocess actions."""
        def _clean_actions(d):
            if 'metadata' in d:
                if 'mapped_reaction_smiles' in d['metadata']:
                    r = d['metadata']['mapped_reaction_smiles'].split(">>")
                    yield {"reactants": r[1], "products": r[0]}
            if 'children' in d:
                for c in d['children']:
                    yield from _clean_actions(c)

        rxns = list(_clean_actions(path))
        return rxns

    def _summary_gpt(self, json: dict) -> str:
        """Describe synthesis."""
        llm = ChatOpenAI(  # type: ignore
            temperature=0.05,
            model_name="gpt-3.5-turbo-16k",
            request_timeout=2000,
            max_tokens=2000,
            openai_api_key=self.openai_api_key,
        )
        prompt = (
            "Here is a chemical synthesis described as a json.\nYour task is "
            "to describe the synthesis, as if you were giving instructions for"
            "a recipe. Use only the substances, quantities, temperatures and "
            "in general any action mentioned in the json file. This is your "
            "only source of information, do not make up anything else. Also, "
            "add 15mL of DCM as a solvent in the first step. If you ever need "
            'to refer to the json file, refer to it as "(by) the tool". '
            "However avoid references to it. \nFor this task, give as many "
            f"details as possible.\n {str(json)}"
        )
        return llm([HumanMessage(content=prompt)]).content

```


================================================================================
=== FILE: chemcrow\tools\rxn4chem.py ===
================================================================================

```python
"""Wrapper for RXN4Chem functionalities."""

import abc
import ast
import re
from time import sleep
from typing import Optional

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.tools import BaseTool
from rxn4chemistry import RXN4ChemistryWrapper  # type: ignore

from chemcrow.utils import is_smiles

__all__ = ["RXNPredict", "RXNRetrosynthesis"]


class RXN4Chem(BaseTool):
    """Wrapper for RXN4Chem functionalities."""

    name: str
    description: str
    rxn4chem_api_key: Optional[str] = None
    rxn4chem: RXN4ChemistryWrapper = None
    base_url: str = "https://rxn.res.ibm.com"
    sleep_time: int = 5

    def __init__(self, rxn4chem_api_key):
        """Init object."""
        super().__init__()

        self.rxn4chem_api_key = rxn4chem_api_key
        self.rxn4chem = RXN4ChemistryWrapper(
            api_key=self.rxn4chem_api_key, base_url=self.base_url
        )
        self.rxn4chem.project_id = "655b7b760fb57c001f25dc91"

    @abc.abstractmethod
    def _run(self, smiles: str):  # type: ignore
        """Execute operation."""
        pass

    @abc.abstractmethod
    async def _arun(self, smiles: str):
        """Async execute operation."""
        pass

    @staticmethod
    def retry(times: int, exceptions, sleep_time: int = 5):
        """
        Retry Decorator.

        Retries the wrapped function/method `times` times if the exceptions
        listed in ``exceptions`` are thrown
        :param times: The number of times to repeat the wrapped function/method
        :type times: Int
        :param Exceptions: Lists of exceptions that trigger a retry attempt
        :type Exceptions: Tuple of Exceptions
        """

        def decorator(func):
            def newfn(*args, **kwargs):
                attempt = 0
                while attempt < times:
                    try:
                        sleep(sleep_time)
                        return func(*args, **kwargs)
                    except exceptions:
                        print(
                            "Exception thrown when attempting to run %s, "
                            "attempt %d of %d" % (func, attempt, times)
                        )
                        attempt += 1
                return func(*args, **kwargs)

            return newfn

        return decorator


class RXNPredict(RXN4Chem):
    """Predict reaction."""

    name = "ReactionPredict"
    description = (
        "Predict the outcome of a chemical reaction. "
        "Takes as input the SMILES of the reactants separated by a dot '.', "
        "returns SMILES of the products."
    )

    def _run(self, reactants: str) -> str:
        """Run reaction prediction."""
        # Check that input is smiles
        if not is_smiles(reactants):
            return "Incorrect input."

        prediction_id = self.predict_reaction(reactants)
        results = self.get_results(prediction_id)
        product = results["productMolecule"]["smiles"]
        return product

    # @RXN4Chem.retry(10, KeyError)
    def predict_reaction(self, reactants: str) -> str:
        """Make api request."""
        response = self.rxn4chem.predict_reaction(reactants)
        if "prediction_id" in response.keys():
            return response["prediction_id"]
        else:
            raise KeyError

    # @RXN4Chem.retry(10, KeyError)
    def get_results(self, prediction_id: str) -> str:
        """Make api request."""
        results = self.rxn4chem.get_predict_reaction_results(prediction_id)
        if "payload" in results["response"].keys():
            return results["response"]["payload"]["attempts"][0]
        else:
            raise KeyError

    async def _arun(self, cas_number):
        """Async run reaction prediction."""
        raise NotImplementedError("Async not implemented.")


class RXNRetrosynthesis(RXN4Chem):
    """Predict retrosynthesis."""

    name = "ReactionRetrosynthesis"
    description = (
        "Obtain the synthetic route to a chemical compound. "
        "Takes as input the SMILES of the product, returns recipe."
    )
    openai_api_key: str = ""

    def __init__(self, rxn4chem_api_key, openai_api_key):
        """Init object."""
        super().__init__(rxn4chem_api_key)
        self.openai_api_key = openai_api_key

    def _run(self, target: str) -> str:
        """Run retrosynthesis prediction."""
        # Check that input is smiles
        if not is_smiles(target):
            return "Incorrect input."

        prediction_id = self.predict_retrosynthesis(target)
        paths = self.get_paths(prediction_id)
        # path_img = self.visualize_path(paths[0])
        procedure = self.get_action_sequence(paths[0])
        return procedure

    async def _arun(self, cas_number):
        """Async run retrosynthesis prediction."""
        raise NotImplementedError("Async not implemented.")

    @RXN4Chem.retry(10, KeyError)
    def predict_retrosynthesis(self, target: str) -> str:
        """Make api request."""
        response = self.rxn4chem.predict_automatic_retrosynthesis(
            product=target,
            fap=0.6,
            max_steps=3,
            nbeams=10,
            pruning_steps=2,
            ai_model="12class-tokens-2021-05-14",
        )
        if "prediction_id" in response.keys():
            return response["prediction_id"]
        raise KeyError

    @RXN4Chem.retry(20, KeyError)
    def get_paths(self, prediction_id: str) -> str:
        """Make api request."""
        results = self.rxn4chem.get_predict_automatic_retrosynthesis_results(
            prediction_id
        )
        if "retrosynthetic_paths" not in results.keys():
            raise KeyError
        paths = results["retrosynthetic_paths"]
        if paths is not None:
            if len(paths) > 0:
                return paths
        if results["status"] == "PROCESSING":
            sleep(self.sleep_time * 2)
            raise KeyError
        raise KeyError

    def get_action_sequence(self, path):
        """Get sequence of actions."""
        response = self.synth_from_sequence(path["sequenceId"])
        if "synthesis_id" not in response.keys():
            return path

        synthesis_id = response["synthesis_id"]
        nodeids = self.get_node_ids(synthesis_id)
        if nodeids is None:
            return "Tool error"

        # Attempt to get actions for each node + product information
        real_nodes = []
        actions_and_products = []
        for node in nodeids:
            node_resp = self.get_reaction_settings(
                synthesis_id=synthesis_id, node_id=node
            )
            if "actions" in node_resp.keys():
                real_nodes.append(node)
                actions_and_products.append(node_resp)

        json_actions = self._preproc_actions(actions_and_products)
        llm_sum = self._summary_gpt(json_actions)
        return llm_sum

    @RXN4Chem.retry(20, KeyError)
    def synth_from_sequence(self, sequence_id: str) -> str:
        """Make api request."""
        response = self.rxn4chem.create_synthesis_from_sequence(sequence_id=sequence_id)
        if "synthesis_id" in response.keys():
            return response
        raise KeyError

    @RXN4Chem.retry(20, KeyError)
    def get_node_ids(self, synthesis_id: str):
        """Make api request."""
        response = self.rxn4chem.get_node_ids(synthesis_id=synthesis_id)
        if isinstance(response, list):
            if len(response) > 0:
                return response
        return KeyError

    @RXN4Chem.retry(20, KeyError)
    def get_reaction_settings(self, synthesis_id: str, node_id: str):
        """Make api request."""
        response = self.rxn4chem.get_reaction_settings(
            synthesis_id=synthesis_id, node_id=node_id
        )
        if "actions" in response.keys():
            return response
        elif "response" in response.keys():
            if "error" in response["response"].keys():
                if response["response"]["error"] == "Too Many Requests":
                    sleep(self.sleep_time * 2)
                    raise KeyError
            return response
        raise KeyError

    def _preproc_actions(self, actions_and_products):
        """Preprocess actions."""
        json_actions = {"number_of_steps": len(actions_and_products)}

        for i, actn in enumerate(actions_and_products):
            json_actions[f"Step_{i}"] = {}
            json_actions[f"Step_{i}"]["actions"] = actn["actions"]
            json_actions[f"Step_{i}"]["product"] = actn["product"]

        # Clean actions to use less tokens: Remove False, None, ''
        clean_act_str = re.sub(
            r"\'[A-Za-z]+\': (None|False|\'\'),? ?", "", str(json_actions)
        )
        json_actions = ast.literal_eval(clean_act_str)

        return json_actions

    def _summary_gpt(self, json: dict) -> str:
        """Describe synthesis."""
        llm = ChatOpenAI(  # type: ignore
            temperature=0.05,
            model_name="gpt-3.5-turbo-16k",
            request_timeout=2000,
            max_tokens=2000,
            openai_api_key=self.openai_api_key,
        )
        prompt = (
            "Here is a chemical synthesis described as a json.\nYour task is "
            "to describe the synthesis, as if you were giving instructions for"
            "a recipe. Use only the substances, quantities, temperatures and "
            "in general any action mentioned in the json file. This is your "
            "only source of information, do not make up anything else. Also, "
            "add 15mL of DCM as a solvent in the first step. If you ever need "
            'to refer to the json file, refer to it as "(by) the tool". '
            "However avoid references to it. \nFor this task, give as many "
            f"details as possible.\n {str(json)}"
        )
        return llm([HumanMessage(content=prompt)]).content

    def visualize_path(self, path):
        """Visualize path."""
        from aizynthfinder import reactiontree  # type: ignore

        rxn_dict = self._path_to_dict(path)
        tree = reactiontree.ReactionTree.from_dict(rxn_dict)
        return tree.to_image()

    def _path_to_dict(self, path):
        """Convert path to dict."""
        if len(path["children"]) != 0:
            in_stock = False
            rxn_smi = path["smiles"] + ">>"
            for prec in path["children"]:
                rxn_smi += prec["smiles"] + "."
            rxn_smi = rxn_smi[:-1]

            children = [
                {
                    "type": "reaction",
                    "hide": False,
                    "smiles": rxn_smi,
                    "is_reaction": True,
                    "metadata": {},
                    "children": [self._path_to_dict(c) for c in path["children"]],
                }
            ]
        else:
            in_stock = True
            children = []

        return {
            "type": "mol",
            "route_metadata": {"created_at_iteration": 1, "is_solved": True},
            "hide": False,
            "smiles": path["smiles"],
            "is_chemical": True,
            "in_stock": in_stock,
            "children": children,
        }

```


================================================================================
=== FILE: chemcrow\tools\safety.py ===
================================================================================

```python
import re
import urllib
from time import sleep

import langchain
import molbloom
import pandas as pd
import pkg_resources
import requests
import tiktoken
from langchain import LLMChain, PromptTemplate
from langchain.llms import BaseLLM
from langchain.tools import BaseTool

from chemcrow.utils import is_smiles, pubchem_query2smiles, tanimoto

from .prompts import safety_summary_prompt, summary_each_data


class MoleculeSafety:
    def __init__(self, llm: BaseLLM = None):
        while True:
            try:
                self.clintox = pd.read_csv(
                    "https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/clintox.csv.gz"
                )
                break
            except (ConnectionRefusedError, urllib.error.URLError):
                sleep(5)
                continue
        self.pubchem_data = {}
        self.llm = llm

    def _fetch_pubchem_data(self, cas_number):
        """Fetch data from PubChem for a given CAS number, or use cached data if it's already been fetched."""
        if cas_number not in self.pubchem_data:
            try:
                url1 = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{cas_number}/cids/JSON"
                url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{requests.get(url1).json()['IdentifierList']['CID'][0]}/JSON"
                r = requests.get(url)
                self.pubchem_data[cas_number] = r.json()
            except:
                return "Invalid molecule input, no Pubchem entry."
        return self.pubchem_data[cas_number]

    def ghs_classification(self, text):
        """Gives the ghs classification from Pubchem. Give this tool the name or CAS number of one molecule."""
        if is_smiles(text):
            return "Please input a valid CAS number."
        data = self._fetch_pubchem_data(text)
        if isinstance(data, str):
            return "Molecule not found in Pubchem."
        try:
            for section in data["Record"]["Section"]:
                if section.get("TOCHeading") == "Chemical Safety":
                    ghs = [
                        markup["Extra"]
                        for markup in section["Information"][0]["Value"][
                            "StringWithMarkup"
                        ][0]["Markup"]
                    ]
                    if ghs:
                        return ghs
        except (StopIteration, KeyError):
            return None

    @staticmethod
    def _scrape_pubchem(data, heading1, heading2, heading3):
        try:
            filtered_sections = []
            for section in data["Record"]["Section"]:
                toc_heading = section.get("TOCHeading")
                if toc_heading == heading1:
                    for section2 in section["Section"]:
                        if section2.get("TOCHeading") == heading2:
                            for section3 in section2["Section"]:
                                if section3.get("TOCHeading") == heading3:
                                    filtered_sections.append(section3)
            return filtered_sections
        except:
            return None

    def _get_safety_data(self, cas):
        data = self._fetch_pubchem_data(cas)
        safety_data = []

        iterations = [
            (
                [
                    "Health Hazards",
                    "GHS Classification",
                    "Hazards Summary",
                    "NFPA Hazard Classification",
                ],
                "Safety and Hazards",
                "Hazards Identification",
            ),
            (
                ["Explosive Limits and Potential", "Preventive Measures"],
                "Safety and Hazards",
                "Safety and Hazard Properties",
            ),
            (
                [
                    "Inhalation Risk",
                    "Effects of Long Term Exposure",
                    "Personal Protective Equipment (PPE)",
                ],
                "Safety and Hazards",
                "Exposure Control and Personal Protection",
            ),
            (
                ["Toxicity Summary", "Carcinogen Classification"],
                "Toxicity",
                "Toxicological Information",
            ),
        ]

        for items, header1, header2 in iterations:
            safety_data.extend(
                [self._scrape_pubchem(data, header1, header2, item)] for item in items
            )

        return safety_data

    @staticmethod
    def _num_tokens(string, encoding_name="text-davinci-003"):
        """Returns the number of tokens in a text string."""
        encoding = tiktoken.encoding_for_model(encoding_name)
        num_tokens = len(encoding.encode(string))
        return num_tokens

    def get_safety_summary(self, cas):
        safety_data = self._get_safety_data(cas)
        approx_length = int(
            (3500 * 4) / len(safety_data) - 0.1 * ((3500 * 4) / len(safety_data))
        )
        prompt_short = PromptTemplate(
            template=summary_each_data, input_variables=["data", "approx_length"]
        )
        llm_chain_short = LLMChain(prompt=prompt_short, llm=self.llm)

        llm_output = []
        for info in safety_data:
            if self._num_tokens(str(info)) > approx_length:
                trunc_info = str(info)[:approx_length]
                llm_output.append(
                    llm_chain_short.run(
                        {"data": str(trunc_info), "approx_length": approx_length}
                    )
                )
            else:
                llm_output.append(
                    llm_chain_short.run(
                        {"data": str(info), "approx_length": approx_length}
                    )
                )
        return llm_output


class SafetySummary(BaseTool):
    name = "SafetySummary"
    description = (
        "Input CAS number, returns a summary of safety information."
        "The summary includes Operator safety, GHS information, "
        "Environmental risks, and Societal impact."
    )
    llm: BaseLLM = None
    llm_chain: LLMChain = None
    pubchem_data: dict = dict()
    mol_safety: MoleculeSafety = None

    def __init__(self, llm):
        super().__init__()
        self.mol_safety = MoleculeSafety(llm=llm)
        self.llm = llm
        prompt = PromptTemplate(
            template=safety_summary_prompt, input_variables=["data"]
        )
        self.llm_chain = LLMChain(prompt=prompt, llm=self.llm)

    def _run(self, cas: str) -> str:
        if is_smiles(cas):
            return "Please input a valid CAS number."
        data = self.mol_safety._fetch_pubchem_data(cas)
        if isinstance(data, str):
            return "Molecule not found in Pubchem."

        data = self.mol_safety.get_safety_summary(cas)
        return self.llm_chain.run(" ".join(data))

    async def _arun(self, cas_number):
        raise NotImplementedError("Async not implemented.")


class ExplosiveCheck(BaseTool):
    name = "ExplosiveCheck"
    description = "Input CAS number, returns if molecule is explosive."
    mol_safety: MoleculeSafety = None

    def __init__(self):
        super().__init__()
        self.mol_safety = MoleculeSafety()

    def _run(self, cas_number):
        """Checks if a molecule has an explosive GHS classification using pubchem."""
        # first check if the input is a CAS number
        if is_smiles(cas_number):
            return "Please input a valid CAS number."
        cls = self.mol_safety.ghs_classification(cas_number)
        if cls is None:
            return (
                "Explosive Check Error. The molecule may not be assigned a GHS rating. "
            )
        if "Explos" in str(cls) or "explos" in str(cls):
            return "Molecule is explosive"
        else:
            return "Molecule is not known to be explosive"

    async def _arun(self, cas_number):
        raise NotImplementedError("Async not implemented.")


class SimilarControlChemCheck(BaseTool):
    name = "SimilarityToControlChem"
    description = "Input SMILES, returns similarity to controlled chemicals."

    def _run(self, smiles: str) -> str:
        """Checks max similarity between compound and controlled chemicals.
        Input SMILES string."""

        data_path = pkg_resources.resource_filename("chemcrow", "data/chem_wep_smi.csv")
        cw_df = pd.read_csv(data_path)

        try:
            if not is_smiles(smiles):
                return "Please input a valid SMILES string."

            max_sim = cw_df["smiles"].apply(lambda x: self.tanimoto(smiles, x)).max()
            if max_sim > 0.35:
                return (
                    f"{smiles} has a high similarity "
                    f"({max_sim:.4}) to a known controlled chemical."
                )
            else:
                return (
                    f"{smiles} has a low similarity "
                    f"({max_sim:.4}) to a known controlled chemical. "
                    "This is substance is safe, you may proceed with the task."
                )
        except:
            return "Tool error."

    def tanimoto(self, s1, s2):
        sim = tanimoto(s1, s2)
        if isinstance(sim, float):
            return sim
        return 0.0

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()


class ControlChemCheck(BaseTool):
    name = "ControlChemCheck"
    description = "Input CAS number, True if molecule is a controlled chemical."
    similar_control_chem_check = SimilarControlChemCheck()

    def _run(self, query: str) -> str:
        """Checks if compound is a controlled chemical. Input CAS number."""
        data_path = pkg_resources.resource_filename("chemcrow", "data/chem_wep_smi.csv")
        cw_df = pd.read_csv(data_path)
        try:
            if is_smiles(query):
                query_esc = re.escape(query)
                found = (
                    cw_df["smiles"]
                    .astype(str)
                    .str.contains(f"^{query_esc}$", regex=True)
                    .any()
                )
            else:
                found = (
                    cw_df["cas"]
                    .astype(str)
                    .str.contains(f"^\({query}\)$", regex=True)
                    .any()
                )
            if found:
                return (
                    f"The molecule {query} appears in a list of "
                    "controlled chemicals."
                )
            else:
                # Get smiles of CAS number
                try:
                    smi = pubchem_query2smiles(query)
                except ValueError as e:
                    return str(e)
                # Check similarity to known controlled chemicals
                return self.similar_control_chem_check._run(smi)

        except Exception as e:
            return f"Error: {e}"

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()

```


================================================================================
=== FILE: chemcrow\tools\search.py ===
================================================================================

```python
import os
import re

import langchain
import molbloom
import paperqa
import paperscraper
from langchain import SerpAPIWrapper
from langchain.base_language import BaseLanguageModel
from langchain.tools import BaseTool
from langchain.embeddings.openai import OpenAIEmbeddings
from pypdf.errors import PdfReadError

from chemcrow.utils import is_multiple_smiles, split_smiles


def paper_scraper(search: str, pdir: str = "query", semantic_scholar_api_key: str = None) -> dict:
    try:
        return paperscraper.search_papers(
            search,
            pdir=pdir,
            semantic_scholar_api_key=semantic_scholar_api_key,
        )
    except KeyError:
        return {}


def paper_search(llm, query, semantic_scholar_api_key=None):
    prompt = langchain.prompts.PromptTemplate(
        input_variables=["question"],
        template="""
        I would like to find scholarly papers to answer
        this question: {question}. Your response must be at
        most 10 words long.
        'A search query that would bring up papers that can answer
        this question would be: '""",
    )

    query_chain = langchain.chains.llm.LLMChain(llm=llm, prompt=prompt)
    if not os.path.isdir("./query"):  # todo: move to ckpt
        os.mkdir("query/")
    search = query_chain.run(query)
    print("\nSearch:", search)
    papers = paper_scraper(search, pdir=f"query/{re.sub(' ', '', search)}", semantic_scholar_api_key=semantic_scholar_api_key)
    return papers


def scholar2result_llm(llm, query, k=5, max_sources=2, openai_api_key=None, semantic_scholar_api_key=None):
    """Useful to answer questions that require
    technical knowledge. Ask a specific question."""
    papers = paper_search(llm, query, semantic_scholar_api_key=semantic_scholar_api_key)
    if len(papers) == 0:
        return "Not enough papers found"
    docs = paperqa.Docs(
        llm=llm,
        summary_llm=llm,
        embeddings=OpenAIEmbeddings(openai_api_key=openai_api_key),
    )
    not_loaded = 0
    for path, data in papers.items():
        try:
            docs.add(path, data["citation"])
        except (ValueError, FileNotFoundError, PdfReadError):
            not_loaded += 1

    if not_loaded > 0:
        print(f"\nFound {len(papers.items())} papers but couldn't load {not_loaded}.")
    else:
        print(f"\nFound {len(papers.items())} papers and loaded all of them.")

    answer = docs.query(query, k=k, max_sources=max_sources).formatted_answer
    return answer


class Scholar2ResultLLM(BaseTool):
    name = "LiteratureSearch"
    description = (
        "Useful to answer questions that require technical "
        "knowledge. Ask a specific question."
    )
    llm: BaseLanguageModel = None
    openai_api_key: str = None 
    semantic_scholar_api_key: str = None


    def __init__(self, llm, openai_api_key, semantic_scholar_api_key):
        super().__init__()
        self.llm = llm
        # api keys
        self.openai_api_key = openai_api_key
        self.semantic_scholar_api_key = semantic_scholar_api_key

    def _run(self, query) -> str:
        return scholar2result_llm(
            self.llm,
            query,
            openai_api_key=self.openai_api_key,
            semantic_scholar_api_key=self.semantic_scholar_api_key
        )

    async def _arun(self, query) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("this tool does not support async")


def web_search(keywords, search_engine="google"):
    try:
        return SerpAPIWrapper(
            serpapi_api_key=os.getenv("SERP_API_KEY"), search_engine=search_engine
        ).run(keywords)
    except:
        return "No results, try another search"


class WebSearch(BaseTool):
    name = "WebSearch"
    description = (
        "Input a specific question, returns an answer from web search. "
        "Do not mention any specific molecule names, but use more general features to formulate your questions."
    )
    serp_api_key: str = None

    def __init__(self, serp_api_key: str = None):
        super().__init__()
        self.serp_api_key = serp_api_key

    def _run(self, query: str) -> str:
        if not self.serp_api_key:
            return (
                "No SerpAPI key found. This tool may not be used without a SerpAPI key."
            )
        return web_search(query)

    async def _arun(self, query: str) -> str:
        raise NotImplementedError("Async not implemented")


class PatentCheck(BaseTool):
    name = "PatentCheck"
    description = "Input SMILES, returns if molecule is patented. You may also input several SMILES, separated by a period."

    def _run(self, smiles: str) -> str:
        """Checks if compound is patented. Give this tool only one SMILES string"""
        if is_multiple_smiles(smiles):
            smiles_list = split_smiles(smiles)
        else:
            smiles_list = [smiles]
        try:
            output_dict = {}
            for smi in smiles_list:
                r = molbloom.buy(smi, canonicalize=True, catalog="surechembl")
                if r:
                    output_dict[smi] = "Patented"
                else:
                    output_dict[smi] = "Novel"
            return str(output_dict)
        except:
            return "Invalid SMILES string"

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError()

```


================================================================================
=== FILE: chemcrow\tools\__init__.py ===
================================================================================

```python
"""load all tools.""" 

from .rdkit import *      # noqa
from .search import *     # noqa
from .rxn4chem import *   # noqa
from .safety import *     # noqa
from .chemspace import *  # noqa
from .converters import * # noqa
from .reactions import *  # noqa

# ---- custom tools in tools/New ----
from .New.Arxiv2ResultLLM import *   # noqa
from .New.motif_tools import *       # noqa
from .New.VastraVisualise import *   # noqa

```


================================================================================
=== FILE: chemcrow\tools\New\Arxiv2ResultLLM.py ===
================================================================================

```python
import os
import re
import time
import requests
import xml.etree.ElementTree as ET

import langchain
import paperqa
from langchain.base_language import BaseLanguageModel
from langchain.tools import BaseTool
from langchain.embeddings.openai import OpenAIEmbeddings
from pypdf.errors import PdfReadError

# ------------ arXiv API config ------------ #

# Use HTTPS as best practice
ARXIV_API_URL = "https://export.arxiv.org/api/query"

# Shared session with polite User-Agent
SESSION = requests.Session()
SESSION.headers.update(
    {
        "User-Agent": (
            "ChemCrow-ArxivTool/0.1 "
            "(https://example.org; mailto:you@example.org)"
        )
    }
)

# Simple global rate limit for API calls (arXiv ToU: <= 1 request / 3s)
_LAST_API_CALL = 0.0
_API_MIN_INTERVAL = 3.0  # seconds


# ------------ ArXiv helper functions (raw API) ------------ #

def _throttled_arxiv_get(params: dict) -> requests.Response:
    """Call the arXiv export API with a simple 3s throttle."""
    global _LAST_API_CALL
    now = time.time()
    elapsed = now - _LAST_API_CALL
    if elapsed < _API_MIN_INTERVAL:
        time.sleep(_API_MIN_INTERVAL - elapsed)
    _LAST_API_CALL = time.time()

    resp = SESSION.get(ARXIV_API_URL, params=params, timeout=30)
    resp.raise_for_status()
    return resp


def _arxiv_api_query(
    search: str,
    max_results: int = 20,
) -> list[dict]:
    """
    Call the arXiv export API directly and parse results.

    Returns a list of dicts with keys:
      'id', 'title', 'authors', 'summary', 'published', 'pdf_url'
    """
    params = {
        # search all fields with the LLM query
        "search_query": f"all:{search}",
        "start": 0,
        "max_results": max_results,
    }

    resp = _throttled_arxiv_get(params)

    root = ET.fromstring(resp.text)
    ns = {"atom": "http://www.w3.org/2005/Atom"}

    entries: list[dict] = []
    for entry in root.findall("atom:entry", ns):
        title_el = entry.find("atom:title", ns)
        id_el = entry.find("atom:id", ns)
        summary_el = entry.find("atom:summary", ns)
        published_el = entry.find("atom:published", ns)

        title = (title_el.text or "").strip() if title_el is not None else ""
        entry_id = (id_el.text or "").strip() if id_el is not None else ""
        summary = (summary_el.text or "").strip() if summary_el is not None else ""
        published = (published_el.text or "").strip() if published_el is not None else ""

        # authors
        authors_list = []
        for a in entry.findall("atom:author", ns):
            name_el = a.find("atom:name", ns)
            if name_el is not None and name_el.text:
                authors_list.append(name_el.text.strip())
        authors = ", ".join(authors_list)

        # build a pdf URL from the id: http://arxiv.org/abs/... -> http://arxiv.org/pdf/....pdf
        pdf_url = ""
        if "/abs/" in entry_id:
            pdf_url = entry_id.replace("/abs/", "/pdf/") + ".pdf"

        entries.append(
            {
                "id": entry_id,
                "title": title,
                "authors": authors,
                "summary": summary,
                "published": published,
                "pdf_url": pdf_url,
            }
        )

    return entries


def arxiv_scraper(
    search: str,
    pdir: str = "arxiv_query",
    max_results: int = 20,
) -> dict:
    """
    Use the raw arXiv API to find up to max_results papers matching `search`,
    download their PDFs, and return a mapping:

        path -> {"citation": <citation_string>}
    """
    if not os.path.isdir(pdir):
        os.makedirs(pdir, exist_ok=True)

    results = _arxiv_api_query(search, max_results=max_results)
    papers: dict = {}

    for r in results:
        if not r.get("pdf_url"):
            continue

        try:
            # build a reasonably unique filename from the id
            arxiv_id = r["id"].split("/")[-1] if r["id"] else "arxiv"
            filename = f"{arxiv_id}.pdf"
            path = os.path.join(pdir, filename)

            # PDF downloads: sequential, via same SESSION/UA
            pdf_resp = SESSION.get(r["pdf_url"], timeout=30)
            pdf_resp.raise_for_status()

            with open(path, "wb") as f:
                f.write(pdf_resp.content)

            citation = (
                f"Title: {r['title']}\n"
                f"Authors: {r['authors']}\n"
                f"Published: {r['published']}\n"
                f"ArXiv ID: {arxiv_id}\n"
                f"URL: {r['id']}\n\n"
                f"Abstract: {r['summary']}"
            )

            papers[path] = {"citation": citation}
        except Exception:
            # skip any entry that fails download / parse
            continue

    return papers


# ------------ LLM wrapper functions (ChemCrow-style) ------------ #

def arxiv_paper_search(llm, query, max_results=20):
    """
    Use an LLM to compress the user query to a short arXiv search string,
    then run an arXiv search and download the PDFs.
    """
    prompt = langchain.prompts.PromptTemplate(
        input_variables=["question"],
        template="""
        I would like to find scholarly papers to answer
        this question: {question}. Your response must be at
        most 10 words long.
        A search query that would bring up papers that can answer
        this question would be: """,
    )

    query_chain = langchain.chains.llm.LLMChain(llm=llm, prompt=prompt)

    base_dir = "arxiv_query"
    if not os.path.isdir(base_dir):
        os.mkdir(base_dir)

    search = query_chain.run(query).strip()
    print("\nArXiv search:", search)

    # subdirectory per search term (strip whitespace)
    subdir = re.sub(r"\s+", "", search) or "default"
    search_dir = os.path.join(base_dir, subdir)
    if not os.path.isdir(search_dir):
        os.mkdir(search_dir)

    papers = arxiv_scraper(search, pdir=search_dir, max_results=max_results)
    return papers


def arxiv2result_llm(
    llm,
    query,
    k: int = 5,
    max_sources: int = 2,
    openai_api_key: str = None,
    max_results: int = 20,
):
    """
    ArXiv-based analogue of scholar2result_llm:
    - Use LLM to generate a focused search query
    - Search arXiv & download PDFs
    - Use paperqa to answer the question from those PDFs
    """
    papers = arxiv_paper_search(llm, query, max_results=max_results)
    if len(papers) == 0:
        return "Not enough arXiv papers found"

    docs = paperqa.Docs(
        llm=llm,
        summary_llm=llm,
        embeddings=OpenAIEmbeddings(openai_api_key=openai_api_key),
    )

    not_loaded = 0
    for path, data in papers.items():
        try:
            docs.add(path, data["citation"])
        except (ValueError, FileNotFoundError, PdfReadError):
            not_loaded += 1

    if not_loaded > 0:
        print(
            f"\nFound {len(papers.items())} arXiv papers "
            f"but couldn't load {not_loaded}."
        )
    else:
        print(f"\nFound {len(papers.items())} arXiv papers and loaded all of them.")

    answer = docs.query(query, k=k, max_sources=max_sources).formatted_answer
    return answer


# ------------ ChemCrow tool wrapper ------------ #

class Arxiv2ResultLLM(BaseTool):
    """
    ChemCrow tool for answering technical questions using arxiv.org papers.

    Usage pattern mirrors the existing Semantic Scholar-based LiteratureSearch tool.
    """

    name = "ArxivLiteratureSearch"
    description = (
        "Useful to answer questions that require technical knowledge, "
        "by searching arxiv.org (physics, CS, math, etc.) and reading "
        "the top papers. Ask a specific question."
    )

    llm: BaseLanguageModel = None
    openai_api_key: str = None
    max_results: int = 20

    def __init__(
        self,
        llm: BaseLanguageModel,
        openai_api_key: str = None,
        max_results: int = 20,
    ):
        super().__init__()
        self.llm = llm
        self.openai_api_key = openai_api_key
        self.max_results = max_results

    def _run(self, query: str) -> str:
        return arxiv2result_llm(
            self.llm,
            query,
            openai_api_key=self.openai_api_key,
            max_results=self.max_results,
        )

    async def _arun(self, query: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("this tool does not support async")

```


================================================================================
=== FILE: chemcrow\tools\New\BayesianOptimizer.py ===
================================================================================

```python
import os
import json
import glob
import math
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd
from langchain.base_language import BaseLanguageModel
from langchain.tools import BaseTool
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (
    Matern,
    WhiteKernel,
    ConstantKernel as C,
)
from sklearn.preprocessing import StandardScaler


# ===========================
#  Utility: normal PDF / CDF
# ===========================

def _norm_pdf(z: np.ndarray) -> np.ndarray:
    return np.exp(-0.5 * z ** 2) / math.sqrt(2.0 * math.pi)


def _norm_cdf(z: np.ndarray) -> np.ndarray:
    # 0.5 * (1 + erf(z / sqrt(2)))
    return 0.5 * (1.0 + _erf(z / math.sqrt(2.0)))


def _erf(x: np.ndarray) -> np.ndarray:
    """
    Approximate error function (vectorised A&S 7.1.26).
    Good enough for EI in BO.
    """
    # handle vector input
    x = np.asarray(x, dtype=float)
    sign = np.sign(x)
    x_abs = np.abs(x)

    a1 = 0.254829592
    a2 = -0.284496736
    a3 = 1.421413741
    a4 = -1.453152027
    a5 = 1.061405429
    p = 0.3275911

    t = 1.0 / (1.0 + p * x_abs)
    y = 1.0 - (
        (((a5 * t + a4) * t + a3) * t + a2) * t + a1
    ) * t * np.exp(-x_abs * x_abs)

    return sign * y


# ===========================
#  CSV / ID handling
# ===========================

def _infer_id_column(dfs: List[pd.DataFrame]) -> str:
    """
    Infer a crystal identifier column present across all dataframes.
    Prioritises typical names like 'crystal_name' or 'xtal'.
    """
    if not dfs:
        raise ValueError("No dataframes provided to infer ID column")

    common = set(dfs[0].columns)
    for df in dfs[1:]:
        common &= set(df.columns)

    preferred = ["crystal_name", "xtal", "name", "id"]

    for col in preferred:
        if col in common:
            return col

    if not common:
        # Fall back to the first column of the first dataframe
        return dfs[0].columns[0]

    # Prefer string-like columns among the common set
    for col in dfs[0].columns:
        if col in common and dfs[0][col].dtype == object:
            return col

    # Otherwise just take an arbitrary common column
    return list(common)[0]


def _load_descriptor_csv(path: str) -> pd.DataFrame:
    if not os.path.isfile(path):
        raise FileNotFoundError(f"Descriptor CSV not found: {path}")
    return pd.read_csv(path)


def _load_property_csvs(paths: List[str]) -> List[pd.DataFrame]:
    dfs = []
    for p in paths:
        if not os.path.isfile(p):
            raise FileNotFoundError(f"Property CSV not found: {p}")
        dfs.append(pd.read_csv(p))
    return dfs


def _build_merged_table(
    cif_dir: str,
    descriptor_csv: str,
    property_csvs: List[str],
    target_properties: List[str],
    id_column: Optional[str] = None,
    property_agg: str = "mean",
) -> pd.DataFrame:
    """
    Load descriptors + property CSV(s), join on the ID column,
    and (optionally) restrict to crystals for which we have CIFs.

    target_properties: list of columns (objectives) we want to maximise.
    """
    desc_df = _load_descriptor_csv(descriptor_csv)
    prop_dfs = _load_property_csvs(property_csvs)

    # Infer ID column if needed
    if id_column is None:
        id_column = _infer_id_column([desc_df] + prop_dfs)

    if id_column not in desc_df.columns:
        raise ValueError(
            f"Inferred ID column '{id_column}' not found in descriptor CSV."
        )

    # Merge all property CSVs, aggregating numeric columns by ID
    merged_props = None
    for df in prop_dfs:
        if id_column not in df.columns:
            raise ValueError(
                f"ID column '{id_column}' not found in property CSV with columns {list(df.columns)}"
            )

        if property_agg == "max":
            g = df.groupby(id_column).max(numeric_only=True)
        elif property_agg == "min":
            g = df.groupby(id_column).min(numeric_only=True)
        else:
            g = df.groupby(id_column).mean(numeric_only=True)

        g = g.reset_index()

        if merged_props is None:
            merged_props = g
        else:
            merged_props = pd.merge(
                merged_props,
                g,
                on=id_column,
                how="outer",
                suffixes=("", "_prop"),
            )

    if merged_props is None:
        raise ValueError("No property data could be loaded / merged.")

    merged = pd.merge(desc_df, merged_props, on=id_column, how="inner")

    # Restrict to COFs we have CIFs for (if directory given)
    if cif_dir and os.path.isdir(cif_dir):
        cif_files = {
            os.path.splitext(os.path.basename(p))[0]
            for p in glob.glob(os.path.join(cif_dir, "*.cif"))
        }
        if cif_files:
            mask = merged[id_column].astype(str).isin(cif_files)
            merged = merged[mask].copy()

    # Check all target properties exist
    missing = [t for t in target_properties if t not in merged.columns]
    if missing:
        raise ValueError(
            f"Target properties {missing} not found in merged data. "
            f"Available columns: {list(merged.columns)}"
        )

    merged.attrs["id_column"] = id_column
    return merged


# ===========================
#  Pareto utilities
# ===========================

def _compute_pareto_front(
    Y: np.ndarray, maximise: bool = True
) -> np.ndarray:
    """
    Compute indices of Pareto-optimal points in Y (n x m).

    maximise=True means each objective is to be maximised.
    """
    Y = np.asarray(Y, dtype=float)
    n, m = Y.shape
    is_dominated = np.zeros(n, dtype=bool)

    # For maximisation, domination means >= in all, > in at least one
    for i in range(n):
        if is_dominated[i]:
            continue
        for j in range(n):
            if j == i or is_dominated[j]:
                continue
            if maximise:
                if np.all(Y[j] >= Y[i]) and np.any(Y[j] > Y[i]):
                    is_dominated[i] = True
                    break
            else:
                if np.all(Y[j] <= Y[i]) and np.any(Y[j] < Y[i]):
                    is_dominated[i] = True
                    break
    return np.where(~is_dominated)[0]


# ===========================
#  Multi-objective BO core
# ===========================

def _fit_multi_gp(
    X_obs: np.ndarray,
    Y_obs: np.ndarray,
) -> Tuple[StandardScaler, List[GaussianProcessRegressor]]:
    """
    Fit one GP per objective on observed points.
    Returns feature scaler + list of fitted GPs.
    """
    scaler = StandardScaler()
    X_obs_scaled = scaler.fit_transform(X_obs)

    gps: List[GaussianProcessRegressor] = []
    for j in range(Y_obs.shape[1]):
        y_j = Y_obs[:, j]

        kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, nu=2.5) \
                 + WhiteKernel(noise_level=1e-3)

        gp = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=5,
            normalize_y=True,
            random_state=0,
        )
        gp.fit(X_obs_scaled, y_j)
        gps.append(gp)

    return scaler, gps


def _predict_multi_gp(
    scaler: StandardScaler,
    gps: List[GaussianProcessRegressor],
    X_all: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Predict means and stds for all objectives at all points.

    Returns:
      mu_all: (n_points, n_obj)
      sigma_all: (n_points, n_obj)
    """
    X_all_scaled = scaler.transform(X_all)
    n_points = X_all.shape[0]
    n_obj = len(gps)

    mu_all = np.zeros((n_points, n_obj), dtype=float)
    sigma_all = np.zeros((n_points, n_obj), dtype=float)

    for j, gp in enumerate(gps):
        mu_j, sigma_j = gp.predict(X_all_scaled, return_std=True)
        mu_all[:, j] = mu_j
        sigma_all[:, j] = sigma_j

    return mu_all, sigma_all


def _sample_weights(
    n_obj: int,
    weights_cfg: Optional[Dict],
) -> np.ndarray:
    """
    Return an array of weight vectors on the simplex.

    weights_cfg can contain:
      - 'weights': list[float] (single weight vector) OR
                   list[list[float]] (multiple)
      - 'n_weight_samples': int
    """
    if weights_cfg is None:
        weights_cfg = {}

    user_w = weights_cfg.get("weights")
    n_samples = int(weights_cfg.get("n_weight_samples", 4))

    if user_w is not None:
        # Single vector: [w1, w2, ...]
        if isinstance(user_w, list) and user_w and isinstance(user_w[0], (float, int)):
            w = np.asarray(user_w, dtype=float)
            if len(w) != n_obj:
                raise ValueError(
                    f"weights length {len(w)} != number of objectives {n_obj}"
                )
            w = w / w.sum()
            return np.atleast_2d(w)

        # List of vectors: [[...], [...], ...]
        if isinstance(user_w, list) and user_w and isinstance(user_w[0], list):
            W = []
            for vec in user_w:
                v = np.asarray(vec, dtype=float)
                if len(v) != n_obj:
                    raise ValueError(
                        f"One weight vector has length {len(v)} != n_obj {n_obj}"
                    )
                v = v / v.sum()
                W.append(v)
            return np.asarray(W)

        raise ValueError("Invalid 'weights' format in config.")

    # Default: random Dirichlet samples on simplex
    rng = np.random.default_rng(0)
    W = rng.dirichlet(np.ones(n_obj, dtype=float), size=n_samples)
    return W


def _run_multiobjective_bo(
    merged: pd.DataFrame,
    target_properties: List[str],
    top_k: int = 15,
    weights_cfg: Optional[Dict] = None,
) -> str:
    """
    Multi-objective BO over a discrete set of COFs.

    - Fits a GP surrogate per objective on points where all objectives are observed.
    - Normalises objectives (z-score).
    - Samples weight vectors on the simplex and forms a linear scalarisation
      g(x) = sum_j w_j * z_j(x) (scalarized normalised objective).
    - Uses Expected Improvement (EI) on g(x) to rank unobserved points,
      aggregating by max EI across sampled weight vectors.

    Returns a human-readable summary string.
    """
    id_column = merged.attrs.get("id_column", merged.columns[0])
    ids = merged[id_column].astype(str).values

    # Numeric features + objectives
    numeric_cols = merged.select_dtypes(include=[np.number]).columns.tolist()
    for t in target_properties:
        if t not in numeric_cols:
            raise ValueError(f"Target property '{t}' is not numeric.")

    feature_cols = [c for c in numeric_cols if c not in target_properties]
    if not feature_cols:
        raise ValueError("No numeric descriptor features found for BO.")

    X_all = merged[feature_cols].values.astype(float)
    Y_all = merged[target_properties].values.astype(float)

    # Observed if all objectives are present
    mask_obs = np.all(np.isfinite(Y_all), axis=1)
    if mask_obs.sum() < max(3, len(target_properties) + 1):
        # Not enough data for proper GP; just Pareto + ranking
        Y_obs = Y_all[mask_obs]
        ids_obs = ids[mask_obs]
        pf_idx = _compute_pareto_front(Y_obs, maximise=True)

        lines = []
        lines.append(
            f"Not enough data for GP-based multi-objective BO "
            f"(only {mask_obs.sum()} fully-observed points; need â‰¥ {max(3, len(target_properties)+1)})."
        )
        lines.append("")
        lines.append("Pareto-optimal COFs among observed points:")
        for i in pf_idx:
            prop_str = ", ".join(
                f"{name} = {val:.4g}"
                for name, val in zip(target_properties, Y_obs[i])
            )
            lines.append(f"- {ids_obs[i]}: {prop_str}")
        return "\n".join(lines)

    X_obs = X_all[mask_obs]
    Y_obs = Y_all[mask_obs]
    ids_obs = ids[mask_obs]

    # Fit one GP per objective
    scaler, gps = _fit_multi_gp(X_obs, Y_obs)

    # Predict for all points
    mu_all, sigma_all = _predict_multi_gp(scaler, gps, X_all)

    # Normalise objectives using observed Y
    y_mean = Y_obs.mean(axis=0)
    y_std = Y_obs.std(axis=0)
    y_std[y_std == 0.0] = 1.0

    # Predicted mean & std in z-space
    z_mu_all = (mu_all - y_mean) / y_std
    z_sigma_all = sigma_all / y_std

    # Observed z-values (true) for computing g_best
    Z_obs = (Y_obs - y_mean) / y_std

    n_points, n_obj = z_mu_all.shape

    # Candidate indices: prioritise unobserved
    unobs_mask = ~mask_obs
    if unobs_mask.any():
        candidate_idx = np.where(unobs_mask)[0]
    else:
        candidate_idx = np.arange(n_points)

    # Sample weights on simplex
    W = _sample_weights(n_obj, weights_cfg)
    n_w = W.shape[0]

    # For each weight vector, compute EI on scalarised objective
    EI_mat = np.zeros((n_w, candidate_idx.shape[0]), dtype=float)

    xi = 0.01  # exploration parameter

    for w_i, w in enumerate(W):
        # scalarised normalised means at all points
        g_mu_all = (z_mu_all * w).sum(axis=1)
        g_sigma_all = np.sqrt(
            np.sum((z_sigma_all * w) ** 2, axis=1)
        )

        # Best scalarised value over observed points
        g_obs = (Z_obs * w).sum(axis=1)
        g_best = float(np.max(g_obs))

        # EI for candidates
        mu_c = g_mu_all[candidate_idx]
        sig_c = g_sigma_all[candidate_idx]

        with np.errstate(divide="ignore", invalid="ignore"):
            z = (mu_c - g_best - xi) / sig_c
            cdf_z = _norm_cdf(z)
            pdf_z = _norm_pdf(z)
            ei = (mu_c - g_best - xi) * cdf_z + sig_c * pdf_z
            ei[sig_c <= 0.0] = 0.0

        EI_mat[w_i, :] = ei

    # Aggregate EI across weights: max EI per candidate
    ei_max = EI_mat.max(axis=0)
    # Sort candidates by EI descending
    ranking_local = np.argsort(-ei_max)
    top_k = min(top_k, len(ranking_local))
    best_local = ranking_local[:top_k]
    best_idx = candidate_idx[best_local]

    # Pareto front among observed points (based on true Y_obs)
    pf_idx = _compute_pareto_front(Y_obs, maximise=True)

    # Build summary
    lines = []
    lines.append(
        f"Multi-objective Bayesian Optimization over {n_points} COFs "
        f"with {n_obj} objectives: {', '.join(target_properties)} (all maximised)."
    )
    lines.append(
        f"Fully-observed points: {len(Y_obs)}; unobserved candidates: {int(unobs_mask.sum())}."
    )
    lines.append(
        f"Using scalarisation-based MOBO with {n_w} weight vector(s) on the objective simplex "
        f"and EI as acquisition."
    )
    lines.append("")

    # Current Pareto front
    lines.append("Pareto-optimal COFs among currently observed points:")
    for i in pf_idx:
        prop_str = ", ".join(
            f"{name} = {val:.4g}"
            for name, val in zip(target_properties, Y_obs[i])
        )
        lines.append(f"- {ids_obs[i]}: {prop_str}")

    # BO suggestions
    lines.append("")
    lines.append(f"Top {top_k} BO suggestions (max EI over sampled weights):")
    for idx, ei_val in zip(best_idx, ei_max[best_local]):
        # predicted objectives for this candidate
        pred_props = [
            f"{name} â‰ˆ {mu_all[idx, j]:.4g} Â± {sigma_all[idx, j]:.4g}"
            for j, name in enumerate(target_properties)
        ]
        lines.append(
            f"- {ids[idx]}: EI â‰ˆ {ei_val:.4g}; "
            + "; ".join(pred_props)
        )

    lines.append("")
    lines.append(
        "Note: suggestions are generated via linear scalarisation of "
        "normalised objectives plus Expected Improvement, which is a standard "
        "scalarisation-based multi-objective BO strategy."
    )

    return "\n".join(lines)


def run_cof_multi_bo_from_config(config_json: str) -> str:
    """
    Entry point: takes a JSON string describing the multi-objective BO problem
    and returns a text summary.

    Expected JSON structure:

    {
      "cif_dir": "C:/.../crystals",                # optional
      "descriptor_csv": "C:/.../cof_descriptors.csv",
      "property_csvs": ["C:/.../gcmc_calculations.csv", "..."],
      "target_properties": [
          "âŸ¨NâŸ© (mmol/g)",
          "selectivity Xe/Kr"
      ],
      "id_column": "crystal_name",                 # optional; inferred if omitted
      "property_agg": "mean",                      # optional: "mean" | "max" | "min"
      "top_k": 15,                                 # optional
      "weights": [0.5, 0.5],                       # optional: or [[...], [...], ...]
      "n_weight_samples": 4                        # optional if no weights given
    }

    All objectives are assumed to be maximised.
    """
    try:
        cfg = json.loads(config_json)
    except json.JSONDecodeError as e:
        return f"Failed to parse JSON config: {e}"

    descriptor_csv = cfg.get("descriptor_csv")
    property_csvs = cfg.get("property_csvs")
    target_properties = cfg.get("target_properties")

    if not descriptor_csv or not property_csvs or not target_properties:
        return (
            "Config must include 'descriptor_csv', 'property_csvs', "
            "and 'target_properties' (list of columns to maximise)."
        )

    if isinstance(property_csvs, str):
        property_csvs = [property_csvs]

    if not isinstance(target_properties, list) or len(target_properties) < 2:
        return (
            "'target_properties' must be a list of at least two objective "
            "column names for multi-objective BO."
        )

    cif_dir = cfg.get("cif_dir", "")
    id_column = cfg.get("id_column")
    property_agg = cfg.get("property_agg", "mean")
    top_k = int(cfg.get("top_k", 15))

    weights_cfg = {
        k: v for k, v in cfg.items()
        if k in ("weights", "n_weight_samples")
    } or None

    try:
        merged = _build_merged_table(
            cif_dir=cif_dir,
            descriptor_csv=descriptor_csv,
            property_csvs=property_csvs,
            target_properties=target_properties,
            id_column=id_column,
            property_agg=property_agg,
        )
        summary = _run_multiobjective_bo(
            merged=merged,
            target_properties=target_properties,
            top_k=top_k,
            weights_cfg=weights_cfg,
        )
        return summary
    except Exception as e:
        return f"Error during multi-objective COF BO: {e}"


# ===========================
#  ChemCrow tool wrapper
# ===========================

class COFMultiObjectiveBO(BaseTool):
    """
    ChemCrow tool for **multi-objective** Bayesian Optimization over a COF library.

    Input MUST be a single JSON string with fields:

      - 'cif_dir':        directory containing CIF files (optional but recommended)
      - 'descriptor_csv': CSV with numeric descriptors per crystal
      - 'property_csvs':  list of CSVs with properties per crystal
      - 'target_properties': list of property column names to MAXIMISE
                             (e.g. uptake and selectivity)
      - 'id_column':      common crystal ID column across CSVs (optional; inferred)
      - 'property_agg':   'mean' (default), 'max', or 'min' aggregation over repeats
      - 'top_k':          number of BO suggestions to return (default: 15)

    Optional scalarisation config:

      - 'weights':        either [w1, ..., wm] for a single preference vector
                          or [[...], [...], ...] for several.
      - 'n_weight_samples': number of random weight vectors if 'weights' omitted.

    The tool:
      - Fits a Gaussian Process surrogate for each objective.
      - Uses scalarisation-based MOBO (linear scalarisation + EI) across
        sampled weight vectors.
      - Reports the current Pareto front (observed) and top BO suggestions.
    """

    name = "COFMultiObjectiveBO"
    description = (
        "Run multi-objective Bayesian optimisation over a COF dataset using "
        "descriptors + property CSVs. Input is a JSON string with paths and "
        "a list of target_properties to maximise; output is a text summary "
        "including the current Pareto front and BO suggestions."
    )

    llm: Optional[BaseLanguageModel] = None  # not used, kept for symmetry

    def __init__(self, llm: Optional[BaseLanguageModel] = None):
        super().__init__()
        self.llm = llm

    def _run(self, query: str) -> str:
        return run_cof_multi_bo_from_config(query)

    async def _arun(self, query: str) -> str:
        raise NotImplementedError("This tool does not support async.")

```


================================================================================
=== FILE: chemcrow\tools\New\motifs_cof_simple.json ===
================================================================================

```json
[
  {
    "name": "C_sp2_like",
    "central_species": "C",
    "neighbor_species_counts": { "C": 2, "N": 1 },
    "max_distance": 1.7
  },
  {
    "name": "C_sp2_allC",
    "central_species": "C",
    "neighbor_species_counts": { "C": 3 },
    "max_distance": 1.7
  },
  {
    "name": "N_linker",
    "central_species": "N",
    "neighbor_species_counts": { "C": 2 },
    "max_distance": 1.6
  },
  {
    "name": "O_linker",
    "central_species": "O",
    "neighbor_species_counts": { "C": 1 },
    "max_distance": 1.6
  }
]

```


================================================================================
=== FILE: chemcrow\tools\New\motif_tools.py ===
================================================================================

```python
import json
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence

from langchain.tools import BaseTool
from pymatgen.core import Structure
from pymatgen.analysis.local_env import MinimumDistanceNN

# Optional: handle NumPy types when they appear
try:
    import numpy as np
except ImportError:
    np = None


def _json_default(o):
    """
    Helper for json.dumps to convert NumPy types to plain Python types.
    """
    if np is not None:
        if isinstance(o, np.integer):
            return int(o)
        if isinstance(o, np.floating):
            return float(o)
        if isinstance(o, np.ndarray):
            return o.tolist()
    # Fallback: just stringify anything unknown
    return str(o)


# =========================
# Motif representation
# =========================

@dataclass
class MotifPattern:
    """
    Simple chemical motif: central species + neighbor species counts + max distance.

    Example JSON entry:
    {
      "name": "TiO6_oct",
      "central_species": "Ti",
      "neighbor_species_counts": {"O": 6},
      "max_distance": 2.3
    }
    """
    name: str
    central_species: str
    neighbor_species_counts: Dict[str, int]
    max_distance: float
    extra: Dict[str, Any] = field(default_factory=dict)


def _load_motif_library_from_file(path: str | Path) -> List[MotifPattern]:
    data = json.loads(Path(path).read_text())
    return [MotifPattern(**entry) for entry in data]


def _load_motif_library(
    motifs: Optional[Sequence[Dict]] = None,
    motif_library_path: Optional[str] = None,
) -> List[MotifPattern]:
    """
    Load motif patterns either from an inline list (motifs)
    or from a JSON file (motif_library_path).
    """
    if motifs is not None:
        return [MotifPattern(**m) for m in motifs]
    if motif_library_path is not None:
        return _load_motif_library_from_file(motif_library_path)
    raise ValueError("No motif definitions provided: supply 'motifs' or 'motif_library_path'.")


# =========================
# Core motif logic
# =========================

def _get_neighbor_info(
    structure: Structure,
    central_index: int,
    max_distance: float,
    nn_strategy=None,
):
    if nn_strategy is None:
        nn_strategy = MinimumDistanceNN()
    neighs = nn_strategy.get_nn_info(structure, central_index)

    neighbors = []
    central_site = structure[central_index]
    for info in neighs:
        j = info["site_index"]
        dist = central_site.distance(structure[j])
        if dist <= max_distance:
            species_j = (
                structure[j].specie.symbol
                if hasattr(structure[j], "specie")
                else structure[j].species_string
            )
            neighbors.append((j, species_j, dist))

    return neighbors


def _match_pattern(
    structure: Structure,
    central_index: int,
    pattern: MotifPattern,
) -> Optional[Dict]:
    # Check central species first
    central_site = structure[central_index]
    central_species = (
        central_site.specie.symbol
        if hasattr(central_site, "specie")
        else central_site.species_string
    )
    if central_species != pattern.central_species:
        return None

    neighbors = _get_neighbor_info(structure, central_index, pattern.max_distance)
    if not neighbors:
        return None

    # Count neighbors by species
    counts: Dict[str, int] = {}
    for _, species, _ in neighbors:
        counts[species] = counts.get(species, 0) + 1

    # Fail fast if we don't have enough of any required species
    for sp, req in pattern.neighbor_species_counts.items():
        if counts.get(sp, 0) < req:
            return None

    # Choose closest neighbors for each species
    chosen_neighbors: List[int] = []
    for sp, req in pattern.neighbor_species_counts.items():
        cand = [(idx, dist) for (idx, species, dist) in neighbors if species == sp]
        cand.sort(key=lambda x: x[1])
        if len(cand) < req:
            return None
        chosen_neighbors.extend(idx for idx, _ in cand[:req])

    return {
        "motif_name": pattern.name,
        "central_index": central_index,
        "neighbor_indices": sorted(set(chosen_neighbors)),
    }


def find_motif_occurrences(
    structure: Structure,
    pattern: MotifPattern,
) -> List[Dict]:
    """
    Find all occurrences of `pattern` in `structure`.
    """
    matches: List[Dict] = []
    for i in range(len(structure)):
        m = _match_pattern(structure, i, pattern)
        if m is not None:
            matches.append(m)
    return matches


def decompose_structure(
    structure: Structure,
    motif_library: Sequence[MotifPattern],
    allowed_motifs: Optional[Sequence[str]] = None,
    allow_overlap: bool = True,
) -> Dict:
    """
    Decompose a structure into motif instances from `motif_library`.
    """
    if allowed_motifs is not None:
        allowed = set(allowed_motifs)
        motifs = [m for m in motif_library if m.name in allowed]
    else:
        motifs = list(motif_library)

    raw_matches: List[Dict] = []
    for pattern in motifs:
        occs = find_motif_occurrences(structure, pattern)
        raw_matches.extend(occs)

    if allow_overlap:
        assigned = raw_matches
    else:
        # Greedy: prefer motifs with largest number of sites
        raw_matches.sort(key=lambda m: len(m["neighbor_indices"]) + 1, reverse=True)
        used_sites = set()
        assigned = []
        for m in raw_matches:
            sites = {m["central_index"], *m["neighbor_indices"]}
            if sites & used_sites:
                continue
            assigned.append(m)
            used_sites |= sites

    all_sites = set(range(len(structure)))
    covered_sites = set()
    for m in assigned:
        covered_sites.add(m["central_index"])
        covered_sites.update(m["neighbor_indices"])

    unassigned_sites = sorted(all_sites - covered_sites)

    return {
        "motifs": assigned,
        "unassigned_sites": unassigned_sites,
    }


# =========================
# Structure comparison logic
# =========================

def compare_two_structures(
    structure1: Structure,
    structure2: Structure,
    motif_library: Sequence[MotifPattern],
    allowed_motifs: Optional[Sequence[str]] = None,
    allow_overlap: bool = True,
) -> Dict:
    """
    Decompose both structures and compare motifs.
    """
    decomp1 = decompose_structure(
        structure1, motif_library, allowed_motifs=allowed_motifs, allow_overlap=allow_overlap
    )
    decomp2 = decompose_structure(
        structure2, motif_library, allowed_motifs=allowed_motifs, allow_overlap=allow_overlap
    )

    index1: Dict[str, List[Dict]] = defaultdict(list)
    index2: Dict[str, List[Dict]] = defaultdict(list)

    for m in decomp1["motifs"]:
        index1[m["motif_name"]].append(m)
    for m in decomp2["motifs"]:
        index2[m["motif_name"]].append(m)

    names1 = set(index1.keys())
    names2 = set(index2.keys())

    shared = []
    for name in sorted(names1 & names2):
        shared.append(
            {
                "motif_name": name,
                "count_1": len(index1[name]),
                "count_2": len(index2[name]),
                "instances_1": index1[name],
                "instances_2": index2[name],
            }
        )

    result = {
        "decomp_1": decomp1,
        "decomp_2": decomp2,
        "shared_motifs": shared,
        "unique_to_1": sorted(names1 - names2),
        "unique_to_2": sorted(names2 - names1),
    }
    return result


# =========================
# Small summaries for the LLM
# =========================

def _summarise_decomposition_for_llm(result: Dict, max_examples_per_motif: int = 3) -> Dict:
    """
    Compress a full decomposition result into something LLM-friendly:
    - counts per motif_name
    - total motif instances
    - number of unassigned sites
    - a few example instances per motif
    """
    motifs: List[Dict] = result.get("motifs", [])
    unassigned_sites = result.get("unassigned_sites", [])

    counts: Dict[str, int] = defaultdict(int)
    examples: Dict[str, List[Dict]] = defaultdict(list)
    for m in motifs:
        name = m.get("motif_name", "UNKNOWN")
        counts[name] += 1
        if len(examples[name]) < max_examples_per_motif:
            examples[name].append(
                {
                    "central_index": int(m.get("central_index", -1)),
                    "neighbor_indices": m.get("neighbor_indices", []),
                }
            )

    return {
        "total_motif_instances": len(motifs),
        "motif_counts": dict(counts),
        "unassigned_sites_count": len(unassigned_sites),
        "unassigned_sites_sample": unassigned_sites[:20],
        "example_instances": {k: v for k, v in examples.items()},
    }


def _summarise_comparison_for_llm(result: Dict, max_examples_per_motif: int = 3) -> Dict:
    """
    Compress the full comparison result:
    - summaries of decomp_1 and decomp_2
    - shared motif names + counts
    - unique motif names
    """
    decomp1 = result.get("decomp_1", {})
    decomp2 = result.get("decomp_2", {})
    shared = result.get("shared_motifs", [])
    unique1 = result.get("unique_to_1", [])
    unique2 = result.get("unique_to_2", [])

    summary_decomp1 = _summarise_decomposition_for_llm(
        {"motifs": decomp1.get("motifs", []), "unassigned_sites": decomp1.get("unassigned_sites", [])},
        max_examples_per_motif=max_examples_per_motif,
    )
    summary_decomp2 = _summarise_decomposition_for_llm(
        {"motifs": decomp2.get("motifs", []), "unassigned_sites": decomp2.get("unassigned_sites", [])},
        max_examples_per_motif=max_examples_per_motif,
    )

    shared_summary = []
    for s in shared:
        shared_summary.append(
            {
                "motif_name": s.get("motif_name", "UNKNOWN"),
                "count_1": int(s.get("count_1", 0)),
                "count_2": int(s.get("count_2", 0)),
            }
        )

    return {
        "decomp_1_summary": summary_decomp1,
        "decomp_2_summary": summary_decomp2,
        "shared_motifs": shared_summary,
        "unique_to_1": unique1,
        "unique_to_2": unique2,
    }


# =========================
# ChemCrow tools
# =========================

class MotifDecompositionTool(BaseTool):
    """
    ChemCrow tool for motif analysis in CIF files.

    INPUT FORMAT (query string):
    -----------------------------
    JSON with keys:

      Required:
        - "mode": one of ["search", "all", "from-list"]
        - "cif_path": path to the CIF file on disk

      Motif definitions: provide EITHER:
        - "motifs": list of motif definitions, OR
        - "motif_library_path": path to JSON motifs file

      Optional:
        - "motif_name" (for mode == "search")
        - "allowed_motifs" (for mode == "from-list")
        - "allow_overlap": bool

    OUTPUT:
    -------
    A small JSON summary **plus** a path to the full JSON file on disk
    with all motif instances and unassigned sites.
    """

    name: str = "MotifDecomposition"
    description: str = (
        "Analyze coordination motifs in a CIF file. "
        "Input MUST be a JSON string with keys: "
        "'mode' (search|all|from-list), 'cif_path', and "
        "either 'motifs' or 'motif_library_path'. "
        "Returns a compact summary and also saves the **full** result "
        "to a JSON file on disk for offline inspection."
    )

    default_motif_library_path: Optional[str] = None

    def __init__(self, default_motif_library_path: Optional[str] = None):
        super().__init__()
        self.default_motif_library_path = default_motif_library_path

    def _run(self, query: str) -> str:
        try:
            params = json.loads(query)
        except json.JSONDecodeError:
            return (
                "Invalid input for MotifDecomposition. "
                "Expected a JSON string. Example:\n"
                '{ "mode": "all", "cif_path": "path/to/file.cif", '
                '"motif_library_path": "path/to/motifs.json" }'
            )

        mode = params.get("mode", "all")
        cif_path = params.get("cif_path", None)
        if cif_path is None:
            return "Error: 'cif_path' is required."

        motif_defs = params.get("motifs", None)
        motif_library_path = params.get("motif_library_path", self.default_motif_library_path)
        allow_overlap = params.get("allow_overlap", True)

        try:
            motif_library = _load_motif_library(
                motifs=motif_defs,
                motif_library_path=motif_library_path,
            )
        except Exception as e:
            return f"Error loading motif library: {e}"

        try:
            structure = Structure.from_file(cif_path)
        except Exception as e:
            return f"Error reading CIF file '{cif_path}': {e}"

        # Build full internal result
        if mode == "search":
            motif_name = params.get("motif_name", None)
            if not motif_name:
                return "Error: 'motif_name' is required in 'search' mode."
            try:
                pattern = next(m for m in motif_library if m.name == motif_name)
            except StopIteration:
                return f"Error: motif '{motif_name}' not found in library."
            occs = find_motif_occurrences(structure, pattern)
            full_result = {
                "mode": "search",
                "motif_name": motif_name,
                "motifs": occs,
                "unassigned_sites": [],
            }

        elif mode == "from-list":
            allowed = params.get("allowed_motifs", None)
            if not allowed:
                return "Error: 'allowed_motifs' is required in 'from-list' mode."
            decomp = decompose_structure(
                structure,
                motif_library,
                allowed_motifs=allowed,
                allow_overlap=allow_overlap,
            )
            decomp["mode"] = "from-list"
            full_result = decomp

        elif mode == "all":
            decomp = decompose_structure(
                structure,
                motif_library,
                allowed_motifs=None,
                allow_overlap=allow_overlap,
            )
            decomp["mode"] = "all"
            full_result = decomp

        else:
            return "Error: 'mode' must be one of 'search', 'all', or 'from-list'."

        # Save full result to disk
        out_dir = Path("motif_results")
        out_dir.mkdir(parents=True, exist_ok=True)
        base = Path(cif_path).stem
        out_path = out_dir / f"{base}_motifs_{mode}.json"
        out_path.write_text(json.dumps(full_result, indent=2, default=_json_default), encoding="utf-8")

        # Build small summary for the LLM
        summary_core = _summarise_decomposition_for_llm(full_result)
        summary = {
            "mode": mode,
            "cif_path": cif_path,
            "full_result_path": str(out_path),
            **summary_core,
        }

        return json.dumps(summary, indent=2, default=_json_default)

    async def _arun(self, query: str) -> str:
        raise NotImplementedError("this tool does not support async")


class MotifComparisonTool(BaseTool):
    """
    Compare motifs between TWO CIF files using a motif library.

    INPUT (query string):
    ---------------------
    JSON with:

      Required:
        - "cif_path_1": path to first CIF
        - "cif_path_2": path to second CIF

      Motif definitions: provide EITHER:
        - "motifs": list of motif definitions, OR
        - "motif_library_path": path to JSON file of motifs

      Optional:
        - "allowed_motifs": list of motif names to consider (subset)
        - "allow_overlap": bool, default True

    OUTPUT:
    -------
    A small JSON summary **plus** a path to a full JSON file on disk.
    """

    name: str = "MotifComparison"
    description: str = (
        "Compare motifs between two CIF files using a motif library. "
        "Input MUST be a JSON string with keys: "
        "'cif_path_1', 'cif_path_2', and either 'motifs' or 'motif_library_path'. "
        "Returns a compact summary and saves the full comparison result to disk."
    )

    default_motif_library_path: Optional[str] = None

    def __init__(self, default_motif_library_path: Optional[str] = None):
        super().__init__()
        self.default_motif_library_path = default_motif_library_path

    def _run(self, query: str) -> str:
        try:
            params = json.loads(query)
        except json.JSONDecodeError:
            return (
                "Invalid input for MotifComparison. Expected a JSON string. Example:\n"
                '{ "cif_path_1": "path/to/a.cif", '
                '"cif_path_2": "path/to/b.cif", '
                '"motif_library_path": "path/to/motifs.json" }'
            )

        cif_path_1 = params.get("cif_path_1", None)
        cif_path_2 = params.get("cif_path_2", None)
        if not cif_path_1 or not cif_path_2:
            return "Error: 'cif_path_1' and 'cif_path_2' are both required."

        motif_defs = params.get("motifs", None)
        motif_library_path = params.get("motif_library_path", self.default_motif_library_path)
        allowed_motifs = params.get("allowed_motifs", None)
        allow_overlap = params.get("allow_overlap", True)

        try:
            motif_library = _load_motif_library(
                motifs=motif_defs,
                motif_library_path=motif_library_path,
            )
        except Exception as e:
            return f"Error loading motif library: {e}"

        try:
            struct1 = Structure.from_file(cif_path_1)
        except Exception as e:
            return f"Error reading CIF file 1 '{cif_path_1}': {e}"

        try:
            struct2 = Structure.from_file(cif_path_2)
        except Exception as e:
            return f"Error reading CIF file 2 '{cif_path_2}': {e}"

        try:
            full_result = compare_two_structures(
                struct1,
                struct2,
                motif_library,
                allowed_motifs=allowed_motifs,
                allow_overlap=allow_overlap,
            )
        except Exception as e:
            return f"Error comparing motifs: {e}"

        # Save full comparison result to disk
        out_dir = Path("motif_results")
        out_dir.mkdir(parents=True, exist_ok=True)
        base1 = Path(cif_path_1).stem
        base2 = Path(cif_path_2).stem
        out_path = out_dir / f"compare_{base1}_vs_{base2}.json"
        out_path.write_text(json.dumps(full_result, indent=2, default=_json_default), encoding="utf-8")

        # Small summary for LLM
        summary_core = _summarise_comparison_for_llm(full_result)
        summary = {
            "cif_path_1": cif_path_1,
            "cif_path_2": cif_path_2,
            "full_result_path": str(out_path),
            **summary_core,
        }

        return json.dumps(summary, indent=2, default=_json_default)

    async def _arun(self, query: str) -> str:
        raise NotImplementedError("this tool does not support async")

```


================================================================================
=== FILE: chemcrow\tools\New\VastraVisualise.py ===
================================================================================

```python
import os
import subprocess
import tempfile
from pathlib import Path
from typing import Optional

from langchain.tools import BaseTool


# -------------------- VESTA helpers (based on qlip.visualization.vesta) -------------------- #

def _resolve_vesta(exe_hint: Optional[str] = None) -> str:
    """
    Resolve the VESTA executable path.

    Priority:
      1. explicit exe_hint argument (if given and exists)
      2. VESTA_EXE environment variable (if set and exists)
      3. shutil.which("VESTA") / shutil.which("VESTA.exe")
      4. (optional) a common default installation path on Windows

    This is modelled on qlip.visualization.vesta._resolve_vesta, with a couple of
    extra robustness tweaks for ChemCrow usage.
    """
    from shutil import which

    # 1) direct hint
    if exe_hint:
        p = Path(exe_hint)
        if p.exists():
            return str(p.resolve())

    # 2) env var
    env_path = os.getenv("VESTA_EXE")
    if env_path:
        p = Path(env_path)
        if p.exists():
            return str(p.resolve())

    # 3) PATH lookup
    for name in ("VESTA", "VESTA.exe"):
        p = which(name)
        if p:
            return str(Path(p).resolve())

    # 4) optional hard-coded default (edit if you like)
    default_win = Path(r"C:\Program Files\VESTA\VESTA.exe")
    if default_win.exists():
        return str(default_win.resolve())

    raise RuntimeError(
        "VESTA executable not found. "
        "Pass vesta_exe=..., set VESTA_EXE, or add VESTA to PATH."
    )


def _norm(p: Path) -> str:
    """
    VESTA CLI on Windows prefers absolute paths with forward slashes.
    Mirrors qlip.visualization.vesta._norm.
    """
    return str(Path(p).resolve()).replace("\\", "/")


def _write_temp_cif_from_text(cif_text: str, tmp_dir: Path) -> Path:
    """
    Write raw CIF text to a temporary file in tmp_dir, return its path.
    """
    cif_path = tmp_dir / "vastra_input.cif"
    cif_path.write_text(cif_text)
    return cif_path


def _run_vesta_export(
    cif_path: Path,
    out_png: Path,
    vesta_exe: Optional[str] = None,
    scale: Optional[int] = 2,
    nogui: bool = True,
) -> str:
    """
    Call VESTA to export a PNG from a CIF, in a headless-friendly way.

    This is analogous to your qlip.visualization.vesta._vesta_export_and_open,
    but specialised for 'export PNG and exit' and suitable as a non-interactive tool.
    """
    exe = _resolve_vesta(vesta_exe)
    out_png.parent.mkdir(parents=True, exist_ok=True)

    cmd = [exe]
    if nogui:
        cmd.append("-nogui")
    # Basic: open CIF and export image
    cmd += ["-open", _norm(cif_path), "-export_img"]

    if scale is not None:
        cmd += [f"scale={scale}", _norm(out_png)]
    else:
        cmd += [_norm(out_png)]

    # For tool use we *do* want VESTA to exit afterwards
    # (if your VESTA respects -close you could add it, but it's often unnecessary)
    # cmd += ["-close", _norm(cif_path)]

    creationflags = 0
    if os.name == "nt":
        # Hide console window on Windows
        creationflags = getattr(subprocess, "CREATE_NO_WINDOW", 0)

    result = subprocess.run(
        cmd,
        check=False,
        capture_output=True,
        text=True,
        creationflags=creationflags,
    )

    if result.returncode != 0:
        raise RuntimeError(
            f"VESTA command failed with code {result.returncode}.\n"
            f"STDOUT:\n{result.stdout}\n\nSTDERR:\n{result.stderr}"
        )

    if not out_png.exists():
        raise RuntimeError(
            f"VESTA reported success but '{out_png}' was not created."
        )

    return str(out_png.resolve())


# ----------------------- ChemCrow / LangChain tool ----------------------- #

class VastraVisualise(BaseTool):
    """
    ChemCrow tool: given CIF content (or a CIF file path), generate a PNG via VESTA.

    Input contract for the LLM:
      - Pass EITHER:
          (a) the path to an existing '.cif' file, OR
          (b) the full CIF contents as text, exactly as they appear in the file.

    Behaviour:
      - If the input string looks like a path to an existing '.cif' file, Vastra will
        read that file and send it to VESTA.
      - Otherwise, Vastra will treat the input as raw CIF text, write a temporary CIF,
        and send that to VESTA.
      - VESTA is run in no-GUI mode if supported ('-nogui'), and a PNG image is saved
        to 'viz/vastra_output.png' (relative to the current working directory).

    Output:
      - A string of the form: 'Saved structure image to: <absolute/path/to/png>'
      - On failure, a string describing the error.
    """

    name = "VastraVisualise"
    description = (
        "Input CIF data or a path to a .cif file. "
        "Uses VESTA to render a PNG of the crystal structure into the local "
        "directory (viz/vastra_output.png) and returns the PNG path. "
        "Use this to visualise crystal structures from CIF."
    )

    # Optional configuration
    vesta_exe: Optional[str] = None
    output_png: str = "viz/vastra_output.png"
    scale: int = 2
    nogui: bool = True

    def __init__(
        self,
        vesta_exe: Optional[str] = None,
        output_png: str = "viz/vastra_output.png",
        scale: int = 2,
        nogui: bool = True,
    ):
        super().__init__()
        self.vesta_exe = vesta_exe
        self.output_png = output_png
        self.scale = scale
        self.nogui = nogui

    def _run(self, cif_input: str) -> str:
        """
        cif_input: either CIF text or a path to a .cif file.

        Heuristic, identical idea to what we discussed:
          - If cif_input is a path to an existing .cif file, read that file.
          - Otherwise, treat cif_input as raw CIF contents.
        """
        cif_input = cif_input.strip()
        potential_path = Path(cif_input)

        try:
            if potential_path.suffix.lower() == ".cif" and potential_path.exists():
                # Interpret as file path
                try:
                    cif_text = potential_path.read_text()
                except Exception as e:
                    return f"Vastra: failed to read CIF file '{potential_path}': {e}"
            else:
                # Interpret as raw CIF text
                cif_text = cif_input

            out_png_path = Path(self.output_png)

            # Use a temp dir for the intermediate cif if we are using raw text
            if potential_path.suffix.lower() == ".cif" and potential_path.exists():
                # Use the original path directly
                cif_path = potential_path
                # But still normalise/ensure absolute
                cif_path = cif_path.resolve()
                # run VESTA
                png_path = _run_vesta_export(
                    cif_path=cif_path,
                    out_png=out_png_path,
                    vesta_exe=self.vesta_exe,
                    scale=self.scale,
                    nogui=self.nogui,
                )
            else:
                # Raw text â†’ temp file
                with tempfile.TemporaryDirectory() as tmpdir:
                    tmpdir = Path(tmpdir)
                    cif_path = _write_temp_cif_from_text(cif_text, tmpdir)
                    png_path = _run_vesta_export(
                        cif_path=cif_path,
                        out_png=out_png_path,
                        vesta_exe=self.vesta_exe,
                        scale=self.scale,
                        nogui=self.nogui,
                    )

            return f"Saved structure image to: {png_path}"
        except Exception as e:
            return f"Vastra/VESTA visualisation failed: {e}"

    async def _arun(self, cif_input: str) -> str:
        """Async use is not implemented for this tool."""
        raise NotImplementedError("This tool does not support async.")

```


================================================================================
=== FILE: chemcrow.egg-info\dependency_links.txt ===
================================================================================

```


```


================================================================================
=== FILE: chemcrow.egg-info\PKG-INFO ===
================================================================================

```
Metadata-Version: 2.1
Name: chemcrow
Version: 0.3.24
Summary: Accurate solution of reasoning-intensive chemical tasks, powered by LLMs.
Home-page: https://github.com/ur-whitelab/chemcrow-public
Author: Andres M Bran, Sam Cox, Andrew White, Philippe Schwaller
Author-email: andrew.white@rochester.edu
License: MIT
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9, <3.12
Description-Content-Type: text/markdown
License-File: LICENSE

[![tests](https://github.com/ur-whitelab/chemcrow-public/actions/workflows/tests.yml/badge.svg)](https://github.com/ur-whitelab/chemcrow-public)
[![PyPI](https://img.shields.io/pypi/v/chemcrow)](https://img.shields.io/pypi/v/chemcrow)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/chemcrow)](https://img.shields.io/pypi/pyversions/chemcrow)
[![DOI:10.1101/2020.07.15.204701](https://zenodo.org/badge/DOI/10.48550/arXiv.2304.05376.svg)](https://doi.org/10.48550/arXiv.2304.05376)
[![DOI](https://zenodo.org/badge/649361700.svg)](https://zenodo.org/doi/10.5281/zenodo.10884638)




<picture>
  <source media="(prefers-color-scheme: dark)" srcset="assets/chemcrow_dark_bold.png" width='100%'>
  <source media="(prefers-color-scheme: light)" srcset="assets/chemcrow_light_bold.png" width='100%'>
  <img alt="ChemCrow logo" src="/assets/" width="100%">
</picture>


<br></br>


ChemCrow is an open source package for the accurate solution of reasoning-intensive chemical tasks.

Built with Langchain, it uses a collection of chemical tools including RDKit, paper-qa, as well as some relevant databases in chemistry, like Pubchem and chem-space.

## ðŸ¤— Try it out in [HuggingFace](https://huggingface.co/spaces/doncamilom/ChemCrow)!

[![ChemCrow Demo](assets/hf-demo.png)](https://huggingface.co/spaces/doncamilom/ChemCrow)


## âš ï¸ Note

This package does not contain all the tools described in the [ChemCrow paper](https://arxiv.org/abs/2304.05376) because
of API usage restrictions. This repo will not give the same results as that paper.

All the experiments have been released under [ChemCrow runs](https://github.com/ur-whitelab/chemcrow-runs).


## ðŸ‘©â€ðŸ’» Installation

```
pip install chemcrow
```

## ðŸ”¥ Usage
First set up your API keys in your environment.
```
export OPENAI_API_KEY=your-openai-api-key
```

You can optionally use Serp API:

```
export SERP_API_KEY=your-serpapi-api-key
```

In a Python session:
```python
from chemcrow.agents import ChemCrow

chem_model = ChemCrow(model="gpt-4-0613", temp=0.1, streaming=False)
chem_model.run("What is the molecular weight of tylenol?")
```


## ðŸ› ï¸ Self-hosting of some tools.

By default, ChemCrow relies on the RXN4Chem API for retrosynthetic planning and reaction product prediction. This can however be slow and depends on you having an API key.

Optionally, you can also self host these tools by running some pre-made docker images.

Run 

```
docker run --gpus all -d -p 8051:5000 doncamilom/rxnpred:latest
docker run --gpus all -d -p 8052:5000 doncamilom/retrosynthesis:latest
```


Now ChemCrow can be used like this:

```python
from chemcrow.agents import ChemCrow

chem_model = ChemCrow(model="gpt-4-0613", temp=0.1, streaming=False, local_rxn=True)
chem_model.run("What is the product of the reaction between styrene and dibromine?")
```


## âœ… Citation
Bran, Andres M., et al. "ChemCrow: Augmenting large-language models with chemistry tools." arXiv preprint arXiv:2304.05376 (2023).

```bibtex
@article{bran2023chemcrow,
      title={ChemCrow: Augmenting large-language models with chemistry tools},
      author={Andres M Bran and Sam Cox and Oliver Schilter and Carlo Baldassari and Andrew D White and Philippe Schwaller},
      year={2023},
      eprint={2304.05376},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      publisher={arXiv}
}
```

```


================================================================================
=== FILE: chemcrow.egg-info\requires.txt ===
================================================================================

```
ipython
python-dotenv
rdkit
synspace
openai==0.27.8
molbloom
paper-qa==1.1.1
google-search-results
langchain<=0.0.275,>=0.0.234
langchain_core==0.0.1
nest_asyncio
tiktoken
rmrkl
streamlit
rxn4chemistry
duckduckgo-search
wikipedia

```


================================================================================
=== FILE: chemcrow.egg-info\SOURCES.txt ===
================================================================================

```
LICENSE
README.md
setup.py
chemcrow/__init__.py
chemcrow/utils.py
chemcrow/version.py
chemcrow.egg-info/PKG-INFO
chemcrow.egg-info/SOURCES.txt
chemcrow.egg-info/dependency_links.txt
chemcrow.egg-info/requires.txt
chemcrow.egg-info/top_level.txt
chemcrow/agents/__init__.py
chemcrow/agents/chemcrow.py
chemcrow/agents/prompts.py
chemcrow/agents/tools.py
chemcrow/data/chem_wep_smi.csv
chemcrow/frontend/__init__.py
chemcrow/frontend/streamlit_callback_handler.py
chemcrow/frontend/utils.py
chemcrow/tools/__init__.py
chemcrow/tools/chemspace.py
chemcrow/tools/converters.py
chemcrow/tools/prompts.py
chemcrow/tools/rdkit.py
chemcrow/tools/reactions.py
chemcrow/tools/rxn4chem.py
chemcrow/tools/safety.py
chemcrow/tools/search.py
tests/test_agent.py
tests/test_converters.py
tests/test_rdkit.py
tests/test_safety_tools.py
tests/test_search.py
```


================================================================================
=== FILE: chemcrow.egg-info\top_level.txt ===
================================================================================

```
chemcrow

```


================================================================================
=== FILE: Chris\setup.py ===
================================================================================

```python
from rdkit import RDLogger
RDLogger.DisableLog("rdApp.*")

from chemcrow.agents import ChemCrow


# Create the ChemCrow agent
chem_model = ChemCrow(
    model="gpt-4.1-mini",  # any GPT-4 / GPT-4o-style name you have access to
    temp=0.1,
    streaming=False
)

# Ask it a question
answer = chem_model.run("What is the molecular weight of paracetamol? dont use Name2SMILES")
print(answer)

```


================================================================================
=== FILE: Chris\tool.py ===
================================================================================

```python
import os
import textwrap
import traceback

from chemcrow.agents import make_tools
from langchain.chat_models import ChatOpenAI


# ---------- LLM FOR TOOLS ----------

def make_tools_llm():
    """LLM used by ChemCrow tools (Wikipedia, LiteratureSearch, etc.)."""
    return ChatOpenAI(
        model_name="gpt-4.1-mini",  # or gpt-4o-mini, etc.
        temperature=0.0,
        request_timeout=1000,
    )


# ---------- BUILD TOOLS ----------

def build_tools():
    api_keys = {
        "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY", ""),
        "SEMANTIC_SCHOLAR_API_KEY": os.environ.get("SEMANTIC_SCHOLAR_API_KEY", ""),
        "SERPAPI_API_KEY": os.environ.get("SERPAPI_API_KEY", ""),
        "RXN4CHEM_API_KEY": os.environ.get("RXN4CHEM_API_KEY", ""),
        "POSTERA_API_KEY": os.environ.get("POSTERA_API_KEY", ""),
        "CHEMSPACE_API_KEY": os.environ.get("CHEMSPACE_API_KEY", ""),
    }

    tools_llm = make_tools_llm()

    tools = make_tools(
        tools_llm,
        api_keys=api_keys,
        verbose=False,
    )
    return tools, api_keys


# ---------- TEST INPUTS PER TOOL ----------

def get_test_input(tool_name: str) -> str | None:
    """Return a tool-specific test input, or None if we don't have one."""
    # Some valid SMILES we can reuse
    PARACETAMOL_SMILES = "CC(=O)NC1=CC=C(C=C1)O"
    ASPIRIN_SMILES = "CC(=O)OC1=CC=CC=C1C(=O)O"
    ETHANOL_SMILES = "CCO"

    tests = {
        # General
        "Python_REPL": "2 + 2",
        "Wikipedia": "Paracetamol",

        # Converters / identifiers
        "Name2SMILES": "aspirin",
        "Mol2CAS": "paracetamol",
        "SMILES2Name": ASPIRIN_SMILES,
        "SMILES2Weight": PARACETAMOL_SMILES,

        # Functional / safety / similarity
        "FunctionalGroups": ETHANOL_SMILES,
        "ExplosiveCheck": "C(C(=O)O[N+](=O)[O-])N",  # nitro-ish
        "ControlChemCheck": ASPIRIN_SMILES,
        "SimilarityToControlChem": PARACETAMOL_SMILES,
        "SafetySummary": PARACETAMOL_SMILES,
        # expects "SMI1.SMI2"
        "MolSimilarity": f"{PARACETAMOL_SMILES}.{ASPIRIN_SMILES}",

        # IP / literature
        "PatentCheck": "paracetamol",
        "LiteratureSearch": "melting point of paracetamol",
    }

    return tests.get(tool_name)


# ---------- HEURISTICS FOR PASS / FAIL / SKIP ----------

ERROR_SUBSTRINGS = [
    "invalid smiles",
    "wrong argument",
    "please input a valid",
    "molecule not found",
    "could not find a molecule matching the text",
    "error:",
]

def looks_like_error(output: str) -> bool:
    text = output.lower()
    return any(s in text for s in ERROR_SUBSTRINGS)


def tool_needs_missing_key(tool, api_keys: dict) -> bool:
    """
    Crude heuristic: if description mentions an external API and we don't have its key,
    treat this as SKIP (not a tool bug, just missing credentials).
    """
    desc = (getattr(tool, "description", "") or "").lower()

    patterns = {
        "semantic scholar": "SEMANTIC_SCHOLAR_API_KEY",
        "serpapi": "SERPAPI_API_KEY",
        "rxn for chemistry": "RXN4CHEM_API_KEY",
        "rxn ": "RXN4CHEM_API_KEY",
        "postera": "POSTERA_API_KEY",
        "chemscape": "CHEMSPACE_API_KEY",
        "chemspec": "CHEMSPACE_API_KEY",
        "chems space": "CHEMSPACE_API_KEY",
    }

    for substr, key in patterns.items():
        if substr in desc and not api_keys.get(key):
            return True

    return False


def summarize_output(text: str, max_len: int = 500) -> str:
    text = str(text)
    if len(text) <= max_len:
        return text
    return text[:max_len] + "... [truncated]"


# ---------- MAIN ----------

def main():
    print("ChemCrow tool smoke tests\n")

    tools, api_keys = build_tools()

    print("Loaded tools:")
    for t in tools:
        print(f"  - {t.name}")
    print("\n================ RUNNING TESTS ================\n")

    results = {}

    for tool in tools:
        name = tool.name
        test_input = get_test_input(name)

        print(f"\n--- Tool: {name} ---")

        if tool_needs_missing_key(tool, api_keys):
            print("  [SKIP] Missing required API key for this tool.")
            results[name] = {
                "status": "SKIP",
                "reason": "missing_api_key",
                "output": None,
            }
            continue

        if test_input is None:
            print("  [SKIP] No test case defined yet for this tool.")
            results[name] = {
                "status": "SKIP",
                "reason": "no_test_case",
                "output": None,
            }
            continue

        print(f"  Test input: {test_input!r}")

        try:
            raw_output = tool.run(test_input)
            out_str = str(raw_output)
            errorish = looks_like_error(out_str)
            status = "PASS" if not errorish else "FAIL_OUTPUT"
        except Exception as e:
            raw_output = f"[EXCEPTION] {type(e).__name__}: {e}"
            out_str = raw_output
            status = "FAIL_EXCEPTION"

        print(f"  Status: {status}")
        print("  Output:")
        print(
            textwrap.indent(
                summarize_output(out_str),
                prefix="    ",
            )
        )

        results[name] = {
            "status": status,
            "output": out_str,
        }

    print("\n================ SUMMARY ================\n")
    for name, info in results.items():
        print(f"{name}: {info['status']}")

    print("\n================ SUGGESTED WHITELIST ================\n")
    good_tools = [name for name, info in results.items() if info["status"] == "PASS"]
    print("Tools that look OK to keep in a 'clean' ChemCrow instance:")
    for name in good_tools:
        print("  -", name)

    print("\nYou can use this snippet in your own ChemCrow setup:")
    print("    from chemcrow.agents import make_tools, ChemCrow")
    print("    tools = make_tools(tools_llm, api_keys=api_keys, local_rxn=False)")
    print("    allowed = {")
    for name in good_tools:
        print(f"        {name!r},")
    print("    }")
    print("    tools = [t for t in tools if t.name in allowed]")
    print("    chem_model = ChemCrow(tools=tools, ...)")


if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: motif_results\07000N2_ddec_motifs_all.json ===
================================================================================

```json
{
  "motifs": [
    {
      "motif_name": "C_sp2_allC",
      "central_index": 85,
      "neighbor_indices": [
        91,
        97,
        152
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 87,
      "neighbor_indices": [
        93,
        102,
        147
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 89,
      "neighbor_indices": [
        95,
        107,
        142
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 91,
      "neighbor_indices": [
        85,
        112,
        137
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 93,
      "neighbor_indices": [
        87,
        117,
        132
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 95,
      "neighbor_indices": [
        89,
        122,
        127
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 98,
      "neighbor_indices": [
        100,
        143,
        153
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 103,
      "neighbor_indices": [
        105,
        148,
        153
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 108,
      "neighbor_indices": [
        110,
        143,
        148
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 113,
      "neighbor_indices": [
        115,
        128,
        138
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 118,
      "neighbor_indices": [
        120,
        133,
        138
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 123,
      "neighbor_indices": [
        125,
        128,
        133
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 128,
      "neighbor_indices": [
        113,
        123,
        130
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 133,
      "neighbor_indices": [
        118,
        123,
        135
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 138,
      "neighbor_indices": [
        113,
        118,
        140
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 143,
      "neighbor_indices": [
        98,
        108,
        145
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 148,
      "neighbor_indices": [
        103,
        108,
        150
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 153,
      "neighbor_indices": [
        98,
        103,
        155
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 157,
      "neighbor_indices": [
        163,
        169,
        224
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 159,
      "neighbor_indices": [
        165,
        174,
        219
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 161,
      "neighbor_indices": [
        167,
        179,
        214
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 163,
      "neighbor_indices": [
        157,
        184,
        209
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 165,
      "neighbor_indices": [
        159,
        189,
        204
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 167,
      "neighbor_indices": [
        161,
        194,
        199
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 170,
      "neighbor_indices": [
        172,
        215,
        225
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 175,
      "neighbor_indices": [
        177,
        220,
        225
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 180,
      "neighbor_indices": [
        182,
        215,
        220
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 185,
      "neighbor_indices": [
        187,
        200,
        210
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 190,
      "neighbor_indices": [
        192,
        205,
        210
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 195,
      "neighbor_indices": [
        197,
        200,
        205
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 200,
      "neighbor_indices": [
        185,
        195,
        202
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 205,
      "neighbor_indices": [
        190,
        195,
        207
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 210,
      "neighbor_indices": [
        185,
        190,
        212
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 215,
      "neighbor_indices": [
        170,
        180,
        217
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 220,
      "neighbor_indices": [
        175,
        180,
        222
      ]
    },
    {
      "motif_name": "C_sp2_allC",
      "central_index": 225,
      "neighbor_indices": [
        170,
        175,
        227
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 228,
      "neighbor_indices": [
        99
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 229,
      "neighbor_indices": [
        104
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 230,
      "neighbor_indices": [
        109
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 231,
      "neighbor_indices": [
        114
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 232,
      "neighbor_indices": [
        119
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 233,
      "neighbor_indices": [
        124
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 234,
      "neighbor_indices": [
        129
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 235,
      "neighbor_indices": [
        134
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 236,
      "neighbor_indices": [
        139
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 237,
      "neighbor_indices": [
        144
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 238,
      "neighbor_indices": [
        149
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 239,
      "neighbor_indices": [
        154
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 240,
      "neighbor_indices": [
        171
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 241,
      "neighbor_indices": [
        176
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 242,
      "neighbor_indices": [
        181
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 243,
      "neighbor_indices": [
        186
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 244,
      "neighbor_indices": [
        191
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 245,
      "neighbor_indices": [
        196
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 246,
      "neighbor_indices": [
        201
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 247,
      "neighbor_indices": [
        206
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 248,
      "neighbor_indices": [
        211
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 249,
      "neighbor_indices": [
        216
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 250,
      "neighbor_indices": [
        221
      ]
    },
    {
      "motif_name": "O_linker",
      "central_index": 251,
      "neighbor_indices": [
        226
      ]
    }
  ],
  "unassigned_sites": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31,
    32,
    33,
    34,
    35,
    36,
    37,
    38,
    39,
    40,
    41,
    42,
    43,
    44,
    45,
    46,
    47,
    48,
    49,
    50,
    51,
    52,
    53,
    54,
    55,
    56,
    57,
    58,
    59,
    60,
    61,
    62,
    63,
    64,
    65,
    66,
    67,
    68,
    69,
    70,
    71,
    72,
    73,
    74,
    75,
    76,
    77,
    78,
    79,
    80,
    81,
    82,
    83,
    84,
    86,
    88,
    90,
    92,
    94,
    96,
    101,
    106,
    111,
    116,
    121,
    126,
    131,
    136,
    141,
    146,
    151,
    156,
    158,
    160,
    162,
    164,
    166,
    168,
    173,
    178,
    183,
    188,
    193,
    198,
    203,
    208,
    213,
    218,
    223
  ],
  "mode": "all"
}
```


================================================================================
=== FILE: tests\test_agent.py ===
================================================================================

```python
import os

import pytest

import chemcrow


def test_version():
    assert chemcrow.__version__


@pytest.mark.skip(reason="This requires an api call")
def test_agent_init():
    chem_model = chemcrow.ChemCrow(
        model="gpt-3.5-turbo-0125", temp=0.1, max_iterations=2, api_keys={}
    )
    out = chem_model.run("hello")
    assert isinstance(out, str)

```


================================================================================
=== FILE: tests\test_converters.py ===
================================================================================

```python
import os

import pytest

from chemcrow.tools.chemspace import ChemSpace, GetMoleculePrice
from chemcrow.tools.converters import Query2CAS, Query2SMILES, SMILES2Name
from chemcrow.utils import canonical_smiles


@pytest.fixture
def singlemol():
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O"


@pytest.fixture
def molset1():
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O.CC(C)c1ccccc1"


@pytest.fixture
def single_iupac():
    # Test with a molecule with iupac name
    return "4-(4-hydroxyphenyl)butan-2-one"


def test_q2cas_iupac(single_iupac):
    tool = Query2CAS()
    out = tool._run(single_iupac)
    assert out == "5471-51-2"


def test_q2cas_cafeine(singlemol):
    tool = Query2CAS()
    out = tool._run(singlemol)
    assert out == "58-08-2"


def test_q2cas_badinp():
    tool = Query2CAS()
    out = tool._run("nomol")
    assert out.endswith("no Pubchem entry") or out.endswith("not found")


def test_q2s_iupac(single_iupac):
    tool = Query2SMILES()
    out = tool._run(single_iupac)
    assert out == "CC(=O)CCc1ccc(O)cc1"


def test_q2s_cafeine(singlemol):
    tool = Query2SMILES()
    out = tool._run("caffeine")
    assert out == canonical_smiles(singlemol)


def test_q2s_fail(molset1):
    tool = Query2SMILES()
    out = tool._run(molset1)
    assert out.endswith("input one molecule at a time.")


def test_getmolprice_no_api():
    tool = GetMoleculePrice(chemspace_api_key=None)
    price = tool._run("caffeine")
    assert "No Chemspace API key found" in price

def test_getmolprice(singlemol):
    if os.getenv("CHEMSPACE_API_KEY") is None:
        pytest.skip("No Chemspace API key found")
    else:
        tool = GetMoleculePrice(chemspace_api_key=os.getenv("CHEMSPACE_API_KEY"))
        price = tool._run(singlemol)
        assert "of this molecule cost" in price


def test_query2smiles_chemspace(singlemol, single_iupac):
    if os.getenv("CHEMSPACE_API_KEY") is None:
        pytest.skip("No Chemspace API key found")
    else:
        chemspace = ChemSpace(chemspace_api_key=os.getenv("CHEMSPACE_API_KEY"))
        smiles_from_chemspace = chemspace.convert_mol_rep("caffeine", "smiles")
        assert "CN1C=NC2=C1C(=O)N(C)C(=O)N2C" in smiles_from_chemspace

        price = chemspace.buy_mol(singlemol)
        assert "of this molecule cost" in price

        price = chemspace.buy_mol(single_iupac)
        assert "of this molecule cost" in price


def test_smiles2name():
    smiles2name = SMILES2Name()
    assert (
        smiles2name.run("CN1C=NC2=C1C(=O)N(C)C(=O)N2C")
        == "caffeine"
    )
    assert "acetic acid" in smiles2name.run("CC(=O)O").lower()
    assert "Error:" in smiles2name.run("nonsense")

```


================================================================================
=== FILE: tests\test_rdkit.py ===
================================================================================

```python
import pytest

from chemcrow.tools.rdkit import FuncGroups, MolSimilarity, SMILES2Weight


@pytest.fixture
def singlemol():
    # Single mol
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O"


@pytest.fixture
def molset1():
    # Set of mols
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O.CC(C)c1ccccc1"


@pytest.fixture
def molset2():
    # Set of mols
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O.O=C1N(C)C(C2=C(N=CN2C)N1CCC)=O"


@pytest.fixture
def single_iupac():
    # Test with a molecule with iupac name
    return "4-(4-hydroxyphenyl)butan-2-one"


# MolSimilarity


def test_molsim_1(molset1):
    tool = MolSimilarity()
    assert tool(molset1).endswith("not similar.")


def test_molsim_2(molset2):
    tool = MolSimilarity()
    assert tool(molset2).endswith("very similar.")


def test_molsim_same(singlemol):
    tool = MolSimilarity()
    out = tool("{}.{}".format(singlemol, singlemol))
    assert out == "Error: Input Molecules Are Identical"


def test_molsim_badinp(singlemol):
    tool = MolSimilarity()
    out = tool(singlemol)
    assert out == "Input error, please input two smiles strings separated by '.'"


def test_molsim_iupac(singlemol, single_iupac):
    tool = MolSimilarity()
    out = tool("{}.{}".format(singlemol, single_iupac))
    assert out == "Error: Not a valid SMILES string"


# SMILES2Weight


def test_mw(singlemol):
    tool = SMILES2Weight()
    mw = tool(singlemol)
    assert abs(mw - 194.0) < 1.0


def test_badinp(singlemol):
    tool = SMILES2Weight()
    mw = tool(singlemol + "x")
    assert mw == "Invalid SMILES string"


# FuncGroups


def test_fg_single(singlemol):
    tool = FuncGroups()
    out = tool(singlemol)
    assert "ketones" in out


def test_fg_iupac(single_iupac):
    tool = FuncGroups()
    out = tool(single_iupac)
    assert out == "Wrong argument. Please input a valid molecular SMILES."

```


================================================================================
=== FILE: tests\test_safety_tools.py ===
================================================================================

```python
import pytest
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI

from chemcrow.tools.safety import ControlChemCheck, ExplosiveCheck, SafetySummary

load_dotenv()


@pytest.fixture
def controlledchemcheck():
    return ControlChemCheck()


def test_controlchemcheck_controlled(controlledchemcheck):
    ans_cas = controlledchemcheck._run("10025-87-3")
    ans_smi = controlledchemcheck._run("O=P(Cl)(Cl)Cl")
    assert "appears in a list" in ans_cas
    assert "appears in a list" in ans_smi


def test_controlchemcheck_notsimilar(controlledchemcheck):
    acetone_smi = "CC(=O)C"
    acetone_cas = "67-64-1"
    ans_cas = controlledchemcheck._run(acetone_cas)
    ans_smi = controlledchemcheck._run(acetone_smi)
    print("ans_cas", ans_cas)
    print("ans_smi", ans_smi)
    assert "appears in a list" not in ans_cas
    assert "appears in a list" not in ans_smi

    assert "is similar to" not in ans_cas
    assert "is similar to" not in ans_smi


@pytest.mark.skip(reason="This requires an api call")
def test_safety_summary():
    llm = ChatOpenAI()
    safety_summary = SafetySummary(llm=llm)
    cas = "676-99-3"
    ans = safety_summary(cas)
    assert isinstance(ans, str)
    assert "valid CAS number" not in ans
    assert "not found" not in ans
    assert "operator safety" in ans.lower()
    assert "ghs" in ans.lower()
    assert "environment" in ans.lower()
    assert "societal" in ans.lower()


@pytest.fixture
def explosive():
    return ExplosiveCheck()


def test_explosive_check_exp(explosive):
    tnt_cas = "118-96-7"
    ans = explosive(tnt_cas)
    assert "Error" not in ans
    assert ans == "Molecule is explosive"


def test_explosive_check_nonexp(explosive):
    non_exp_cas = "10025-87-3"
    ans = explosive(non_exp_cas)
    assert "Error" not in ans
    assert ans == "Molecule is not known to be explosive"

```


================================================================================
=== FILE: tests\test_search.py ===
================================================================================

```python
import ast
import os

import pytest
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI

from chemcrow.tools.search import PatentCheck, Scholar2ResultLLM
from chemcrow.utils import split_smiles

load_dotenv()


@pytest.fixture
def questions():
    qs = [
        "What are the effects of norhalichondrin B in mammals?",
    ]
    return qs[0]


@pytest.mark.skip(reason="This requires an api call")
def test_litsearch(questions):
    llm = ChatOpenAI()

    searchtool = Scholar2ResultLLM(llm=llm)
    for q in questions:
        ans = searchtool._run(q)
        assert isinstance(ans, str)
        assert len(ans) > 0
    if os.path.exists("../query"):
        os.rmdir("../query")


@pytest.fixture
def molset1():
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O.CC(C)c1ccccc1"


@pytest.fixture
def singlemol():
    return "O=C1N(C)C(C2=C(N=CN2C)N1C)=O"


@pytest.fixture
def single_iupac():
    # Test with a molecule with iupac name
    return "4-(4-hydroxyphenyl)butan-2-one"


@pytest.fixture
def choline():
    # Test with a molecule in clintox
    return "CCCCCCCCC[NH+]1C[C@@H]([C@H]([C@@H]([C@H]1CO)O)O)O"


@pytest.fixture
def patentcheck():
    return PatentCheck()


def test_patentcheck(singlemol, patentcheck):
    patented = patentcheck._run(singlemol)
    patented = ast.literal_eval(patented)
    assert len(patented) == 1
    assert patented[singlemol] == "Patented"


def test_patentcheck_molset(molset1, patentcheck):
    patented = patentcheck._run(molset1)
    patented = ast.literal_eval(patented)
    mols = split_smiles(molset1)
    assert len(patented) == len(mols)
    assert patented[mols[0]] == "Patented"
    assert patented[mols[1]] == "Novel"


def test_patentcheck_iupac(single_iupac, patentcheck):
    patented = patentcheck._run(single_iupac)
    assert patented == "Invalid SMILES string"


def test_patentcheck_not(choline, patentcheck):
    patented = patentcheck._run(choline)
    patented = ast.literal_eval(patented)
    assert len(patented) == 1
    assert patented[choline] == "Novel"

```


################################################################################
# MANIFEST
# Included files:
#  - clean_chemcrow_minimal copy.py
#  - clean_chemcrow_minimal.py
#  - dev-requirements.txt
#  - LICENSE
#  - list.py
#  - project_context.txt
#  - project_to_txt.py
#  - README.md
#  - requirments.txt
#  - setup.py
#  - test_all_tools copy.py
#  - test_all_tools.py
#  - test_cof_multi_bo.py
#  - chemcrow\utils.py
#  - chemcrow\version.py
#  - chemcrow\__init__.py
#  - chemcrow\agents\chemcrow.py
#  - chemcrow\agents\prompts.py
#  - chemcrow\agents\tools.py
#  - chemcrow\agents\__init__.py
#  - chemcrow\data\chem_wep.csv
#  - chemcrow\data\chem_wep_smi.csv
#  - chemcrow\docker\README.md
#  - chemcrow\docker\aizynthfinder\app.py
#  - chemcrow\docker\aizynthfinder\config.yml
#  - chemcrow\docker\aizynthfinder\Dockerfile
#  - chemcrow\docker\aizynthfinder\files\README.md
#  - chemcrow\docker\molecular-transformer\app.py
#  - chemcrow\docker\molecular-transformer\Dockerfile
#  - chemcrow\docker\molecular-transformer\models\README.md
#  - chemcrow\frontend\streamlit_callback_handler.py
#  - chemcrow\frontend\utils.py
#  - chemcrow\frontend\__init__.py
#  - chemcrow\tools\chemspace.py
#  - chemcrow\tools\converters.py
#  - chemcrow\tools\prompts.py
#  - chemcrow\tools\rdkit.py
#  - chemcrow\tools\reactions.py
#  - chemcrow\tools\rxn4chem.py
#  - chemcrow\tools\safety.py
#  - chemcrow\tools\search.py
#  - chemcrow\tools\__init__.py
#  - chemcrow\tools\New\Arxiv2ResultLLM.py
#  - chemcrow\tools\New\BayesianOptimizer.py
#  - chemcrow\tools\New\motifs_cof_simple.json
#  - chemcrow\tools\New\motif_tools.py
#  - chemcrow\tools\New\VastraVisualise.py
#  - chemcrow.egg-info\dependency_links.txt
#  - chemcrow.egg-info\PKG-INFO
#  - chemcrow.egg-info\requires.txt
#  - chemcrow.egg-info\SOURCES.txt
#  - chemcrow.egg-info\top_level.txt
#  - Chris\setup.py
#  - Chris\tool.py
#  - motif_results\07000N2_ddec_motifs_all.json
#  - tests\test_agent.py
#  - tests\test_converters.py
#  - tests\test_rdkit.py
#  - tests\test_safety_tools.py
#  - tests\test_search.py
#
# Skips summary:
#  dir: 9
#  hidden: 2
#  size: 0
#  binary: 0
#  ext: 28
#  other: 0
# END
